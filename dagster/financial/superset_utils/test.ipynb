{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Union\n",
    "import pandas as pd\n",
    "import logging\n",
    "import re\n",
    "import sqlfluff\n",
    "import requests\n",
    "import ruamel.yaml\n",
    "import psycopg2\n",
    "from bs4 import BeautifulSoup\n",
    "from markdown import markdown\n",
    "import datetime\n",
    "from urllib.parse import unquote\n",
    "from typing import Any, Dict, Iterator, List, Union\n",
    "import json\n",
    "from pgsanity.pgsanity import check_string\n",
    "import os\n",
    "from dbt.cli.main import dbtRunner, dbtRunnerResult\n",
    "import smtplib\n",
    "import ssl\n",
    "from itertools import compress\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from jinja2 import Template\n",
    "with open(DBT_PROJECT_DIR + \"create_model.txt\", \"r\") as f:\n",
    "    MODEL_TEMPLATE = Template(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "MATERIALIZATION_MAPPING = {1: \"table\", 2: \"view\", 3: \"incremental\", 4: \"ephemereal\"}\n",
    "SUPERSET_USERNAME = \"superset\"\n",
    "SUPERSET_PASSWORD = \"superset\"\n",
    "SUPERSET_HOST = \"http://34.82.185.252:30007/\"\n",
    "DATABASE_USERNAME = \"fdp\"\n",
    "DATABASE_PASSWORD = \"fdp\"\n",
    "DATABASE_HOST = \"34.82.185.252\"\n",
    "DATABASE_PORT = 30005\n",
    "DATABASE_NAME = \"financial_data\"\n",
    "QUERY_SCHEMA=\"financial_query\"\n",
    "QUERY_TABLE=\"query\"\n",
    "MANIFEST_PATH=\"/home/vu/Desktop/Projects/Thesis/financial/dbt/target/manifest.json\"\n",
    "EMAIL_PORT = 465\n",
    "SMTP = \"smtp.gmail.com\"\n",
    "EMAIL_SENDER = \"catvu113@gmail.com\"\n",
    "EMAIL_PASSWORD = \"xhtzakhmnsbufufy\"\n",
    "USER_MODEL_PATH = \"/home/vu/Desktop/Projects/Thesis/financial/dbt/models/user\"\n",
    "DBT_PROJECT_DIR = \"/home/vu/Desktop/Projects/Thesis/financial/dbt/\"\n",
    "DATABASE_ID = 1\n",
    "SUPERSET_ID = 34\n",
    "USER_SCHEMA = \"user\"\n",
    "SERVING_SCHEMA=\"marts\"\n",
    "context = ssl.create_default_context()\n",
    "SMTP = smtplib.SMTP_SSL(\"smtp.gmail.com\", 465, context=context)\n",
    "SST_DATABASE_NAME = \"FDP Reader\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupersetDBTSessionConnector:\n",
    "    \"\"\"A class for accessing the Superset API in an easy way.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Instantiates the class.\n",
    "\n",
    "        ''access_token'' will be instantiated via enviromental variable\n",
    "        If ``access_token`` is None, attempts to obtain it using ``refresh_token``.\n",
    "\n",
    "        Args:\n",
    "            api_url: Base API URL of a Superset instance, e.g. https://my-superset/api/v1.\n",
    "            access_token: Access token to use for accessing protected endpoints of the Superset\n",
    "                API. Can be automatically obtained if ``refresh_token`` is not None.\n",
    "            refresh_token: Refresh token to use for obtaining or refreshing the ``access_token``.\n",
    "                If None, no refresh will be done.\n",
    "        \"\"\"\n",
    "        self.__url = SUPERSET_HOST\n",
    "        self.__api_url = self.__url + \"api/v1/\"\n",
    "\n",
    "        self.__session = requests.session()\n",
    "\n",
    "        self.__username = SUPERSET_USERNAME\n",
    "        self.__password = SUPERSET_PASSWORD\n",
    "\n",
    "        self.__refresh_session()\n",
    "\n",
    "    def __refresh_session(self):\n",
    "        logging.info(\"Refreshing session\")\n",
    "\n",
    "        soup = BeautifulSoup(self.__session.post(self.__url + \"login\").text, \"html.parser\")\n",
    "        self.__csrf_token = soup.find(\"input\", {\"id\": \"csrf_token\"})[\"value\"]  # type: ignore\n",
    "\n",
    "        data = {\n",
    "            \"username\": self.__username,\n",
    "            \"password\": self.__password,\n",
    "            \"provider\": \"db\",\n",
    "            \"refresh\": True,\n",
    "        }\n",
    "        headers = {\n",
    "            # 'Authorization': 'Bearer {}'.format(self.____access_token),\n",
    "            \"x-csrftoken\": self.__csrf_token,\n",
    "        }\n",
    "        response = self.__session.post(self.__url + \"login\", json=data, headers=headers)  # type: ignore\n",
    "        return True\n",
    "\n",
    "    def request(self, method, endpoint, **request_kwargs):\n",
    "        \"\"\"Executes a request against the Superset API.\n",
    "\n",
    "        Args:\n",
    "            method: HTTP method to use.\n",
    "            endpoint: Endpoint to use.\n",
    "            **request_kwargs: Any ``requests.request`` arguments to use.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing response body parsed from JSON.\n",
    "\n",
    "        Raises:\n",
    "            HTTPError: There is an HTTP error (detected by ``requests.Response.raise_for_status``)\n",
    "                even after retrying with a fresh session.\n",
    "        \"\"\"\n",
    "\n",
    "        logging.info(\"About to %s execute request for endpoint %s\", method, endpoint)\n",
    "\n",
    "        url = self.__api_url + endpoint\n",
    "        csrf_headers = {\n",
    "            # 'Authorization': 'Bearer {}'.format(self.__access_token),\n",
    "            \"x-csrftoken\": self.__csrf_token,\n",
    "        }\n",
    "\n",
    "        res = self.__session.request(method, url, headers=csrf_headers, **request_kwargs)  # type: ignore\n",
    "\n",
    "        logging.info(\"Request finished with status: %d\", res.status_code)\n",
    "\n",
    "        if res.status_code == 401 and res.json().get(\"msg\") == \"Token has expired\" and self.__refresh_session():\n",
    "            logging.info(f\"Retrying {method} request for {url} %s with refreshed session\")\n",
    "            res = self.__session.request(method, url, headers=csrf_headers, **request_kwargs)  # type: ignore\n",
    "\n",
    "            logging.info(\"Request finished with status: %d\", res.status_code)\n",
    "\n",
    "        if (\n",
    "            res.status_code == 400\n",
    "            and res.json()[\"message\"] == \"400 Bad Request: The CSRF session token is missing.\"\n",
    "            and self.__refresh_session()\n",
    "        ):\n",
    "            logging.info(f\"Retrying {method} request for {url} %s with refreshed session\")\n",
    "            res = self.__session.request(method, url, headers=csrf_headers, **request_kwargs)  # type: ignore\n",
    "            logging.info(f\"Request finished with status: {res.status_code}\")\n",
    "        res.raise_for_status()\n",
    "        return res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "superset = SupersetDBTSessionConnector()\n",
    "\n",
    "if Path(MANIFEST_PATH).is_file():\n",
    "    with open(MANIFEST_PATH) as f:\n",
    "        dbt_manifest = json.load(f)\n",
    "else:\n",
    "    raise Exception(\"No manifest found at path\")\n",
    "\n",
    "dbt_tables = get_tables_from_dbt(dbt_manifest, None)\n",
    "serving_dbt_models = [\n",
    "    dbt_tables[table] for table in dbt_tables if dbt_tables[table][\"schema\"] in (SERVING_SCHEMA, USER_SCHEMA)\n",
    "]\n",
    "datasources = []\n",
    "for model in serving_dbt_models:\n",
    "    datasources.append(\n",
    "        {\n",
    "            \"database_name\": SST_DATABASE_NAME,\n",
    "            \"datasource_name\": model[\"name\"],\n",
    "            \"datasource_type\": \"table\",\n",
    "            \"schema\": model[\"schema\"],\n",
    "        }\n",
    "    )\n",
    "datasources = {\"datasources\": datasources}\n",
    "superset.request(\"POST\", \"cachekey/invalidate\", json=datasources)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:file_log:\u001b[0m18:35:01.230275 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f056c6d88e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f056c86b1c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f056392d2a0>]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m11:35:01  Running with dbt=1.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stdout_log:\u001b[0m11:35:01  Running with dbt=1.5.1\n",
      "INFO:file_log:\n",
      "\n",
      "============================== 18:35:01.232916 | f61cc454-61a9-4a92-bf29-6ea9da20886c ==============================\n",
      "\u001b[0m18:35:01.232916 [info ] [MainThread]: Running with dbt=1.5.1\n",
      "DEBUG:file_log:\u001b[0m18:35:01.240733 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/vu/.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/vu/Desktop/Projects/Thesis/financial/dbt/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}\n",
      "DEBUG:file_log:\u001b[0m18:35:01.284703 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f61cc454-61a9-4a92-bf29-6ea9da20886c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f056c6d9210>]}\n",
      "DEBUG:file_log:\u001b[0m18:35:01.292314 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f61cc454-61a9-4a92-bf29-6ea9da20886c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05a03c8130>]}\n",
      "DEBUG:file_log:\u001b[0m18:35:01.395454 [debug] [MainThread]: checksum: 51f6b581eba8f8101bc020bf9faf8f96af641e2da86d581f66e2bdf0ff384b1c, vars: {}, profile: , target: , version: 1.5.1\n",
      "DEBUG:file_log:\u001b[0m18:35:01.610261 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.\n",
      "DEBUG:file_log:\u001b[0m18:35:01.612364 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing\n",
      "DEBUG:file_log:\u001b[0m18:35:01.637049 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f61cc454-61a9-4a92-bf29-6ea9da20886c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0561cacfd0>]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m11:35:01  Performance info: target/perf_info.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stdout_log:\u001b[0m11:35:01  Performance info: target/perf_info.json\n",
      "INFO:file_log:\u001b[0m18:35:01.640745 [info ] [MainThread]: Performance info: target/perf_info.json\n",
      "DEBUG:file_log:\u001b[0m18:35:01.943399 [debug] [MainThread]: Command `cli parse` succeeded at 18:35:01.942995 after 0.72 seconds\n",
      "DEBUG:file_log:\u001b[0m18:35:01.945565 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f056c6f7880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05637df190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0563d58160>]}\n",
      "DEBUG:file_log:\u001b[0m18:35:01.948197 [debug] [MainThread]: Flushing usage events\n",
      "DEBUG:file_log:\u001b[0m18:35:09.353751 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f056c9ed6c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f056c9ecac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f056c9ed120>]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m11:35:09  Running with dbt=1.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stdout_log:\u001b[0m11:35:09  Running with dbt=1.5.1\n",
      "INFO:file_log:\n",
      "\n",
      "============================== 18:35:09.356057 | b9056fb1-785d-43c0-a145-a3c42b9f014f ==============================\n",
      "\u001b[0m18:35:09.356057 [info ] [MainThread]: Running with dbt=1.5.1\n",
      "DEBUG:file_log:\u001b[0m18:35:09.360814 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/vu/.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/vu/Desktop/Projects/Thesis/financial/dbt/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}\n",
      "DEBUG:file_log:\u001b[0m18:35:09.405928 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b9056fb1-785d-43c0-a145-a3c42b9f014f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f056c9ec700>]}\n",
      "DEBUG:file_log:\u001b[0m18:35:09.415490 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b9056fb1-785d-43c0-a145-a3c42b9f014f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0561e63730>]}\n",
      "DEBUG:file_log:\u001b[0m18:35:09.521138 [debug] [MainThread]: checksum: 51f6b581eba8f8101bc020bf9faf8f96af641e2da86d581f66e2bdf0ff384b1c, vars: {}, profile: , target: , version: 1.5.1\n",
      "DEBUG:file_log:\u001b[0m18:35:09.691880 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.\n",
      "DEBUG:file_log:\u001b[0m18:35:09.694643 [debug] [MainThread]: Partial parsing: added file: dbt_financial://models/user/sda.sql\n",
      "DEBUG:file_log:\u001b[0m18:35:09.722633 [debug] [MainThread]: 1699: static parser successfully parsed user/sda.sql\n",
      "DEBUG:file_log:\u001b[0m18:35:09.787942 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b9056fb1-785d-43c0-a145-a3c42b9f014f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0562570bb0>]}\n",
      "DEBUG:file_log:\u001b[0m18:35:09.849427 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b9056fb1-785d-43c0-a145-a3c42b9f014f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05a03c9c00>]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m11:35:09  Found 32 models, 58 tests, 6 snapshots, 0 analyses, 760 macros, 0 operations, 0 seed files, 12 sources, 1 exposure, 0 metrics, 0 groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stdout_log:\u001b[0m11:35:09  Found 32 models, 58 tests, 6 snapshots, 0 analyses, 760 macros, 0 operations, 0 seed files, 12 sources, 1 exposure, 0 metrics, 0 groups\n",
      "INFO:file_log:\u001b[0m18:35:09.852087 [info ] [MainThread]: Found 32 models, 58 tests, 6 snapshots, 0 analyses, 760 macros, 0 operations, 0 seed files, 12 sources, 1 exposure, 0 metrics, 0 groups\n",
      "DEBUG:file_log:\u001b[0m18:35:09.857068 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b9056fb1-785d-43c0-a145-a3c42b9f014f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05a03cb580>]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m11:35:09  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stdout_log:\u001b[0m11:35:09  \n",
      "INFO:file_log:\u001b[0m18:35:09.863250 [info ] [MainThread]: \n",
      "DEBUG:file_log:\u001b[0m18:35:09.868617 [debug] [MainThread]: Acquiring new postgres connection 'master'\n",
      "DEBUG:file_log:\u001b[0m18:35:09.872451 [debug] [ThreadPool]: Acquiring new postgres connection 'list_financial_data'\n",
      "DEBUG:file_log:\u001b[0m18:35:09.902382 [debug] [ThreadPool]: Using postgres connection \"list_financial_data\"\n",
      "DEBUG:file_log:\u001b[0m18:35:09.910240 [debug] [ThreadPool]: On list_financial_data: /* {\"app\": \"dbt\", \"dbt_version\": \"1.5.1\", \"profile_name\": \"dbt_financial\", \"target_name\": \"dev_cloud\", \"connection_name\": \"list_financial_data\"} */\n",
      "\n",
      "    select distinct nspname from pg_namespace\n",
      "  \n",
      "DEBUG:file_log:\u001b[0m18:35:09.914329 [debug] [ThreadPool]: Opening a new connection, currently in state init\n",
      "DEBUG:file_log:\u001b[0m18:35:12.153724 [debug] [ThreadPool]: SQL status: SELECT 23 in 2.0 seconds\n",
      "DEBUG:file_log:\u001b[0m18:35:12.157800 [debug] [ThreadPool]: On list_financial_data: Close\n",
      "DEBUG:file_log:\u001b[0m18:35:12.166170 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_financial_data, now list_financial_data_staging)\n",
      "DEBUG:file_log:\u001b[0m18:35:12.168695 [debug] [ThreadPool]: Acquiring new postgres connection 'list_financial_data_marts'\n",
      "DEBUG:file_log:\u001b[0m18:35:12.184874 [debug] [ThreadPool]: Using postgres connection \"list_financial_data_staging\"\n",
      "DEBUG:file_log:\u001b[0m18:35:12.186998 [debug] [ThreadPool]: Acquiring new postgres connection 'list_financial_data_user'\n",
      "DEBUG:file_log:\u001b[0m18:35:12.189228 [debug] [ThreadPool]: Acquiring new postgres connection 'list_financial_data_intermediate'\n",
      "DEBUG:file_log:\u001b[0m18:35:12.198378 [debug] [ThreadPool]: Using postgres connection \"list_financial_data_marts\"\n",
      "DEBUG:file_log:\u001b[0m18:35:12.200823 [debug] [ThreadPool]: On list_financial_data_staging: BEGIN\n",
      "DEBUG:file_log:\u001b[0m18:35:12.214924 [debug] [ThreadPool]: Using postgres connection \"list_financial_data_user\"\n",
      "DEBUG:file_log:\u001b[0m18:35:12.237274 [debug] [ThreadPool]: Using postgres connection \"list_financial_data_intermediate\"\n",
      "DEBUG:file_log:\u001b[0m18:35:12.239309 [debug] [ThreadPool]: On list_financial_data_marts: BEGIN\n",
      "DEBUG:file_log:\u001b[0m18:35:12.243289 [debug] [ThreadPool]: Opening a new connection, currently in state closed\n",
      "DEBUG:file_log:\u001b[0m18:35:12.246043 [debug] [ThreadPool]: On list_financial_data_user: BEGIN\n",
      "DEBUG:file_log:\u001b[0m18:35:12.248494 [debug] [ThreadPool]: On list_financial_data_intermediate: BEGIN\n",
      "DEBUG:file_log:\u001b[0m18:35:12.250958 [debug] [ThreadPool]: Opening a new connection, currently in state init\n",
      "DEBUG:file_log:\u001b[0m18:35:12.254801 [debug] [ThreadPool]: Opening a new connection, currently in state init\n",
      "DEBUG:file_log:\u001b[0m18:35:12.257402 [debug] [ThreadPool]: Opening a new connection, currently in state init\n",
      "DEBUG:file_log:\u001b[0m18:35:14.119856 [debug] [ThreadPool]: SQL status: BEGIN in 2.0 seconds\n",
      "DEBUG:file_log:\u001b[0m18:35:14.122095 [debug] [ThreadPool]: Using postgres connection \"list_financial_data_intermediate\"\n",
      "DEBUG:file_log:\u001b[0m18:35:14.123087 [debug] [ThreadPool]: SQL status: BEGIN in 2.0 seconds\n",
      "DEBUG:file_log:\u001b[0m18:35:14.125680 [debug] [ThreadPool]: On list_financial_data_intermediate: /* {\"app\": \"dbt\", \"dbt_version\": \"1.5.1\", \"profile_name\": \"dbt_financial\", \"target_name\": \"dev_cloud\", \"connection_name\": \"list_financial_data_intermediate\"} */\n",
      "select\n",
      "      'financial_data' as database,\n",
      "      tablename as name,\n",
      "      schemaname as schema,\n",
      "      'table' as type\n",
      "    from pg_tables\n",
      "    where schemaname ilike 'intermediate'\n",
      "    union all\n",
      "    select\n",
      "      'financial_data' as database,\n",
      "      viewname as name,\n",
      "      schemaname as schema,\n",
      "      'view' as type\n",
      "    from pg_views\n",
      "    where schemaname ilike 'intermediate'\n",
      "  \n",
      "DEBUG:file_log:\u001b[0m18:35:14.127649 [debug] [ThreadPool]: Using postgres connection \"list_financial_data_staging\"\n",
      "DEBUG:file_log:\u001b[0m18:35:14.131871 [debug] [ThreadPool]: On list_financial_data_staging: /* {\"app\": \"dbt\", \"dbt_version\": \"1.5.1\", \"profile_name\": \"dbt_financial\", \"target_name\": \"dev_cloud\", \"connection_name\": \"list_financial_data_staging\"} */\n",
      "select\n",
      "      'financial_data' as database,\n",
      "      tablename as name,\n",
      "      schemaname as schema,\n",
      "      'table' as type\n",
      "    from pg_tables\n",
      "    where schemaname ilike 'staging'\n",
      "    union all\n",
      "    select\n",
      "      'financial_data' as database,\n",
      "      viewname as name,\n",
      "      schemaname as schema,\n",
      "      'view' as type\n",
      "    from pg_views\n",
      "    where schemaname ilike 'staging'\n",
      "  \n",
      "DEBUG:file_log:\u001b[0m18:35:14.136807 [debug] [ThreadPool]: SQL status: BEGIN in 2.0 seconds\n",
      "DEBUG:file_log:\u001b[0m18:35:14.138716 [debug] [ThreadPool]: Using postgres connection \"list_financial_data_marts\"\n",
      "DEBUG:file_log:\u001b[0m18:35:14.140723 [debug] [ThreadPool]: On list_financial_data_marts: /* {\"app\": \"dbt\", \"dbt_version\": \"1.5.1\", \"profile_name\": \"dbt_financial\", \"target_name\": \"dev_cloud\", \"connection_name\": \"list_financial_data_marts\"} */\n",
      "select\n",
      "      'financial_data' as database,\n",
      "      tablename as name,\n",
      "      schemaname as schema,\n",
      "      'table' as type\n",
      "    from pg_tables\n",
      "    where schemaname ilike 'marts'\n",
      "    union all\n",
      "    select\n",
      "      'financial_data' as database,\n",
      "      viewname as name,\n",
      "      schemaname as schema,\n",
      "      'view' as type\n",
      "    from pg_views\n",
      "    where schemaname ilike 'marts'\n",
      "  \n",
      "DEBUG:file_log:\u001b[0m18:35:14.227719 [debug] [ThreadPool]: SQL status: BEGIN in 2.0 seconds\n",
      "DEBUG:file_log:\u001b[0m18:35:14.230985 [debug] [ThreadPool]: Using postgres connection \"list_financial_data_user\"\n",
      "DEBUG:file_log:\u001b[0m18:35:14.234197 [debug] [ThreadPool]: On list_financial_data_user: /* {\"app\": \"dbt\", \"dbt_version\": \"1.5.1\", \"profile_name\": \"dbt_financial\", \"target_name\": \"dev_cloud\", \"connection_name\": \"list_financial_data_user\"} */\n",
      "select\n",
      "      'financial_data' as database,\n",
      "      tablename as name,\n",
      "      schemaname as schema,\n",
      "      'table' as type\n",
      "    from pg_tables\n",
      "    where schemaname ilike 'user'\n",
      "    union all\n",
      "    select\n",
      "      'financial_data' as database,\n",
      "      viewname as name,\n",
      "      schemaname as schema,\n",
      "      'view' as type\n",
      "    from pg_views\n",
      "    where schemaname ilike 'user'\n",
      "  \n",
      "DEBUG:file_log:\u001b[0m18:35:14.340509 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.0 seconds\n",
      "DEBUG:file_log:\u001b[0m18:35:14.344791 [debug] [ThreadPool]: On list_financial_data_intermediate: ROLLBACK\n",
      "DEBUG:file_log:\u001b[0m18:35:14.346438 [debug] [ThreadPool]: SQL status: SELECT 14 in 0.0 seconds\n",
      "DEBUG:file_log:\u001b[0m18:35:14.352662 [debug] [ThreadPool]: On list_financial_data_staging: ROLLBACK\n",
      "DEBUG:file_log:\u001b[0m18:35:14.353800 [debug] [ThreadPool]: SQL status: SELECT 28 in 0.0 seconds\n",
      "DEBUG:file_log:\u001b[0m18:35:14.362461 [debug] [ThreadPool]: On list_financial_data_marts: ROLLBACK\n",
      "DEBUG:file_log:\u001b[0m18:35:14.466972 [debug] [ThreadPool]: SQL status: SELECT 0 in 0.0 seconds\n",
      "DEBUG:file_log:\u001b[0m18:35:14.471306 [debug] [ThreadPool]: On list_financial_data_user: ROLLBACK\n",
      "DEBUG:file_log:\u001b[0m18:35:14.555007 [debug] [ThreadPool]: On list_financial_data_intermediate: Close\n",
      "DEBUG:file_log:\u001b[0m18:35:14.559506 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_financial_data_intermediate, now list_financial_data_snapshots)\n",
      "DEBUG:file_log:\u001b[0m18:35:14.564364 [debug] [ThreadPool]: On list_financial_data_staging: Close\n",
      "DEBUG:file_log:\u001b[0m18:35:14.571522 [debug] [ThreadPool]: Using postgres connection \"list_financial_data_snapshots\"\n",
      "DEBUG:file_log:\u001b[0m18:35:14.573104 [debug] [ThreadPool]: On list_financial_data_marts: Close\n",
      "DEBUG:file_log:\u001b[0m18:35:14.577217 [debug] [ThreadPool]: On list_financial_data_snapshots: BEGIN\n",
      "DEBUG:file_log:\u001b[0m18:35:14.602147 [debug] [ThreadPool]: Opening a new connection, currently in state closed\n",
      "DEBUG:file_log:\u001b[0m18:35:14.702326 [debug] [ThreadPool]: On list_financial_data_user: Close\n",
      "DEBUG:file_log:\u001b[0m18:35:16.796204 [debug] [ThreadPool]: SQL status: BEGIN in 2.0 seconds\n",
      "DEBUG:file_log:\u001b[0m18:35:16.798954 [debug] [ThreadPool]: Using postgres connection \"list_financial_data_snapshots\"\n",
      "DEBUG:file_log:\u001b[0m18:35:16.802150 [debug] [ThreadPool]: On list_financial_data_snapshots: /* {\"app\": \"dbt\", \"dbt_version\": \"1.5.1\", \"profile_name\": \"dbt_financial\", \"target_name\": \"dev_cloud\", \"connection_name\": \"list_financial_data_snapshots\"} */\n",
      "select\n",
      "      'financial_data' as database,\n",
      "      tablename as name,\n",
      "      schemaname as schema,\n",
      "      'table' as type\n",
      "    from pg_tables\n",
      "    where schemaname ilike 'snapshots'\n",
      "    union all\n",
      "    select\n",
      "      'financial_data' as database,\n",
      "      viewname as name,\n",
      "      schemaname as schema,\n",
      "      'view' as type\n",
      "    from pg_views\n",
      "    where schemaname ilike 'snapshots'\n",
      "  \n",
      "DEBUG:file_log:\u001b[0m18:35:17.036641 [debug] [ThreadPool]: SQL status: SELECT 0 in 0.0 seconds\n",
      "DEBUG:file_log:\u001b[0m18:35:17.040569 [debug] [ThreadPool]: On list_financial_data_snapshots: ROLLBACK\n",
      "DEBUG:file_log:\u001b[0m18:35:17.372342 [debug] [ThreadPool]: On list_financial_data_snapshots: Close\n",
      "DEBUG:file_log:\u001b[0m18:35:17.388399 [debug] [MainThread]: Using postgres connection \"master\"\n",
      "DEBUG:file_log:\u001b[0m18:35:17.390512 [debug] [MainThread]: On master: BEGIN\n",
      "DEBUG:file_log:\u001b[0m18:35:17.392768 [debug] [MainThread]: Opening a new connection, currently in state init\n",
      "DEBUG:file_log:\u001b[0m18:35:19.279934 [debug] [MainThread]: SQL status: BEGIN in 2.0 seconds\n",
      "DEBUG:file_log:\u001b[0m18:35:19.282509 [debug] [MainThread]: Using postgres connection \"master\"\n",
      "DEBUG:file_log:\u001b[0m18:35:19.285796 [debug] [MainThread]: On master: /* {\"app\": \"dbt\", \"dbt_version\": \"1.5.1\", \"profile_name\": \"dbt_financial\", \"target_name\": \"dev_cloud\", \"connection_name\": \"master\"} */\n",
      "with relation as (\n",
      "        select\n",
      "            pg_rewrite.ev_class as class,\n",
      "            pg_rewrite.oid as id\n",
      "        from pg_rewrite\n",
      "    ),\n",
      "    class as (\n",
      "        select\n",
      "            oid as id,\n",
      "            relname as name,\n",
      "            relnamespace as schema,\n",
      "            relkind as kind\n",
      "        from pg_class\n",
      "    ),\n",
      "    dependency as (\n",
      "        select distinct\n",
      "            pg_depend.objid as id,\n",
      "            pg_depend.refobjid as ref\n",
      "        from pg_depend\n",
      "    ),\n",
      "    schema as (\n",
      "        select\n",
      "            pg_namespace.oid as id,\n",
      "            pg_namespace.nspname as name\n",
      "        from pg_namespace\n",
      "        where nspname != 'information_schema' and nspname not like 'pg\\_%'\n",
      "    ),\n",
      "    referenced as (\n",
      "        select\n",
      "            relation.id AS id,\n",
      "            referenced_class.name ,\n",
      "            referenced_class.schema ,\n",
      "            referenced_class.kind\n",
      "        from relation\n",
      "        join class as referenced_class on relation.class=referenced_class.id\n",
      "        where referenced_class.kind in ('r', 'v')\n",
      "    ),\n",
      "    relationships as (\n",
      "        select\n",
      "            referenced.name as referenced_name,\n",
      "            referenced.schema as referenced_schema_id,\n",
      "            dependent_class.name as dependent_name,\n",
      "            dependent_class.schema as dependent_schema_id,\n",
      "            referenced.kind as kind\n",
      "        from referenced\n",
      "        join dependency on referenced.id=dependency.id\n",
      "        join class as dependent_class on dependency.ref=dependent_class.id\n",
      "        where\n",
      "            (referenced.name != dependent_class.name or\n",
      "             referenced.schema != dependent_class.schema)\n",
      "    )\n",
      "\n",
      "    select\n",
      "        referenced_schema.name as referenced_schema,\n",
      "        relationships.referenced_name as referenced_name,\n",
      "        dependent_schema.name as dependent_schema,\n",
      "        relationships.dependent_name as dependent_name\n",
      "    from relationships\n",
      "    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id\n",
      "    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id\n",
      "    group by referenced_schema, referenced_name, dependent_schema, dependent_name\n",
      "    order by referenced_schema, referenced_name, dependent_schema, dependent_name;\n",
      "DEBUG:file_log:\u001b[0m18:35:19.511508 [debug] [MainThread]: SQL status: SELECT 56 in 0.0 seconds\n",
      "DEBUG:file_log:\u001b[0m18:35:19.521030 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b9056fb1-785d-43c0-a145-a3c42b9f014f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f056252c340>]}\n",
      "DEBUG:file_log:\u001b[0m18:35:19.523467 [debug] [MainThread]: On master: ROLLBACK\n",
      "DEBUG:file_log:\u001b[0m18:35:19.780405 [debug] [MainThread]: Using postgres connection \"master\"\n",
      "DEBUG:file_log:\u001b[0m18:35:19.782422 [debug] [MainThread]: On master: BEGIN\n",
      "DEBUG:file_log:\u001b[0m18:35:20.382872 [debug] [MainThread]: SQL status: BEGIN in 1.0 seconds\n",
      "DEBUG:file_log:\u001b[0m18:35:20.384824 [debug] [MainThread]: On master: COMMIT\n",
      "DEBUG:file_log:\u001b[0m18:35:20.387254 [debug] [MainThread]: Using postgres connection \"master\"\n",
      "DEBUG:file_log:\u001b[0m18:35:20.389754 [debug] [MainThread]: On master: COMMIT\n",
      "DEBUG:file_log:\u001b[0m18:35:20.684034 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds\n",
      "DEBUG:file_log:\u001b[0m18:35:20.686818 [debug] [MainThread]: On master: Close\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m11:35:20  Concurrency: 4 threads (target='dev_cloud')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stdout_log:\u001b[0m11:35:20  Concurrency: 4 threads (target='dev_cloud')\n",
      "INFO:file_log:\u001b[0m18:35:20.690206 [info ] [MainThread]: Concurrency: 4 threads (target='dev_cloud')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m11:35:20  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stdout_log:\u001b[0m11:35:20  \n",
      "INFO:file_log:\u001b[0m18:35:20.694394 [info ] [MainThread]: \n",
      "DEBUG:file_log:\u001b[0m18:35:20.707133 [debug] [Thread-6 (]: Began running node model.dbt_financial.sda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m11:35:20  1 of 1 START sql table model user.sda .......................................... [RUN]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stdout_log:\u001b[0m11:35:20  1 of 1 START sql table model user.sda .......................................... [RUN]\n",
      "INFO:file_log:\u001b[0m18:35:20.709786 [info ] [Thread-6 (]: 1 of 1 START sql table model user.sda .......................................... [RUN]\n",
      "DEBUG:file_log:\u001b[0m18:35:20.717329 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly list_financial_data_snapshots, now model.dbt_financial.sda)\n",
      "DEBUG:file_log:\u001b[0m18:35:20.719512 [debug] [Thread-6 (]: Began compiling node model.dbt_financial.sda\n",
      "DEBUG:file_log:\u001b[0m18:35:20.729019 [debug] [Thread-6 (]: Writing injected SQL for node \"model.dbt_financial.sda\"\n",
      "DEBUG:file_log:\u001b[0m18:35:20.733397 [debug] [Thread-6 (]: Timing info for model.dbt_financial.sda (compile): 18:35:20.721290 => 18:35:20.732806\n",
      "DEBUG:file_log:\u001b[0m18:35:20.735772 [debug] [Thread-6 (]: Began executing node model.dbt_financial.sda\n",
      "DEBUG:file_log:\u001b[0m18:35:20.831073 [debug] [Thread-6 (]: Writing runtime sql for node \"model.dbt_financial.sda\"\n",
      "DEBUG:file_log:\u001b[0m18:35:20.834483 [debug] [Thread-6 (]: Using postgres connection \"model.dbt_financial.sda\"\n",
      "DEBUG:file_log:\u001b[0m18:35:20.836919 [debug] [Thread-6 (]: On model.dbt_financial.sda: BEGIN\n",
      "DEBUG:file_log:\u001b[0m18:35:20.839390 [debug] [Thread-6 (]: Opening a new connection, currently in state closed\n",
      "DEBUG:file_log:\u001b[0m18:35:23.048351 [debug] [Thread-6 (]: SQL status: BEGIN in 2.0 seconds\n",
      "DEBUG:file_log:\u001b[0m18:35:23.050378 [debug] [Thread-6 (]: Using postgres connection \"model.dbt_financial.sda\"\n",
      "DEBUG:file_log:\u001b[0m18:35:23.052525 [debug] [Thread-6 (]: On model.dbt_financial.sda: /* {\"app\": \"dbt\", \"dbt_version\": \"1.5.1\", \"profile_name\": \"dbt_financial\", \"target_name\": \"dev_cloud\", \"node_id\": \"model.dbt_financial.sda\"} */\n",
      "\n",
      "  \n",
      "    \n",
      "\n",
      "  create  table \"financial_data\".\"user\".\"sda__dbt_tmp\"\n",
      "  \n",
      "  \n",
      "    as\n",
      "  \n",
      "  (\n",
      "    \n",
      "\n",
      "-- depends_on: fact_financial_health_rating\n",
      "\n",
      "SELECT * from marts.fact_financial_health_rating\n",
      "  );\n",
      "  \n",
      "DEBUG:file_log:\u001b[0m18:35:23.769415 [debug] [Thread-6 (]: SQL status: SELECT 5 in 1.0 seconds\n",
      "DEBUG:file_log:\u001b[0m18:35:23.785846 [debug] [Thread-6 (]: Using postgres connection \"model.dbt_financial.sda\"\n",
      "DEBUG:file_log:\u001b[0m18:35:23.788404 [debug] [Thread-6 (]: On model.dbt_financial.sda: /* {\"app\": \"dbt\", \"dbt_version\": \"1.5.1\", \"profile_name\": \"dbt_financial\", \"target_name\": \"dev_cloud\", \"node_id\": \"model.dbt_financial.sda\"} */\n",
      "alter table \"financial_data\".\"user\".\"sda__dbt_tmp\" rename to \"sda\"\n",
      "DEBUG:file_log:\u001b[0m18:35:24.096228 [debug] [Thread-6 (]: SQL status: ALTER TABLE in 0.0 seconds\n",
      "DEBUG:file_log:\u001b[0m18:35:24.142567 [debug] [Thread-6 (]: On model.dbt_financial.sda: COMMIT\n",
      "DEBUG:file_log:\u001b[0m18:35:24.146472 [debug] [Thread-6 (]: Using postgres connection \"model.dbt_financial.sda\"\n",
      "DEBUG:file_log:\u001b[0m18:35:24.148707 [debug] [Thread-6 (]: On model.dbt_financial.sda: COMMIT\n",
      "DEBUG:file_log:\u001b[0m18:35:24.379153 [debug] [Thread-6 (]: SQL status: COMMIT in 0.0 seconds\n",
      "DEBUG:file_log:\u001b[0m18:35:24.394144 [debug] [Thread-6 (]: Using postgres connection \"model.dbt_financial.sda\"\n",
      "DEBUG:file_log:\u001b[0m18:35:24.396247 [debug] [Thread-6 (]: On model.dbt_financial.sda: /* {\"app\": \"dbt\", \"dbt_version\": \"1.5.1\", \"profile_name\": \"dbt_financial\", \"target_name\": \"dev_cloud\", \"node_id\": \"model.dbt_financial.sda\"} */\n",
      "drop table if exists \"financial_data\".\"user\".\"sda__dbt_backup\" cascade\n",
      "DEBUG:file_log:\u001b[0m18:35:24.697918 [debug] [Thread-6 (]: SQL status: DROP TABLE in 0.0 seconds\n",
      "DEBUG:file_log:\u001b[0m18:35:24.703267 [debug] [Thread-6 (]: Timing info for model.dbt_financial.sda (execute): 18:35:20.737924 => 18:35:24.702552\n",
      "DEBUG:file_log:\u001b[0m18:35:24.706349 [debug] [Thread-6 (]: On model.dbt_financial.sda: Close\n",
      "DEBUG:file_log:\u001b[0m18:35:24.709867 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b9056fb1-785d-43c0-a145-a3c42b9f014f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05a03c81c0>]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m11:35:24  1 of 1 OK created sql table model user.sda ..................................... [\u001b[32mSELECT 5\u001b[0m in 3.99s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stdout_log:\u001b[0m11:35:24  1 of 1 OK created sql table model user.sda ..................................... [\u001b[32mSELECT 5\u001b[0m in 3.99s]\n",
      "INFO:file_log:\u001b[0m18:35:24.712478 [info ] [Thread-6 (]: 1 of 1 OK created sql table model user.sda ..................................... [\u001b[32mSELECT 5\u001b[0m in 3.99s]\n",
      "DEBUG:file_log:\u001b[0m18:35:24.718212 [debug] [Thread-6 (]: Finished running node model.dbt_financial.sda\n",
      "DEBUG:file_log:\u001b[0m18:35:24.723510 [debug] [MainThread]: Using postgres connection \"master\"\n",
      "DEBUG:file_log:\u001b[0m18:35:24.725744 [debug] [MainThread]: On master: BEGIN\n",
      "DEBUG:file_log:\u001b[0m18:35:24.728261 [debug] [MainThread]: Opening a new connection, currently in state closed\n",
      "DEBUG:file_log:\u001b[0m18:35:27.006083 [debug] [MainThread]: SQL status: BEGIN in 2.0 seconds\n",
      "DEBUG:file_log:\u001b[0m18:35:27.008207 [debug] [MainThread]: On master: COMMIT\n",
      "DEBUG:file_log:\u001b[0m18:35:27.010247 [debug] [MainThread]: Using postgres connection \"master\"\n",
      "DEBUG:file_log:\u001b[0m18:35:27.012387 [debug] [MainThread]: On master: COMMIT\n",
      "DEBUG:file_log:\u001b[0m18:35:27.307126 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds\n",
      "DEBUG:file_log:\u001b[0m18:35:27.309106 [debug] [MainThread]: On master: Close\n",
      "DEBUG:file_log:\u001b[0m18:35:27.313294 [debug] [MainThread]: Connection 'master' was properly closed.\n",
      "DEBUG:file_log:\u001b[0m18:35:27.317192 [debug] [MainThread]: Connection 'list_financial_data_staging' was properly closed.\n",
      "DEBUG:file_log:\u001b[0m18:35:27.319979 [debug] [MainThread]: Connection 'list_financial_data_marts' was properly closed.\n",
      "DEBUG:file_log:\u001b[0m18:35:27.323490 [debug] [MainThread]: Connection 'list_financial_data_user' was properly closed.\n",
      "DEBUG:file_log:\u001b[0m18:35:27.327353 [debug] [MainThread]: Connection 'model.dbt_financial.sda' was properly closed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m11:35:27  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stdout_log:\u001b[0m11:35:27  \n",
      "INFO:file_log:\u001b[0m18:35:27.331547 [info ] [MainThread]: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m11:35:27  Finished running 1 table model in 0 hours 0 minutes and 17.46 seconds (17.46s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stdout_log:\u001b[0m11:35:27  Finished running 1 table model in 0 hours 0 minutes and 17.46 seconds (17.46s).\n",
      "INFO:file_log:\u001b[0m18:35:27.343097 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 17.46 seconds (17.46s).\n",
      "DEBUG:file_log:\u001b[0m18:35:27.350104 [debug] [MainThread]: Command end result\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m11:35:27  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stdout_log:\u001b[0m11:35:27  \n",
      "INFO:file_log:\u001b[0m18:35:27.411133 [info ] [MainThread]: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m11:35:27  \u001b[32mCompleted successfully\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stdout_log:\u001b[0m11:35:27  \u001b[32mCompleted successfully\u001b[0m\n",
      "INFO:file_log:\u001b[0m18:35:27.416686 [info ] [MainThread]: \u001b[32mCompleted successfully\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m11:35:27  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stdout_log:\u001b[0m11:35:27  \n",
      "INFO:file_log:\u001b[0m18:35:27.422582 [info ] [MainThread]: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m11:35:27  Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stdout_log:\u001b[0m11:35:27  Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1\n",
      "INFO:file_log:\u001b[0m18:35:27.429810 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1\n",
      "DEBUG:file_log:\u001b[0m18:35:27.436760 [debug] [MainThread]: Command `cli run` succeeded at 18:35:27.436365 after 18.09 seconds\n",
      "DEBUG:file_log:\u001b[0m18:35:27.438982 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0562330610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0561e63730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05a03c8d00>]}\n",
      "DEBUG:file_log:\u001b[0m18:35:27.441645 [debug] [MainThread]: Flushing usage events\n"
     ]
    }
   ],
   "source": [
    "df, succeeded = get_records()\n",
    "\n",
    "# if df.empty:\n",
    "#     logging.info(\"Early stopping because no records\")\n",
    "#     return \"Early stopping because no records\"\n",
    "\n",
    "for filename in os.listdir(USER_MODEL_PATH):\n",
    "    # If file is not present in list\n",
    "    if filename not in succeeded:\n",
    "        # Get full path of file and remove it\n",
    "        full_file_path = os.path.join(USER_MODEL_PATH, filename)\n",
    "        if os.path.isfile(full_file_path):\n",
    "            os.remove(full_file_path)\n",
    "\n",
    "dbt = dbtRunner()\n",
    "cli_args = [\n",
    "    \"parse\",\n",
    "    \"--project-dir\",\n",
    "    DBT_PROJECT_DIR,\n",
    "]\n",
    "res: dbtRunnerResult = dbt.invoke(cli_args)\n",
    "if not res.success:\n",
    "    raise Exception(\"Unable to parse project.\")\n",
    "# Get dagster execution time, see: https://stackoverflow.com/questions/75099470/getting-current-execution-date-in-a-task-or-asset-in-dagster\n",
    "EXEC_TIME = datetime.datetime.today().strftime(\"%d/%m/%Y_%H:%M:%S\")\n",
    "# raise Exception(DBT_PROJECT_DIR, MANIFEST_PATH, USER_MODEL_PATH)\n",
    "# Get all schema names in project\n",
    "# Either this or defined schema name available to the user before\n",
    "with open('target/manifest.json') as f:\n",
    "    dbt_manifest = json.load(f)\n",
    "    dbt_tables = get_tables_from_dbt(dbt_manifest, None)\n",
    "\n",
    "# Getting the dbt tables keys\n",
    "dbt_tables_names = list(dbt_tables.keys())\n",
    "mapped = map(lambda x: x.startswith((SERVING_SCHEMA, USER_SCHEMA)), dbt_tables_names)\n",
    "mask = list(mapped)\n",
    "\n",
    "dbt_tables_reporting = list(compress(dbt_tables_names, mask))\n",
    "\n",
    "dbt_names_aliases = [dbt_tables[table][\"name\"] for table in dbt_tables] + [\n",
    "    dbt_tables[table][\"alias\"] for table in dbt_tables\n",
    "]  # Name and aliases wo schema\n",
    "\n",
    "status = {}  # Status of preliminary checking\n",
    "for i in df.index:\n",
    "    # Check name validity\n",
    "    name_validation = is_valid_table_name(df.loc[i][\"name\"])\n",
    "    if not name_validation:\n",
    "        status[i] = \"Invalid name\"\n",
    "        df.loc[i, \"success\"] = False\n",
    "        continue\n",
    "    name_unique = is_unique_table_name(df.loc[i][\"name\"], dbt_names_aliases)  # check aliases and name\n",
    "    if not name_unique:\n",
    "        status[i] = \"Model name is duplicated with another existing model\"\n",
    "        df.loc[i, \"success\"] = False\n",
    "        continue\n",
    "    # Check syntax\n",
    "    query_string = df.loc[i][\"query_string\"]\n",
    "    query_string = query_string + \";\" if query_string[-1] != \";\" else query_string\n",
    "    validation = check_string(query_string)\n",
    "    if not validation[0]:\n",
    "        df.loc[i, \"success\"] = False\n",
    "        status[i] = \"Invalid query: {error}\".format(error=validation[1])\n",
    "        continue\n",
    "    # Check multi-query\n",
    "    parsed = sqlfluff.parse(query_string, \"postgres\")[\"file\"]\n",
    "    if type(parsed) == list:\n",
    "        df.loc[i, \"success\"] = False\n",
    "        status[i] = \"Multiple statement\"\n",
    "        continue\n",
    "    # Check select statements\n",
    "    # if list(statement_list[0][\"statement\"].keys())[0] != \"select_statement\":\n",
    "    #     df.loc[i, \"success\"] = False\n",
    "    #     status[i] = (\"Query is not 'SELECT'\")\n",
    "    #     continue\n",
    "    # Check tables and add model ref\n",
    "    ref_tables, processed_status = get_ref(df.loc[i, \"query_string\"], dbt_tables, parsed, dbt_tables_reporting)\n",
    "    if processed_status != \"Success\":\n",
    "        df.loc[i, \"success\"] = False\n",
    "        status[i] = processed_status\n",
    "        continue\n",
    "    model_path = USER_MODEL_PATH + \"/{name}.sql\".format(name=df.loc[i, \"name\"])\n",
    "    if os.path.exists(model_path):\n",
    "        status[i] = \"Model name is duplicated with another in processing batch\"\n",
    "        df.loc[i, \"success\"] = False\n",
    "        continue\n",
    "    with open(model_path, \"w+\") as f:\n",
    "        template_output = MODEL_TEMPLATE.render(\n",
    "            materialization=MATERIALIZATION_MAPPING[df.loc[i, \"materialization\"]],\n",
    "            desc=df.loc[i, \"description\"],\n",
    "            user_id=str(df.loc[i, \"user_id\"]),\n",
    "            exec_time=EXEC_TIME,\n",
    "            schema=USER_SCHEMA,\n",
    "            refs=ref_tables,\n",
    "            query=df.loc[i, \"query_string\"],\n",
    "        )\n",
    "        f.write(template_output)\n",
    "        logging.info(\"Wrote model {name} contents\".format(name=df.loc[i, \"name\"]))\n",
    "        f.close()\n",
    "    status[i] = \"Success\"\n",
    "# Get Emails from API\n",
    "superset = SupersetDBTSessionConnector()\n",
    "users = set(df[\"user_id\"].to_list())\n",
    "email_list = get_emails(superset, users)\n",
    "email_dict = {key: value for element_dict in email_list for key, value in element_dict.items()}\n",
    "\n",
    "SMTP.login(EMAIL_SENDER, EMAIL_PASSWORD)\n",
    "\n",
    "for i in df.index:\n",
    "    # Check Success\n",
    "    if df.loc[i, \"success\"] == False:\n",
    "        message = get_mail_content(df.loc[i, \"name\"], df.loc[i, \"query_string\"], status[i])\n",
    "        # Add checked\n",
    "        df.loc[i, \"checked\"] = True\n",
    "        SMTP.sendmail(EMAIL_SENDER, email_dict[str(df.loc[i, \"user_id\"])], message)\n",
    "\n",
    "# If every record is unsuccesful, terminate script early\n",
    "# if not df[\"success\"].any():\n",
    "#     update_records(df)\n",
    "#     logging.info(\"Early stopping because no successful records\")\n",
    "#     return \"Early stopping because no successful records\"\n",
    "\n",
    "sst_datasets = get_physical_datasets_from_superset(superset, DATABASE_ID)\n",
    "sst_user_tables = [table[\"name\"] for table in sst_datasets if table[\"schema\"] == USER_SCHEMA]\n",
    "\n",
    "# initialize\n",
    "dbt = dbtRunner()\n",
    "\n",
    "# create CLI args as a list of strings\n",
    "cli_args = [\n",
    "    \"run\",\n",
    "    \"--project-dir\",\n",
    "    DBT_PROJECT_DIR,\n",
    "    \"--select\",\n",
    "    \"tag:{exec_time}\".format(exec_time=EXEC_TIME),\n",
    "]\n",
    "\n",
    "# run the command\n",
    "res: dbtRunnerResult = dbt.invoke(cli_args)\n",
    "\n",
    "# inspect the results\n",
    "for r in res.result:\n",
    "    logging.info(f\"dbt run result: {r.node.name}: {r.status}\")\n",
    "# Map df index to result\n",
    "dbt_res_df_map = {}\n",
    "\n",
    "for i in df.index:\n",
    "    for r in res.result:\n",
    "        if r.node.name == df.loc[i, \"name\"]:\n",
    "            dbt_res_df_map[i] = r\n",
    "            break\n",
    "\n",
    "for i in df.index:\n",
    "    # Check Success\n",
    "    if df.loc[i, \"success\"] is not False:\n",
    "        if dbt_res_df_map[i].status == \"success\" and df.loc[i, \"name\"] not in sst_user_tables:\n",
    "            df.loc[i, \"success\"] = True\n",
    "            rison_request = \"/dataset/\"\n",
    "            # Data to be written\n",
    "            dictionary = {\n",
    "                # Parameter database\n",
    "                \"database\": DATABASE_ID,\n",
    "                \"schema\": USER_SCHEMA,\n",
    "                \"table_name\": df.loc[i, \"name\"],\n",
    "                \"owners\": [int(df.loc[i, \"user_id\"]), SUPERSET_ID],\n",
    "            }\n",
    "            # Serializing json\n",
    "            json_object = json.dumps(dictionary)\n",
    "            response = superset.request(\"POST\", rison_request, json=dictionary)\n",
    "\n",
    "            message = get_mail_content(df.loc[i, \"name\"], df.loc[i, \"query_string\"], \"dbt success\")\n",
    "\n",
    "        else:\n",
    "            df.loc[i, \"success\"] = False\n",
    "            message = get_mail_content(\n",
    "                df.loc[i, \"name\"], df.loc[i, \"query_string\"], \"dbt fail\", dbt_res_df_map[i].message\n",
    "            )\n",
    "        # Add checked\n",
    "        df.loc[i, \"checked\"] = True\n",
    "\n",
    "        SMTP.sendmail(EMAIL_SENDER, email_dict[str(df.loc[i, \"user_id\"])], message)\n",
    "\n",
    "SMTP.quit()\n",
    "# Delete unsucessful model\n",
    "for i in df.index:\n",
    "    # Check Success\n",
    "    if not df.loc[i, \"success\"]:\n",
    "        full_file_path = os.path.join(USER_MODEL_PATH, \"{name}.sql\".format(name=df.loc[i, \"name\"]))\n",
    "        if os.path.isfile(full_file_path):\n",
    "            os.remove(full_file_path)\n",
    "\n",
    "update_records(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "update_records(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 126,\n",
       "  'name': 'dim_organization',\n",
       "  'schema': 'marts',\n",
       "  'database': 'FDP Reader',\n",
       "  'dataset_id': 126,\n",
       "  'key': 'marts.dim_organization',\n",
       "  'table': ['marts.dim_organization']},\n",
       " {'id': 160,\n",
       "  'name': 'fact_price_history',\n",
       "  'schema': 'marts',\n",
       "  'database': 'FDP Reader',\n",
       "  'dataset_id': 160,\n",
       "  'key': 'marts.fact_price_history',\n",
       "  'table': ['marts.fact_price_history']},\n",
       " {'id': 164,\n",
       "  'name': 'fact_bollinger',\n",
       "  'schema': 'marts',\n",
       "  'database': 'FDP Reader',\n",
       "  'dataset_id': 164,\n",
       "  'key': 'marts.fact_bollinger',\n",
       "  'table': ['marts.fact_bollinger']},\n",
       " {'id': 165,\n",
       "  'name': 'fact_industry_health_rating',\n",
       "  'schema': 'marts',\n",
       "  'database': 'FDP Reader',\n",
       "  'dataset_id': 165,\n",
       "  'key': 'marts.fact_industry_health_rating',\n",
       "  'table': ['marts.fact_industry_health_rating']},\n",
       " {'id': 166,\n",
       "  'name': 'fact_cash_flow',\n",
       "  'schema': 'marts',\n",
       "  'database': 'FDP Reader',\n",
       "  'dataset_id': 166,\n",
       "  'key': 'marts.fact_cash_flow',\n",
       "  'table': ['marts.fact_cash_flow']},\n",
       " {'id': 167,\n",
       "  'name': 'fact_valuation_rating',\n",
       "  'schema': 'marts',\n",
       "  'database': 'FDP Reader',\n",
       "  'dataset_id': 167,\n",
       "  'key': 'marts.fact_valuation_rating',\n",
       "  'table': ['marts.fact_valuation_rating']},\n",
       " {'id': 168,\n",
       "  'name': 'fact_balance_sheet',\n",
       "  'schema': 'marts',\n",
       "  'database': 'FDP Reader',\n",
       "  'dataset_id': 168,\n",
       "  'key': 'marts.fact_balance_sheet',\n",
       "  'table': ['marts.fact_balance_sheet']},\n",
       " {'id': 169,\n",
       "  'name': 'fact_financial_health_rating',\n",
       "  'schema': 'marts',\n",
       "  'database': 'FDP Reader',\n",
       "  'dataset_id': 169,\n",
       "  'key': 'marts.fact_financial_health_rating',\n",
       "  'table': ['marts.fact_financial_health_rating']},\n",
       " {'id': 170,\n",
       "  'name': 'fact_stock_intraday',\n",
       "  'schema': 'marts',\n",
       "  'database': 'FDP Reader',\n",
       "  'dataset_id': 170,\n",
       "  'key': 'marts.fact_stock_intraday',\n",
       "  'table': ['marts.fact_stock_intraday']},\n",
       " {'id': 171,\n",
       "  'name': 'fact_income_statement',\n",
       "  'schema': 'marts',\n",
       "  'database': 'FDP Reader',\n",
       "  'dataset_id': 171,\n",
       "  'key': 'marts.fact_income_statement',\n",
       "  'table': ['marts.fact_income_statement']},\n",
       " {'id': 172,\n",
       "  'name': 'fact_mfi',\n",
       "  'schema': 'marts',\n",
       "  'database': 'FDP Reader',\n",
       "  'dataset_id': 172,\n",
       "  'key': 'marts.fact_mfi',\n",
       "  'table': ['marts.fact_mfi']},\n",
       " {'id': 173,\n",
       "  'name': 'fact_business_model_rating',\n",
       "  'schema': 'marts',\n",
       "  'database': 'FDP Reader',\n",
       "  'dataset_id': 173,\n",
       "  'key': 'marts.fact_business_model_rating',\n",
       "  'table': ['marts.fact_business_model_rating']},\n",
       " {'id': 174,\n",
       "  'name': 'fact_business_operation_rating',\n",
       "  'schema': 'marts',\n",
       "  'database': 'FDP Reader',\n",
       "  'dataset_id': 174,\n",
       "  'key': 'marts.fact_business_operation_rating',\n",
       "  'table': ['marts.fact_business_operation_rating']},\n",
       " {'id': 175,\n",
       "  'name': 'fact_bov',\n",
       "  'schema': 'marts',\n",
       "  'database': 'FDP Reader',\n",
       "  'dataset_id': 175,\n",
       "  'key': 'marts.fact_bov',\n",
       "  'table': ['marts.fact_bov']},\n",
       " {'id': 176,\n",
       "  'name': 'fact_general_rating',\n",
       "  'schema': 'marts',\n",
       "  'database': 'FDP Reader',\n",
       "  'dataset_id': 176,\n",
       "  'key': 'marts.fact_general_rating',\n",
       "  'table': ['marts.fact_general_rating']}]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sst_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['marts.fact_bollinger', 'marts.fact_bov', 'marts.fact_mfi', 'marts.fact_business_operation_rating', 'marts.fact_valuation_rating', 'marts.fact_business_model_rating', 'marts.fact_industry_health_rating', 'marts.fact_general_rating', 'marts.fact_financial_health_rating', 'marts.fact_price_history', 'marts.fact_cash_flow', 'marts.fact_stock_intraday', 'marts.fact_balance_sheet', 'marts.fact_income_statement', 'marts.dim_organization']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['marts.balance_sheet']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tables_from_sql(\"select * from marts.balance_sheet\", \"postgres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_query=\"select * from marts.dim_balance_sheet\"\n",
    "parsed_result = sqlfluff.parse(original_query)\n",
    "dbt_tables_names = list(dbt_tables.keys())\n",
    "mapped = map(lambda x: x.startswith((SERVING_SCHEMA, USER_SCHEMA)), dbt_tables_names)\n",
    "mask = list(mapped)\n",
    "\n",
    "serving_tables_names = list(compress(dbt_tables_names, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'marts.balance_sheet'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(['marts.balance_sheet']).difference(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables referenced out of serving schemas\n"
     ]
    }
   ],
   "source": [
    "fixed_query = str(original_query)\n",
    "table_names = set(get_tables_from_sql(fixed_query, dialect=\"postgres\", sql_parsed=parsed_result))\n",
    "fixed_query = sqlfluff.fix(fixed_query, dialect=\"postgres\")\n",
    "dbt_set = set(serving_tables_names)\n",
    "if not table_names.issubset(dbt_set):  # serving_tables_names include schema\n",
    "    print(\"Tables referenced out of serving schemas\")\n",
    "# Put tables in subqueries\n",
    "final_tables = tuple(table_names.intersection(serving_tables_names))  # Filter out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'marts.dim_balance_sheet'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'marts.dim_organization',\n",
       " 'marts.fact_balance_sheet',\n",
       " 'marts.fact_bollinger',\n",
       " 'marts.fact_bov',\n",
       " 'marts.fact_business_model_rating',\n",
       " 'marts.fact_business_operation_rating',\n",
       " 'marts.fact_cash_flow',\n",
       " 'marts.fact_financial_health_rating',\n",
       " 'marts.fact_general_rating',\n",
       " 'marts.fact_income_statement',\n",
       " 'marts.fact_industry_health_rating',\n",
       " 'marts.fact_mfi',\n",
       " 'marts.fact_price_history',\n",
       " 'marts.fact_stock_intraday',\n",
       " 'marts.fact_valuation_rating'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbt_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_raw = list(get_json_segment(parsed_result, \"table_reference\"))  # type: ignore\n",
    "tables_cleaned = []  # With schema\n",
    "for table_ref in tables_raw:\n",
    "    if isinstance(table_ref, list):\n",
    "        table_ref_identifier = []\n",
    "        # Get last 2 \"naked_identifier\"\n",
    "        for dictionary in table_ref[::-1]:\n",
    "            if \"naked_identifier\" in dictionary:\n",
    "                table_ref_identifier.append(dictionary[\"naked_identifier\"])\n",
    "                if len(table_ref_identifier) == 2:\n",
    "                    tables_cleaned.append(\".\".join(table_ref_identifier[::-1]))\n",
    "                    break\n",
    "    if isinstance(table_ref, dict):\n",
    "        tables_cleaned.append(table_ref[\"naked_identifier\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'naked_identifier': 'marts'},\n",
       "  {'dot': '.'},\n",
       "  {'naked_identifier': 'balance_sheet'}]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   id name                           query_string  user_id  materialization  \\\n",
       " 0  39    a  SELECT * from marts.dim_balance_sheet        1                1   \n",
       " \n",
       "   description                insert_time  checked success  \n",
       " 0             2023-07-30 08:43:18.845008    False    None  ,\n",
       " [])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries_to_update=str(tuple(zip(df.checked, df.success, df.id))).replace(\"None\", \"Null\")[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if entries_to_update[-1]==\",\": entries_to_update=entries_to_update[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(False, Null, 39)'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entries_to_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m08:47:07  Running with dbt=1.5.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m08:47:08  Unable to do partial parsing because profile has changed\n",
      "\u001b[0m08:47:13  [\u001b[33mWARNING\u001b[0m]: Test 'test.dbt_financial.source_relationships_source_cash_flow_ticker__ticker__source_organization_organization_.e869812b91' (models/staging/src_tcbs.yml) depends on a source named 'organization.organization' in package '' which was not found\n",
      "\u001b[0m08:47:13  [\u001b[33mWARNING\u001b[0m]: Test 'test.dbt_financial.source_relationships_source_balance_sheet_ticker__ticker__source_organization_organization_.3c28133061' (models/staging/src_tcbs.yml) depends on a source named 'organization.organization' in package '' which was not found\n",
      "\u001b[0m08:47:13  [\u001b[33mWARNING\u001b[0m]: Test 'test.dbt_financial.source_relationships_source_income_statement_ticker__ticker__source_organization_organization_.fddbfd9ba7' (models/staging/src_tcbs.yml) depends on a source named 'organization.organization' in package '' which was not found\n",
      "\u001b[0m08:47:13  [\u001b[33mWARNING\u001b[0m]: Test 'test.dbt_financial.source_relationships_source_price_history_ticker__ticker__source_organization_organization_.e620f5b6d7' (models/staging/src_tcbs.yml) depends on a source named 'organization.organization' in package '' which was not found\n",
      "\u001b[0m08:47:13  [\u001b[33mWARNING\u001b[0m]: Test 'test.dbt_financial.source_relationships_source_stock_intraday_ticker__ticker__source_organization_organization_.5b349e6c3f' (models/staging/src_tcbs.yml) depends on a source named 'organization.organization' in package '' which was not found\n",
      "\u001b[0m08:47:13  [\u001b[33mWARNING\u001b[0m]: Test 'test.dbt_financial.source_relationships_source_business_model_rating_ticker__ticker__source_organization_organization_.bc90162f3d' (models/staging/src_tcbs.yml) depends on a source named 'organization.organization' in package '' which was not found\n",
      "\u001b[0m08:47:13  [\u001b[33mWARNING\u001b[0m]: Test 'test.dbt_financial.source_relationships_source_business_operation_rating_ticker__ticker__source_organization_organization_.9392e2592b' (models/staging/src_tcbs.yml) depends on a source named 'organization.organization' in package '' which was not found\n",
      "\u001b[0m08:47:13  [\u001b[33mWARNING\u001b[0m]: Test 'test.dbt_financial.source_relationships_source_financial_health_rating_ticker__ticker__source_organization_organization_.8eea5b5f7a' (models/staging/src_tcbs.yml) depends on a source named 'organization.organization' in package '' which was not found\n",
      "\u001b[0m08:47:13  [\u001b[33mWARNING\u001b[0m]: Test 'test.dbt_financial.source_relationships_source_valuation_rating_ticker__ticker__source_organization_organization_.f828a4c8ec' (models/staging/src_tcbs.yml) depends on a source named 'organization.organization' in package '' which was not found\n",
      "\u001b[0m08:47:13  [\u001b[33mWARNING\u001b[0m]: Test 'test.dbt_financial.source_relationships_source_industry_health_rating_ticker__ticker__source_organization_organization_.d1e2fa7f14' (models/staging/src_tcbs.yml) depends on a source named 'organization.organization' in package '' which was not found\n",
      "\u001b[0m08:47:13  Performance info: target/perf_info.json\n"
     ]
    }
   ],
   "source": [
    "dbt = dbtRunner()\n",
    "cli_args = [\n",
    "    \"parse\",\n",
    "    \"--project-dir\",\n",
    "    DBT_PROJECT_DIR,\n",
    "]\n",
    "res: dbtRunnerResult = dbt.invoke(cli_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/vu/Desktop/Projects/Thesis/financial/dagster/financial/superset_utils'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "force\": true,\n",
    "  \"thumb_size\": [\n",
    "    0\n",
    "  ],\n",
    "  \"window_size\": [\n",
    "    0\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "superset.request(\"GET\",f'chart/1/cache_screenshot/?q={{\"page\":{page_number},\"page_size\":100}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "superset = SupersetDBTSessionConnector()\n",
    "datasources = {\n",
    "    \"datasources\": [\n",
    "        {\n",
    "            \"database_name\": \"FDP Reader\",\n",
    "            \"datasource_name\": \"marts.fact_price_history\",\n",
    "            \"datasource_type\": \"table\",\n",
    "            \"schema\": \"marts\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "superset.request(\"POST\", \"cachekey/invalidate\", json=datasources)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "#### ADD ALL OF THE END POINT USED TO CSRF EXEMPT LIST TO RUN PARALLELY\n",
    "#### ONLY USE SESSION FOR SEQUENTIAL RUNNING SCRIPTS\n",
    "\n",
    "\n",
    "class SupersetDBTSessionConnector:\n",
    "    \"\"\"A class for accessing the Superset API in an easy way.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Instantiates the class.\n",
    "\n",
    "        ''access_token'' will be instantiated via enviromental variable\n",
    "        If ``access_token`` is None, attempts to obtain it using ``refresh_token``.\n",
    "\n",
    "        Args:\n",
    "            api_url: Base API URL of a Superset instance, e.g. https://my-superset/api/v1.\n",
    "            access_token: Access token to use for accessing protected endpoints of the Superset\n",
    "                API. Can be automatically obtained if ``refresh_token`` is not None.\n",
    "            refresh_token: Refresh token to use for obtaining or refreshing the ``access_token``.\n",
    "                If None, no refresh will be done.\n",
    "        \"\"\"\n",
    "        self.__url = SUPERSET_HOST\n",
    "        self.__api_url = self.__url + \"api/v1/\"\n",
    "\n",
    "        self.__session = requests.session()\n",
    "\n",
    "        self.__username = SUPERSET_USERNAME\n",
    "        self.__password = SUPERSET_PASSWORD\n",
    "\n",
    "        self.__refresh_session()\n",
    "\n",
    "    def __refresh_session(self):\n",
    "        logging.info(\"Refreshing session\")\n",
    "\n",
    "        soup = BeautifulSoup(self.__session.post(self.__url + \"login\").text, \"html.parser\")\n",
    "        self.__csrf_token = soup.find(\"input\", {\"id\": \"csrf_token\"})[\"value\"]  # type: ignore\n",
    "\n",
    "        data = {\n",
    "            \"username\": self.__username,\n",
    "            \"password\": self.__password,\n",
    "            \"provider\": \"db\",\n",
    "            \"refresh\": True,\n",
    "        }\n",
    "        headers = {\n",
    "            # 'Authorization': 'Bearer {}'.format(self.____access_token),\n",
    "            \"x-csrftoken\": self.__csrf_token,\n",
    "        }\n",
    "        response = self.__session.post(self.__url + \"login\", json=data, headers=headers)  # type: ignore\n",
    "        return True\n",
    "\n",
    "    def request(self, method, endpoint, **request_kwargs):\n",
    "        \"\"\"Executes a request against the Superset API.\n",
    "\n",
    "        Args:\n",
    "            method: HTTP method to use.\n",
    "            endpoint: Endpoint to use.\n",
    "            **request_kwargs: Any ``requests.request`` arguments to use.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing response body parsed from JSON.\n",
    "\n",
    "        Raises:\n",
    "            HTTPError: There is an HTTP error (detected by ``requests.Response.raise_for_status``)\n",
    "                even after retrying with a fresh session.\n",
    "        \"\"\"\n",
    "\n",
    "        logging.info(\"About to %s execute request for endpoint %s\", method, endpoint)\n",
    "\n",
    "        url = self.__api_url + endpoint\n",
    "        csrf_headers = {\n",
    "            # 'Authorization': 'Bearer {}'.format(self.__access_token),\n",
    "            \"x-csrftoken\": self.__csrf_token,\n",
    "        }\n",
    "\n",
    "        res = self.__session.request(method, url, headers=csrf_headers, **request_kwargs)  # type: ignore\n",
    "\n",
    "        logging.info(\"Request finished with status: %d\", res.status_code)\n",
    "\n",
    "        if res.status_code == 401 and res.json().get(\"msg\") == \"Token has expired\" and self.__refresh_session():\n",
    "            logging.info(f\"Retrying {method} request for {url} %s with refreshed session\")\n",
    "            res = self.__session.request(method, url, headers=csrf_headers, **request_kwargs)  # type: ignore\n",
    "\n",
    "            logging.info(\"Request finished with status: %d\", res.status_code)\n",
    "\n",
    "        if (\n",
    "            res.status_code == 400\n",
    "            and res.json()[\"message\"] == \"400 Bad Request: The CSRF session token is missing.\"\n",
    "            and self.__refresh_session()\n",
    "        ):\n",
    "            logging.info(f\"Retrying {method} request for {url} %s with refreshed session\")\n",
    "            res = self.__session.request(method, url, headers=csrf_headers, **request_kwargs)  # type: ignore\n",
    "            logging.info(f\"Request finished with status: {res.status_code}\")\n",
    "        res.raise_for_status()\n",
    "        return res.json()\n",
    "\n",
    "\n",
    "def get_tables_from_dbt(dbt_manifest, dbt_db_name):\n",
    "    tables = {}\n",
    "    for table_type in [\"nodes\"]:\n",
    "        manifest_subset = dbt_manifest[table_type]\n",
    "\n",
    "        for table_key_long in manifest_subset:\n",
    "            table = manifest_subset[table_key_long]\n",
    "            name = table[\"name\"]\n",
    "            schema = table[\"schema\"]\n",
    "            database = table[\"database\"]\n",
    "            description = table[\"description\"]\n",
    "            alias = table[\"alias\"]\n",
    "            source = table[\"unique_id\"].split(\".\")[-2]\n",
    "            table_key = schema + \".\" + alias  # Key will be alias, not name\n",
    "            columns = table[\"columns\"]\n",
    "\n",
    "            if dbt_db_name is None or database == dbt_db_name:\n",
    "                # fail if it breaks uniqueness constraint\n",
    "                assert table_key not in tables, (\n",
    "                    f\"Table {table_key} is a duplicate name (schema + table) across databases. \"\n",
    "                    \"This would result in incorrect matching between Superset and dbt. \"\n",
    "                    \"To fix this, remove duplicates or add ``dbt_db_name``.\"\n",
    "                )\n",
    "                tables[table_key] = {\n",
    "                    \"name\": name,\n",
    "                    \"schema\": schema,\n",
    "                    \"database\": database,\n",
    "                    \"type\": table_type[:-1],\n",
    "                    \"ref\": f\"ref('{name}')\" if table_type == \"nodes\" else f\"source('{source}', '{name}')\",\n",
    "                    \"user\": None,\n",
    "                    \"columns\": columns,\n",
    "                    \"description\": description,\n",
    "                    \"alias\": alias,\n",
    "                }\n",
    "            if schema == \"user\":\n",
    "                tables[table_key][\"user\"] = table[\"tags\"][0]\n",
    "\n",
    "    assert tables, \"Manifest is empty!\"\n",
    "\n",
    "    return tables\n",
    "\n",
    "\n",
    "def get_physical_datasets_from_superset(superset: SupersetDBTSessionConnector, superset_db_id):\n",
    "    logging.info(\"Getting physical datasets from Superset.\")\n",
    "    page_number = 0\n",
    "    datasets = []\n",
    "    datasets_keys = set()\n",
    "    while True:\n",
    "        logging.info(\"Getting page %d.\", page_number + 1)\n",
    "        rison_request = f\"dataset/?q=(page_size:100,page:{page_number},order_column:changed_on_delta_humanized,order_direction:asc,filters:!((col:table_name,opr:nct,value:archived),(col:sql,opr:dataset_is_null_or_empty,value:true)))\"\n",
    "        res = superset.request(\"GET\", rison_request)\n",
    "        result = res[\"result\"]\n",
    "        if result:\n",
    "            for r in result:\n",
    "                name = r[\"table_name\"]\n",
    "                schema = r[\"schema\"]\n",
    "                database_name = r[\"database\"][\"database_name\"]\n",
    "                dataset_id = r[\"id\"]\n",
    "                database_id = r[\"database\"][\"id\"]\n",
    "                dataset_key = f\"{schema}.{name}\"  # same format as in dashboards\n",
    "\n",
    "                kind = r[\"kind\"]\n",
    "                if kind == \"physical\" and (superset_db_id is None or database_id == superset_db_id):\n",
    "                    dataset_id = r[\"id\"]\n",
    "\n",
    "                    name = r[\"table_name\"]\n",
    "                    schema = r[\"schema\"]\n",
    "                    dataset_key = f\"{schema}.{name}\"  # used as unique identifier\n",
    "\n",
    "                    dataset_dict = {\n",
    "                        \"id\": dataset_id,\n",
    "                        \"name\": name,\n",
    "                        \"schema\": schema,\n",
    "                        \"database\": database_name,\n",
    "                        \"dataset_id\": dataset_id,\n",
    "                        \"key\": dataset_key,\n",
    "                        \"table\": [dataset_key],\n",
    "                    }\n",
    "\n",
    "                    # fail if it breaks uniqueness constraint\n",
    "                    assert dataset_key not in datasets_keys, (\n",
    "                        f\"Dataset {dataset_key} is a duplicate name (schema + table) \"\n",
    "                        \"across databases. \"\n",
    "                        \"This would result in incorrect matching between Superset and dbt. \"\n",
    "                        \"To fix this, remove duplicates or add the ``superset_db_id`` argument.\"\n",
    "                    )\n",
    "\n",
    "                    datasets_keys.add(dataset_key)\n",
    "                    datasets.append(dataset_dict)\n",
    "\n",
    "            page_number += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def get_tables_from_sql_simple(sql):\n",
    "    \"\"\"\n",
    "    (Superset) Fallback SQL parsing using regular expressions to get tables names.\n",
    "    \"\"\"\n",
    "    sql = re.sub(r\"(--.*)|(#.*)\", \"\", sql)\n",
    "    sql = re.sub(r\"\\s+\", \" \", sql).lower()\n",
    "    sql = re.sub(r\"(/\\*(.|\\n)*\\*/)\", \"\", sql)\n",
    "\n",
    "    regex = re.compile(r\"\\b(from|join)\\b\\s+(\\\"?(\\w+)\\\"?(\\.))?\\\"?(\\w+)\\\"?\\b\")\n",
    "    tables_match = regex.findall(sql)\n",
    "    tables = [\n",
    "        table[2] + \".\" + table[4] if table[2] != \"\" else table[4] for table in tables_match if table[4] != \"unnest\"\n",
    "    ]\n",
    "\n",
    "    tables = list(set(tables))\n",
    "\n",
    "    return tables\n",
    "\n",
    "\n",
    "def get_tables_from_sql(sql, dialect, sql_parsed=None):\n",
    "    \"\"\"\n",
    "    (Superset) SQL parsing using sqlfluff to get clean tables names.\n",
    "    If sqlfluff parsing fails it runs the above regex parsing func.\n",
    "    Returns a tables list.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not sql_parsed:\n",
    "            sql_parsed = sqlfluff.parse(sql, dialect=dialect)\n",
    "        tables_raw = list(get_json_segment(sql_parsed, \"table_reference\"))  # type: ignore\n",
    "        tables_cleaned = []  # With schema\n",
    "        for table_ref in tables_raw:\n",
    "            if isinstance(table_ref, list):\n",
    "                table_ref_identifier = []\n",
    "                # Get last 2 \"naked_identifier\"\n",
    "                for dictionary in table_ref[::-1]:\n",
    "                    if \"naked_identifier\" in dictionary:\n",
    "                        table_ref_identifier.append(dictionary[\"naked_identifier\"])\n",
    "                        if len(table_ref_identifier) == 2:\n",
    "                            tables_cleaned.append(\".\".join(table_ref_identifier[::-1]))\n",
    "                            break\n",
    "            if isinstance(table_ref, dict):\n",
    "                tables_cleaned.append(table_ref[\"naked_identifier\"])\n",
    "    except (\n",
    "        sqlfluff.core.errors.SQLParseError,  # type: ignore\n",
    "        sqlfluff.core.errors.SQLLexError,  # type: ignore\n",
    "        sqlfluff.core.errors.SQLFluffUserError,  # type: ignore\n",
    "        sqlfluff.api.simple.APIParsingError,  # type: ignore\n",
    "    ) as e:  # type: ignore\n",
    "        logging.warning(\n",
    "            \"Parsing SQL through sqlfluff failed. \"\n",
    "            \"Let me attempt this via regular expressions at least and \"\n",
    "            \"check the problematic query and error below.\\n%s\",\n",
    "            sql,\n",
    "            exc_info=e,\n",
    "        )\n",
    "        tables_cleaned = get_tables_from_sql_simple(sql)\n",
    "\n",
    "    return tables_cleaned\n",
    "\n",
    "\n",
    "def get_json_segment(\n",
    "    parse_result: Dict[str, Any], segment_type: str\n",
    ") -> Iterator[Union[str, Dict[str, Any], List[Dict[str, Any]]]]:\n",
    "    \"\"\"Recursively search JSON parse result for specified segment type.\n",
    "\n",
    "    Args:\n",
    "        parse_result (Dict[str, Any]): JSON parse result from `sqlfluff.fix`.\n",
    "        segment_type (str): The segment type to search for.\n",
    "\n",
    "    Yields:\n",
    "        Iterator[Union[str, Dict[str, Any], List[Dict[str, Any]]]]:\n",
    "        Retrieves children of specified segment type as either a string for a raw\n",
    "        segment or as JSON or an array of JSON for non-raw segments.\n",
    "    \"\"\"\n",
    "    for k, v in parse_result.items():\n",
    "        if k == segment_type:\n",
    "            yield v\n",
    "        elif isinstance(v, dict):\n",
    "            yield from get_json_segment(v, segment_type)\n",
    "        elif isinstance(v, list):\n",
    "            for s in v:\n",
    "                yield from get_json_segment(s, segment_type)\n",
    "\n",
    "\n",
    "def get_dashboards_from_superset(superset: SupersetDBTSessionConnector, superset_db_id, user_id):\n",
    "    \"\"\"\n",
    "    This function gets\n",
    "    1. Get dashboards id list from Superset iterating on the pages of the url\n",
    "    2. Get a dashboard detail information :\n",
    "        title, owner, url, unique datasets names\n",
    "\n",
    "    Returns dashboards, dashboards_datasets\n",
    "    \"\"\"\n",
    "\n",
    "    logging.info(\"Getting published dashboards from Superset.\")\n",
    "    page_number = 0\n",
    "    dashboards_id = []\n",
    "    while True:\n",
    "        logging.info(\"Getting page %d.\", page_number + 1)\n",
    "        res = superset.request(\"GET\", f'/dashboard/?q={{\"page\":{page_number},\"page_size\":100}}')\n",
    "        result = res[\"result\"]\n",
    "        if result:\n",
    "            for r in result:\n",
    "                if r[\"published\"] and r[\"created_by\"][\"id\"] == user_id:\n",
    "                    dashboards_id.append(r[\"id\"])\n",
    "            page_number += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    assert len(dashboards_id) > 0, \"There are no dashboards in Superset!\"\n",
    "\n",
    "    logging.info(\"There are %d published dashboards in Superset.\", len(dashboards_id))\n",
    "\n",
    "    dashboards = []\n",
    "    dashboards_datasets_w_db = set()\n",
    "    for i, d in enumerate(dashboards_id):\n",
    "        logging.info(\"Getting info for dashboard %d/%d.\", i + 1, len(dashboards_id))\n",
    "        res = superset.request(\"GET\", f\"/dashboard/{d}\")\n",
    "        result = res[\"result\"]\n",
    "\n",
    "        dashboard_id = result[\"id\"]\n",
    "        title = result[\"dashboard_title\"]\n",
    "        url = superset.url + \"/superset/dashboard/\" + str(dashboard_id)\n",
    "        owner_name = result[\"owners\"][0][\"first_name\"] + \" \" + result[\"owners\"][0][\"last_name\"]\n",
    "\n",
    "        # take unique dataset names, formatted as \"[database].[schema].[table]\" by Superset\n",
    "        res_table_names = superset.request(\"GET\", f\"/dashboard/{d}/datasets\")\n",
    "        result_table_names = res_table_names[\"result\"]\n",
    "\n",
    "        testing = []\n",
    "        for i in range(0, len(result_table_names)):\n",
    "            testing.append(result_table_names[i][\"name\"])\n",
    "\n",
    "        # datasets_raw = list(set(result['table_names'].split(', ')))\n",
    "        datasets_raw = testing\n",
    "\n",
    "        # parse dataset names into parts\n",
    "        datasets_parsed = [dataset[1:-1].split(\"].[\", maxsplit=2) for dataset in datasets_raw]\n",
    "        datasets_parsed = [\n",
    "            [dataset[0], \"None\", dataset[1]]  # add None in the middle\n",
    "            if len(dataset) == 2\n",
    "            else dataset  # if missing the schema\n",
    "            for dataset in datasets_parsed\n",
    "        ]\n",
    "\n",
    "        # put them all back together to get \"database.schema.table\"\n",
    "        datasets_w_db = [\".\".join(dataset) for dataset in datasets_parsed]\n",
    "\n",
    "        dbt_project_name = \"your_dbt_project.\"\n",
    "        datasets_w_db = [dbt_project_name + sub for sub in testing]\n",
    "\n",
    "        dashboards_datasets_w_db.update(datasets_w_db)\n",
    "\n",
    "        # skip database, i.e. first item, to get only \"schema.table\"\n",
    "        datasets_wo_db = [\".\".join(dataset[1:]) for dataset in datasets_parsed]\n",
    "\n",
    "        datasets_wo_db = testing\n",
    "        dashboard = {\n",
    "            \"id\": dashboard_id,\n",
    "            \"title\": title,\n",
    "            \"url\": url,\n",
    "            \"owner_name\": owner_name,\n",
    "            \"owner_email\": \"\",  # required for dbt to accept owner_name but not in response\n",
    "            \"datasets\": datasets_wo_db,  # add in \"schema.table\" format\n",
    "        }\n",
    "        dashboards.append(dashboard)\n",
    "    # test if unique when database disregarded\n",
    "    # loop to get the name of duplicated dataset and work with unique set of datasets w db\n",
    "    dashboards_datasets = set()\n",
    "    for dataset_w_db in dashboards_datasets_w_db:\n",
    "        dataset = \".\".join(dataset_w_db.split(\".\")[1:])  # similar logic as just a bit above\n",
    "\n",
    "        # fail if it breaks uniqueness constraint and not limited to one database\n",
    "        assert dataset not in dashboards_datasets or superset_db_id is not None, (\n",
    "            f\"Dataset {dataset} is a duplicate name (schema + table) across databases. \"\n",
    "            \"This would result in incorrect matching between Superset and dbt. \"\n",
    "            \"To fix this, remove duplicates or add ``superset_db_id``.\"\n",
    "        )\n",
    "\n",
    "        dashboards_datasets.add(dataset)\n",
    "\n",
    "    return dashboards, dashboards_datasets\n",
    "\n",
    "\n",
    "def get_datasets_from_superset_dbt_refs(\n",
    "    superset: SupersetDBTSessionConnector, dashboards_datasets, dbt_tables, sql_dialect, superset_db_id\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns datasets (dict) containing table info and dbt references\n",
    "    \"\"\"\n",
    "\n",
    "    logging.info(\"Getting datasets info from Superset.\")\n",
    "    page_number = 0\n",
    "    datasets = {}\n",
    "    while True:\n",
    "        logging.info(\"Getting page %d.\", page_number + 1)\n",
    "        res = superset.request(\"GET\", f'/dataset/?q={{\"page\":{page_number},\"page_size\":100}}')\n",
    "        result = res[\"result\"]\n",
    "        if result:\n",
    "            for r in result:\n",
    "                name = r[\"table_name\"]\n",
    "                schema = r[\"schema\"]\n",
    "                database_name = r[\"database\"][\"database_name\"]\n",
    "                database_id = r[\"database\"][\"id\"]\n",
    "\n",
    "                dataset_key = f\"{schema}.{name}\"  # same format as in dashboards\n",
    "\n",
    "                # only add datasets that are in dashboards, optionally limit to one database\n",
    "                if dataset_key in dashboards_datasets and (superset_db_id is None or database_id == superset_db_id):\n",
    "                    kind = r[\"kind\"]\n",
    "                    if kind == \"virtual\":  # built on custom sql\n",
    "                        sql = r[\"sql\"]\n",
    "                        tables = get_tables_from_sql(sql, sql_dialect)\n",
    "                        tables = [table if \".\" in table else f\"{schema}.{table}\" for table in tables]\n",
    "                    else:  # built on tables\n",
    "                        tables = [dataset_key]\n",
    "                    dbt_refs = [dbt_tables[table][\"ref\"] for table in tables if table in dbt_tables]\n",
    "\n",
    "                    datasets[dataset_key] = {\n",
    "                        \"name\": name,\n",
    "                        \"schema\": schema,\n",
    "                        \"database\": database_name,\n",
    "                        \"kind\": kind,\n",
    "                        \"tables\": tables,\n",
    "                        \"dbt_refs\": dbt_refs,\n",
    "                    }\n",
    "            page_number += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def refresh_columns_in_superset(superset: SupersetDBTSessionConnector, dataset_id):\n",
    "    logging.info(\"Refreshing columns in Superset.\")\n",
    "    superset.request(\"PUT\", f\"/dataset/{dataset_id}/refresh\")\n",
    "\n",
    "\n",
    "def add_sst_dataset_metadata(superset: SupersetDBTSessionConnector, dataset_id, sst_dataset_key, dbt_tables):\n",
    "    logging.info(\"Refreshing columns in Superset.\")\n",
    "    body = {\n",
    "        \"extra\": '{\"certification\": \\n  {\"certified_by\": \"Data Analytics Team\", \\n   \"details\": \"This table is the source of truth.\" \\n    \\n  }\\n}',\n",
    "        \"description\": dbt_tables[sst_dataset_key][\"description\"],\n",
    "        \"owners\": [SUPERSET_ID],\n",
    "    }\n",
    "    if dbt_tables[sst_dataset_key][\"user\"]:\n",
    "        body[\"owners\"].append(dbt_tables[sst_dataset_key][\"user\"])\n",
    "    superset.request(\"PUT\", f\"/dataset/{dataset_id}\", json=body)\n",
    "\n",
    "\n",
    "def add_superset_columns(superset: SupersetDBTSessionConnector, dataset):\n",
    "    logging.info(\"Pulling fresh columns info from Superset.\")\n",
    "    res = superset.request(\"GET\", f\"/dataset/{dataset['id']}\")\n",
    "    columns = res[\"result\"][\"columns\"]\n",
    "    dataset[\"columns\"] = columns\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def convert_markdown_to_plain_text(md_string):\n",
    "    \"\"\"Converts a markdown string to plaintext.\n",
    "\n",
    "    The following solution is used:\n",
    "    https://gist.github.com/lorey/eb15a7f3338f959a78cc3661fbc255fe\n",
    "    \"\"\"\n",
    "\n",
    "    # md -> html -> text since BeautifulSoup can extract text cleanly\n",
    "    html = markdown(md_string)\n",
    "\n",
    "    # remove code snippets\n",
    "    html = re.sub(r\"<pre>(.*?)</pre>\", \" \", html)\n",
    "    html = re.sub(r\"<code>(.*?)</code >\", \" \", html)\n",
    "\n",
    "    # extract text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    text = \"\".join(soup.findAll(text=True))\n",
    "\n",
    "    # make one line\n",
    "    single_line = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    # make fixes\n",
    "    single_line = re.sub(\"\", \"->\", single_line)\n",
    "    single_line = re.sub(\"<null>\", '\"null\"', single_line)\n",
    "\n",
    "    return single_line\n",
    "\n",
    "\n",
    "def merge_columns_info(dataset, tables):\n",
    "    logging.info(\"Merging columns info from Superset and manifest.json file.\")\n",
    "\n",
    "    key = dataset[\"key\"]\n",
    "    sst_columns = dataset[\"columns\"]\n",
    "    dbt_columns = tables.get(key, {}).get(\"columns\", {})\n",
    "    columns_new = []\n",
    "    for sst_column in sst_columns:\n",
    "        # add the mandatory field\n",
    "        column_new = {\"column_name\": sst_column[\"column_name\"]}\n",
    "\n",
    "        # add optional fields only if not already None, otherwise it would error out\n",
    "        for field in [\n",
    "            \"expression\",\n",
    "            \"filterable\",\n",
    "            \"groupby\",\n",
    "            \"python_date_format\",\n",
    "            \"verbose_name\",\n",
    "            \"type\",\n",
    "            \"is_dttm\",\n",
    "            \"is_active\",\n",
    "        ]:\n",
    "            if sst_column[field] is not None:\n",
    "                column_new[field] = sst_column[field]\n",
    "\n",
    "        # add description\n",
    "        if (\n",
    "            sst_column[\"column_name\"] in dbt_columns\n",
    "            and \"description\" in dbt_columns[sst_column[\"column_name\"]]\n",
    "            and sst_column[\"expression\"] == \"\"\n",
    "        ):  # database columns\n",
    "            description = dbt_columns[sst_column[\"column_name\"]][\"description\"]\n",
    "            description = convert_markdown_to_plain_text(description)\n",
    "        else:  # if cant find in dbt\n",
    "            description = sst_column[\"description\"]\n",
    "        column_new[\"description\"] = description\n",
    "\n",
    "        columns_new.append(column_new)\n",
    "\n",
    "    dataset[\"columns_new\"] = columns_new\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def put_columns_to_superset(superset: SupersetDBTSessionConnector, dataset):\n",
    "    logging.info(\"Putting new columns info with descriptions back into Superset.\")\n",
    "    body = {\"columns\": dataset[\"columns_new\"]}\n",
    "    superset.request(\"PUT\", f\"/dataset/{dataset['id']}?override_columns=true\", json=body)\n",
    "\n",
    "\n",
    "def merge_dashboards_with_datasets(dashboards, datasets):\n",
    "    for dashboard in dashboards:\n",
    "        refs = set()\n",
    "        for dataset in dashboard[\"datasets\"]:\n",
    "            if dataset in datasets:\n",
    "                refs.update(datasets[dataset][\"dbt_refs\"])\n",
    "        refs = list(sorted(refs))\n",
    "\n",
    "        dashboard[\"refs\"] = refs\n",
    "\n",
    "    return dashboards\n",
    "\n",
    "\n",
    "def get_exposures_dict(dashboards, exposures):\n",
    "    dashboards.sort(key=lambda dashboard: dashboard[\"id\"])\n",
    "    titles = [dashboard[\"title\"] for dashboard in dashboards]\n",
    "    # fail if it breaks uniqueness constraint for exposure names\n",
    "    assert len(set(titles)) == len(titles), \"There are duplicate dashboard names!\"\n",
    "\n",
    "    exposures_orig = {exposure[\"url\"]: exposure for exposure in exposures}\n",
    "    exposures_dict = [\n",
    "        {\n",
    "            \"name\": f\"superset__{dashboard['title']}\",\n",
    "            \"type\": \"dashboard\",\n",
    "            \"url\": dashboard[\"url\"],\n",
    "            \"description\": exposures_orig.get(dashboard[\"url\"], {}).get(\"description\", \"\"),\n",
    "            \"depends_on\": dashboard[\"refs\"],\n",
    "            \"owner\": {\"name\": dashboard[\"owner_name\"], \"email\": dashboard[\"owner_email\"]},\n",
    "        }\n",
    "        for dashboard in dashboards\n",
    "    ]\n",
    "\n",
    "    return exposures_dict\n",
    "\n",
    "\n",
    "class YamlFormatted(ruamel.yaml.YAML):\n",
    "    def __init__(self):\n",
    "        super(YamlFormatted, self).__init__()\n",
    "        self.default_flow_style = False\n",
    "        self.allow_unicode = True\n",
    "        self.encoding = \"utf-8\"\n",
    "        self.block_seq_indent = 2\n",
    "        self.indent = 4\n",
    "        self.emitter.alt_null = \"''\"\n",
    "\n",
    "\n",
    "# Create Query\n",
    "\n",
    "\n",
    "def is_valid_table_name(table_name):\n",
    "    \"\"\"\n",
    "    Checks if the given string is a valid table name in PostgreSQL.\n",
    "\n",
    "    Args:\n",
    "        table_name: The string to check.\n",
    "\n",
    "    Returns:\n",
    "        True if the string is a valid table name, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # The regular expression to match a valid table name.\n",
    "    regex = re.compile(r\"^[a-zA-Z0-9_]{1,63}$\")\n",
    "\n",
    "    # Check if the string matches the regular expression.\n",
    "    if regex.match(table_name):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def is_unique_table_name(table_name, dbt_tables):\n",
    "    \"\"\"\n",
    "    Checks if the given string is a valid table name in PostgreSQL and dbt.\n",
    "\n",
    "    Args:\n",
    "        table_name: The string to check.\n",
    "        dbt_tables: Dict of get_dbt_tables\n",
    "    Returns:\n",
    "        True if the string is a valid table name, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # The regular expression to match a valid table name.\n",
    "    regex = re.compile(r\"^[a-zA-Z0-9_]{1,63}$\")\n",
    "\n",
    "    # Check if the string matches the regular expression.\n",
    "    if table_name not in dbt_tables:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_ref(original_query, dbt_tables, parsed_result, dbt_tables_names):\n",
    "    \"\"\"\n",
    "    Returns content of a user-created dbt model file w/o config.\n",
    "\n",
    "    Args:\n",
    "        original_query: Query needed processing\n",
    "        dbt_tables: Dict of dicts obtained by get_tables_from_dbt.\n",
    "        schema_names: List of serving schema names.\n",
    "\n",
    "    Returns:\n",
    "        ref_tables: list of models that is referenced in the query\n",
    "    \"\"\"\n",
    "    # original_query = original_query[:-1] if original_query[-1] == \";\" else original_query # Maybe unneeded since not wrapping with\n",
    "    # Access table names\n",
    "    fixed_query = str(original_query)\n",
    "    table_names = set(get_tables_from_sql(fixed_query, dialect=\"postgres\", sql_parsed=parsed_result))\n",
    "    fixed_query = sqlfluff.fix(fixed_query, dialect=\"postgres\")\n",
    "    if len(table_names.difference(dbt_tables_names)) > 0:  # dbt_tables_names include schema\n",
    "        return None, \"Tables referenced out of serving schemas\"\n",
    "    # Put tables in subqueries\n",
    "    final_tables = tuple(table_names.intersection(dbt_tables_names))  # Filter out\n",
    "\n",
    "    if len(final_tables) == 0:\n",
    "        return None, \"No tables referenced in dbt projects\"\n",
    "\n",
    "    return [dbt_tables[table][\"name\"] for table in final_tables], \"Success\"\n",
    "\n",
    "\n",
    "def get_records():\n",
    "    # Query records\n",
    "    try:\n",
    "        connection = psycopg2.connect(\n",
    "            user=DATABASE_USERNAME,\n",
    "            password=DATABASE_PASSWORD,\n",
    "            host=DATABASE_HOST,\n",
    "            port=DATABASE_PORT,\n",
    "            database=DATABASE_NAME,\n",
    "        )\n",
    "        cursor = connection.cursor()\n",
    "        postgreSQL_select_Query = f\"select * from {QUERY_SCHEMA}.{QUERY_TABLE} where checked = False\"\n",
    "\n",
    "        logging.info(f\"Executing query to fetch records: {postgreSQL_select_Query}\")\n",
    "        cursor.execute(postgreSQL_select_Query)\n",
    "        query_columns = [\n",
    "            \"id\",\n",
    "            \"name\",\n",
    "            \"query_string\",\n",
    "            \"user_id\",\n",
    "            \"materialization\",\n",
    "            \"description\",\n",
    "            \"insert_time\",\n",
    "            \"checked\",\n",
    "            \"success\",\n",
    "        ]\n",
    "        df = pd.DataFrame(cursor.fetchall(), columns=query_columns)\n",
    "\n",
    "        postgreSQL_select_Query = f\"select name from {QUERY_SCHEMA}.{QUERY_TABLE} where success = True\"\n",
    "\n",
    "        logging.info(f\"Executing query to fetch records: {postgreSQL_select_Query}\")\n",
    "        cursor.execute(postgreSQL_select_Query)\n",
    "\n",
    "        succeeded = cursor.fetchall()\n",
    "\n",
    "    finally:\n",
    "        # closing database connection.\n",
    "        if connection:\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            logging.info(\"PostgreSQL connection is closed\")\n",
    "    return df, succeeded\n",
    "\n",
    "\n",
    "def update_records(df):\n",
    "    entries_to_update = str(tuple(zip(df.checked, df.success, df.id))).replace(\"None\", \"Null\")[1:-1]\n",
    "    try:\n",
    "        connection = psycopg2.connect(\n",
    "            user=DATABASE_USERNAME,\n",
    "            password=DATABASE_PASSWORD,\n",
    "            host=DATABASE_HOST,\n",
    "            port=DATABASE_PORT,\n",
    "            database=DATABASE_NAME,\n",
    "        )\n",
    "        cursor = connection.cursor()\n",
    "        update_sql_query = f\"\"\"UPDATE {QUERY_SCHEMA}.{QUERY_TABLE} q \n",
    "                                SET success = v.success,\n",
    "                                    checked = v.checked\n",
    "\n",
    "                                FROM (VALUES {entries_to_update}) AS v (checked, success, id)\n",
    "                                WHERE q.id = v.id;\"\"\"\n",
    "        logging.info(f\"Executing query to update records: {update_sql_query}\")\n",
    "        cursor.execute(update_sql_query)\n",
    "\n",
    "    finally:\n",
    "        # closing database connection.\n",
    "        if connection:\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            logging.info(\"PostgreSQL connection is closed\")\n",
    "\n",
    "\n",
    "def get_emails(superset, user_ids):\n",
    "    url = unquote(f\"/security/get_email/?q={list(user_ids)}\")\n",
    "    res = superset.request(\"GET\", url)\n",
    "    return res[\"emails\"]\n",
    "\n",
    "\n",
    "def get_mail_content(name, sql, status, dbt_reason=None):\n",
    "    if status == \"dbt success\":\n",
    "        message = \"\"\"\\\n",
    "Subject: Superset Model Creation\n",
    "\n",
    "Your Model {name} was successfully created. \n",
    "\n",
    "SQL:{sql}\n",
    "        \"\"\".format(\n",
    "            sql=sql, name=name\n",
    "        )\n",
    "\n",
    "    elif status == \"dbt fail\":\n",
    "        message = \"\"\"\\\n",
    "Subject: Superset Model Creation\n",
    "\n",
    "Your Model {name} was unsuccessfully created during dbt's run, please contact the administrator.\n",
    "\n",
    "Reason:\n",
    "{reason}\n",
    "\n",
    "SQL:\n",
    "{sql}\n",
    "        \"\"\".format(\n",
    "            reason=dbt_reason, sql=sql, name=name\n",
    "        )\n",
    "    else:\n",
    "        message = \"\"\"\\\n",
    "Subject: Superset Model Creation\n",
    "\n",
    "Your Model {name} was unsuccessfully created.\n",
    "\n",
    "Reason:\n",
    "{reason}\n",
    "\n",
    "SQL:\n",
    "{sql}\n",
    "        \"\"\".format(\n",
    "            reason=status, sql=sql, name=name\n",
    "        )\n",
    "    return message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select * from financial_query.query where checked = False\n",
      "PostgreSQL connection is closed\n"
     ]
    }
   ],
   "source": [
    "df = get_records()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_string</th>\n",
       "      <th>materialization</th>\n",
       "      <th>user_id</th>\n",
       "      <th>description</th>\n",
       "      <th>insert_time</th>\n",
       "      <th>name</th>\n",
       "      <th>checked</th>\n",
       "      <th>success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SELECT * from marts.dim_balance_sheet</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2023-07-24 09:37:06.123662</td>\n",
       "      <td>asd</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SELECT * from dim_bollinger</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2023-07-23 08:11:58.188941</td>\n",
       "      <td>unique_2</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SELECT * from marts.dim_bollinger</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>asdad</td>\n",
       "      <td>2023-07-23 08:12:43.670466</td>\n",
       "      <td>unique_3</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SELECT * from marts.dim_balance_sheet</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>asdasdwa</td>\n",
       "      <td>2023-07-23 09:37:33.026336</td>\n",
       "      <td>unique_4</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               query_string  materialization  user_id  \\\n",
       "0     SELECT * from marts.dim_balance_sheet                2        1   \n",
       "1              SELECT * from dim_bollinger                 2        1   \n",
       "2        SELECT * from marts.dim_bollinger                 2        1   \n",
       "3  SELECT * from marts.dim_balance_sheet                   2        1   \n",
       "\n",
       "  description                insert_time      name  checked success  \n",
       "0             2023-07-24 09:37:06.123662       asd    False    None  \n",
       "1             2023-07-23 08:11:58.188941  unique_2    False    None  \n",
       "2       asdad 2023-07-23 08:12:43.670466  unique_3    False    None  \n",
       "3    asdasdwa 2023-07-23 09:37:33.026336  unique_4    False    None  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entries\n",
      "('a', 1, True, False), ('sda', 1, True, True)\n"
     ]
    }
   ],
   "source": [
    "entries_to_update = str(tuple(zip(df.name, df.user_id, df.checked, df.success))).replace(\"None\", \"Null\")[1:-1]\n",
    "print(\"entries\")\n",
    "print(entries_to_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_records(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "entries_to_update = str(tuple(zip(df.checked, df.success, df.id))).replace(\"None\", \"Null\")[1:-1]\n",
    "connection = psycopg2.connect(\n",
    "    user=DATABASE_USERNAME,\n",
    "    password=DATABASE_PASSWORD,\n",
    "    host=DATABASE_HOST,\n",
    "    port=DATABASE_PORT,\n",
    "    database=DATABASE_NAME,\n",
    ")\n",
    "cursor = connection.cursor()\n",
    "update_sql_query = f\"\"\"UPDATE {QUERY_SCHEMA}.{QUERY_TABLE} q \n",
    "                        SET success = v.success,\n",
    "                            checked = v.checked\n",
    "\n",
    "                        FROM (VALUES {entries_to_update}) AS v (checked, success, id)\n",
    "                        WHERE q.id = v.id;\"\"\"\n",
    "logging.info(f\"Executing query to update records: {update_sql_query}\")\n",
    "cursor.execute(update_sql_query)\n",
    "\n",
    "if connection:\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    logging.info(\"PostgreSQL connection is closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATE financial_query.query q \n",
      "                            SET success = v.success,\n",
      "                                checked = v.checked\n",
      "\n",
      "                            FROM (VALUES (True, False, 39), (True, True, 40)) AS v (checked, success, id)\n",
      "                            WHERE q.id = v.id;\n"
     ]
    }
   ],
   "source": [
    "print(update_sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in dbt_res_df_map.keys():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_1: success\n",
      "False\n",
      "tetst: success\n",
      "False\n",
      "unique_1: success\n",
      "False\n",
      "tetst: success\n",
      "False\n",
      "unique_1: success\n",
      "False\n",
      "tetst: success\n",
      "False\n",
      "unique_1: success\n",
      "False\n",
      "tetst: success\n",
      "True\n",
      "unique_1: success\n",
      "True\n",
      "tetst: success\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for i in df.index:\n",
    "    for r in res.result:\n",
    "        print(f\"{r.node.name}: {r.status}\")\n",
    "        print(r.node.name == df.loc[i, \"name\"])\n",
    "# Map df index to result\n",
    "dbt_res_df_map = {}\n",
    "\n",
    "for i in df.index:\n",
    "    for r in res.result:\n",
    "        if r.node.name == df.loc[i, \"name\"]:\n",
    "            dbt_res_df_map[i] = r\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: RunResult(status=<RunStatus.Success: 'success'>, timing=[TimingInfo(name='compile', started_at=datetime.datetime(2023, 7, 23, 6, 49, 39, 256404), completed_at=datetime.datetime(2023, 7, 23, 6, 49, 39, 279021)), TimingInfo(name='execute', started_at=datetime.datetime(2023, 7, 23, 6, 49, 39, 283493), completed_at=datetime.datetime(2023, 7, 23, 6, 49, 42, 346239))], thread_id='Thread-40 (worker)', execution_time=3.102137327194214, adapter_response={'_message': 'SELECT 1979', 'code': 'SELECT', 'rows_affected': 1979}, message='SELECT 1979', failures=None, node=ModelNode(database='financial_data', schema='financial_user', name='tetst', resource_type=<NodeType.Model: 'model'>, package_name='dbt_financial', path='user/tetst.sql', original_file_path='models/user/tetst.sql', unique_id='model.dbt_financial.tetst', fqn=['dbt_financial', 'user', 'tetst'], alias='tetst', checksum=FileHash(name='sha256', checksum='8e23edf47bc6221ab2224b44712988691722cb47b9151c36229d1e70454a867a'), config=NodeConfig(_extra={'name': 'tetst', 'description': 'ffgfs'}, enabled=True, alias=None, schema='financial_user', database=None, tags=['1', 'user_created', '23/07/2023_13:49:17'], meta={}, group=None, materialized='table', incremental_strategy=None, persist_docs={}, post_hook=[], pre_hook=[], quoting={}, column_types={}, full_refresh=None, unique_key=None, on_schema_change='ignore', grants={}, packages=[], docs=Docs(show=True, node_color=None), contract=ContractConfig(enforced=False)), _event_status={}, tags=['1', 'user_created', '23/07/2023_13:49:17'], description='', columns={}, meta={}, group=None, docs=Docs(show=True, node_color=None), patch_path=None, build_path='target/run/dbt_financial/models/user/tetst.sql', deferred=False, unrendered_config={'materialized': 'table', 'name': 'tetst', 'description': 'ffgfs', 'tags': ['1', 'user_created', '23/07/2023_13:49:17'], 'schema': 'financial_user'}, created_at=1690094968.6971893, config_call_dict={'materialized': 'table', 'name': 'tetst', 'description': 'ffgfs', 'tags': ['1', 'user_created', '23/07/2023_13:49:17'], 'schema': 'financial_user'}, relation_name='\"financial_data\".\"financial_user\".\"tetst\"', raw_code=\"{{ config(\\n    materialized='table',\\n    name='tetst',\\n    description='ffgfs',\\n    tags = ['1','user_created','23/07/2023_13:49:17'],\\n    schema = 'financial_user'\\n) }}\\n    -- depends_on: {{ref('dim_price_history')}}\\n    SELECT * from marts.dim_price_history\", language=<ModelLanguage.sql: 'sql'>, refs=[RefArgs(name='dim_price_history', package=None, version=None)], sources=[], metrics=[], depends_on=DependsOn(macros=[], nodes=['model.dbt_financial.dim_price_history']), compiled_path='target/compiled/dbt_financial/models/user/tetst.sql', compiled=True, compiled_code='\\n    -- depends_on: \"financial_data\".\"marts\".\"dim_price_history\"\\n    SELECT * from marts.dim_price_history', extra_ctes_injected=True, extra_ctes=[], _pre_injected_sql=None, contract=Contract(enforced=False, checksum=None), access=<AccessType.Protected: 'protected'>, constraints=[], version=None, latest_version=None), agate_table=None),\n",
       " 4: RunResult(status=<RunStatus.Success: 'success'>, timing=[TimingInfo(name='compile', started_at=datetime.datetime(2023, 7, 23, 6, 49, 39, 265906), completed_at=datetime.datetime(2023, 7, 23, 6, 49, 39, 282062)), TimingInfo(name='execute', started_at=datetime.datetime(2023, 7, 23, 6, 49, 39, 296474), completed_at=datetime.datetime(2023, 7, 23, 6, 49, 42, 342207))], thread_id='Thread-41 (worker)', execution_time=3.0983898639678955, adapter_response={'_message': 'CREATE VIEW', 'code': 'CREATE VIEW', 'rows_affected': -1}, message='CREATE VIEW', failures=None, node=ModelNode(database='financial_data', schema='financial_user', name='unique_1', resource_type=<NodeType.Model: 'model'>, package_name='dbt_financial', path='user/unique_1.sql', original_file_path='models/user/unique_1.sql', unique_id='model.dbt_financial.unique_1', fqn=['dbt_financial', 'user', 'unique_1'], alias='unique_1', checksum=FileHash(name='sha256', checksum='d973ce87c0ba39cacb13b8ba5239fb8f7d0fbb7034bf9a17584f22c8459f5356'), config=NodeConfig(_extra={'name': 'unique_1', 'description': 'asd'}, enabled=True, alias=None, schema='financial_user', database=None, tags=['1', 'user_created', '23/07/2023_13:49:17'], meta={}, group=None, materialized='view', incremental_strategy=None, persist_docs={}, post_hook=[], pre_hook=[], quoting={}, column_types={}, full_refresh=None, unique_key=None, on_schema_change='ignore', grants={}, packages=[], docs=Docs(show=True, node_color=None), contract=ContractConfig(enforced=False)), _event_status={}, tags=['1', 'user_created', '23/07/2023_13:49:17'], description='', columns={}, meta={}, group=None, docs=Docs(show=True, node_color=None), patch_path=None, build_path='target/run/dbt_financial/models/user/unique_1.sql', deferred=False, unrendered_config={'materialized': 'view', 'name': 'unique_1', 'description': 'asd', 'tags': ['1', 'user_created', '23/07/2023_13:49:17'], 'schema': 'financial_user'}, created_at=1690094968.7050407, config_call_dict={'materialized': 'view', 'name': 'unique_1', 'description': 'asd', 'tags': ['1', 'user_created', '23/07/2023_13:49:17'], 'schema': 'financial_user'}, relation_name='\"financial_data\".\"financial_user\".\"unique_1\"', raw_code=\"{{ config(\\n    materialized='view',\\n    name='unique_1',\\n    description='asd',\\n    tags = ['1','user_created','23/07/2023_13:49:17'],\\n    schema = 'financial_user'\\n) }}\\n    -- depends_on: {{ref('dim_price_history')}}\\n    SELECT * from marts.dim_price_history\", language=<ModelLanguage.sql: 'sql'>, refs=[RefArgs(name='dim_price_history', package=None, version=None)], sources=[], metrics=[], depends_on=DependsOn(macros=[], nodes=['model.dbt_financial.dim_price_history']), compiled_path='target/compiled/dbt_financial/models/user/unique_1.sql', compiled=True, compiled_code='\\n    -- depends_on: \"financial_data\".\"marts\".\"dim_price_history\"\\n    SELECT * from marts.dim_price_history', extra_ctes_injected=True, extra_ctes=[], _pre_injected_sql=None, contract=Contract(enforced=False, checksum=None), access=<AccessType.Protected: 'protected'>, constraints=[], version=None, latest_version=None), agate_table=None)}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbt_res_df_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: RunResult(status=<RunStatus.Success: 'success'>, timing=[TimingInfo(name='compile', started_at=datetime.datetime(2023, 7, 23, 6, 36, 18, 918788), completed_at=datetime.datetime(2023, 7, 23, 6, 36, 18, 941524)), TimingInfo(name='execute', started_at=datetime.datetime(2023, 7, 23, 6, 36, 18, 946818), completed_at=datetime.datetime(2023, 7, 23, 6, 36, 22, 629960))], thread_id='Thread-26 (worker)', execution_time=3.7253305912017822, adapter_response={'_message': 'SELECT 1979', 'code': 'SELECT', 'rows_affected': 1979}, message='SELECT 1979', failures=None, node=ModelNode(database='financial_data', schema='financial_user', name='tetst', resource_type=<NodeType.Model: 'model'>, package_name='dbt_financial', path='user/tetst.sql', original_file_path='models/user/tetst.sql', unique_id='model.dbt_financial.tetst', fqn=['dbt_financial', 'user', 'tetst'], alias='tetst', checksum=FileHash(name='sha256', checksum='446ff4b5dcaab790e15e72b1b78d8cec877be517e2f2d157d7fcb5c8c82ed6c1'), config=NodeConfig(_extra={'name': 'tetst', 'description': 'ffgfs'}, enabled=True, alias=None, schema='financial_user', database=None, tags=['1', 'user_created', '23/07/2023_13:35:52'], meta={}, group=None, materialized='table', incremental_strategy=None, persist_docs={}, post_hook=[], pre_hook=[], quoting={}, column_types={}, full_refresh=None, unique_key=None, on_schema_change='ignore', grants={}, packages=[], docs=Docs(show=True, node_color=None), contract=ContractConfig(enforced=False)), _event_status={}, tags=['1', 'user_created', '23/07/2023_13:35:52'], description='', columns={}, meta={}, group=None, docs=Docs(show=True, node_color=None), patch_path=None, build_path='target/run/dbt_financial/models/user/tetst.sql', deferred=False, unrendered_config={'materialized': 'table', 'name': 'tetst', 'description': 'ffgfs', 'tags': ['1', 'user_created', '23/07/2023_13:35:52'], 'schema': 'financial_user'}, created_at=1690094166.8347187, config_call_dict={'materialized': 'table', 'name': 'tetst', 'description': 'ffgfs', 'tags': ['1', 'user_created', '23/07/2023_13:35:52'], 'schema': 'financial_user'}, relation_name='\"financial_data\".\"financial_user\".\"tetst\"', raw_code=\"{{ config(\\n    materialized='table',\\n    name='tetst',\\n    description='ffgfs',\\n    tags = ['1','user_created','23/07/2023_13:35:52'],\\n    schema = 'financial_user'\\n) }}\\n    -- depends_on: {{ref('dim_price_history')}}\\n    SELECT * from marts.dim_price_history\", language=<ModelLanguage.sql: 'sql'>, refs=[RefArgs(name='dim_price_history', package=None, version=None)], sources=[], metrics=[], depends_on=DependsOn(macros=[], nodes=['model.dbt_financial.dim_price_history']), compiled_path='target/compiled/dbt_financial/models/user/tetst.sql', compiled=True, compiled_code='\\n    -- depends_on: \"financial_data\".\"marts\".\"dim_price_history\"\\n    SELECT * from marts.dim_price_history', extra_ctes_injected=True, extra_ctes=[], _pre_injected_sql=None, contract=Contract(enforced=False, checksum=None), access=<AccessType.Protected: 'protected'>, constraints=[], version=None, latest_version=None), agate_table=None)}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbt_res_df_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbt_res_df_map[3].status==\"success\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.index:\n",
    "    # Check Success\n",
    "    if not df.loc[i, \"success\"]:\n",
    "        model_path = USER_MODEL_PATH + \"/{name}.sql\".format(name=df.loc[i, \"name\"])\n",
    "        if os.path.exists(model_path):\n",
    "            os.remove(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_string</th>\n",
       "      <th>materialization</th>\n",
       "      <th>user_id</th>\n",
       "      <th>description</th>\n",
       "      <th>insert_time</th>\n",
       "      <th>name</th>\n",
       "      <th>success</th>\n",
       "      <th>checked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SELECT * from marts.dim_price_history</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>test description</td>\n",
       "      <td>2023-07-15 05:40:36.283000</td>\n",
       "      <td>test_query_hgffhgf</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SELECT * from dim_price_history</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>test_desc</td>\n",
       "      <td>2023-07-15 06:40:36.283000</td>\n",
       "      <td>test_query_2_gftyf</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SELECT * from dim_price_history</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>test_desc</td>\n",
       "      <td>2023-07-20 14:13:51.968099</td>\n",
       "      <td>test_query_123</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SELECT * from marts.dim_price_history</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>ffgfs</td>\n",
       "      <td>2023-07-22 09:47:39.173512</td>\n",
       "      <td>tetst</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SELECT * from marts.dim_price_history</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>asd</td>\n",
       "      <td>2023-07-22 09:47:39.173000</td>\n",
       "      <td>unique_1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            query_string  materialization  user_id  \\\n",
       "0  SELECT * from marts.dim_price_history                1        1   \n",
       "1        SELECT * from dim_price_history                2        1   \n",
       "2        SELECT * from dim_price_history                1        1   \n",
       "3  SELECT * from marts.dim_price_history                1        1   \n",
       "4  SELECT * from marts.dim_price_history                2        1   \n",
       "\n",
       "        description                insert_time                name  success  \\\n",
       "0  test description 2023-07-15 05:40:36.283000  test_query_hgffhgf    False   \n",
       "1         test_desc 2023-07-15 06:40:36.283000  test_query_2_gftyf    False   \n",
       "2         test_desc 2023-07-20 14:13:51.968099      test_query_123    False   \n",
       "3             ffgfs 2023-07-22 09:47:39.173512               tetst    False   \n",
       "4               asd 2023-07-22 09:47:39.173000            unique_1    False   \n",
       "\n",
       "  checked  \n",
       "0    True  \n",
       "1    True  \n",
       "2    True  \n",
       "3    True  \n",
       "4    True  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunExecutionResult(results=[RunResult(status=<RunStatus.Error: 'error'>, timing=[], thread_id='Thread-5 (worker)', execution_time=2.9518167972564697, adapter_response={}, message='Database Error in model tetst (models/user/tetst.sql)\\n  syntax error at or near \")\"\\n  LINE 14:   );\\n             ^\\n  compiled Code at target/run/dbt_financial/models/user/tetst.sql', failures=None, node=ModelNode(database='financial_data', schema='financial_user', name='tetst', resource_type=<NodeType.Model: 'model'>, package_name='dbt_financial', path='user/tetst.sql', original_file_path='models/user/tetst.sql', unique_id='model.dbt_financial.tetst', fqn=['dbt_financial', 'user', 'tetst'], alias='tetst', checksum=FileHash(name='sha256', checksum='0914864f78b4327814b8b537d6a6a724dfb24533730814a7129d43b775b9f5ed'), config=NodeConfig(_extra={'name': 'tetst', 'description': 'ffgfs'}, enabled=True, alias=None, schema='financial_user', database=None, tags=['1', 'user_created', '23/07/2023_13:27:18'], meta={}, group=None, materialized='table', incremental_strategy=None, persist_docs={}, post_hook=[], pre_hook=[], quoting={}, column_types={}, full_refresh=None, unique_key=None, on_schema_change='ignore', grants={}, packages=[], docs=Docs(show=True, node_color=None), contract=ContractConfig(enforced=False)), _event_status={}, tags=['1', 'user_created', '23/07/2023_13:27:18'], description='', columns={}, meta={}, group=None, docs=Docs(show=True, node_color=None), patch_path=None, build_path='target/run/dbt_financial/models/user/tetst.sql', deferred=False, unrendered_config={'materialized': 'table', 'name': 'tetst', 'description': 'ffgfs', 'tags': ['1', 'user_created', '23/07/2023_13:27:18'], 'schema': 'financial_user'}, created_at=1690093652.259449, config_call_dict={'materialized': 'table', 'name': 'tetst', 'description': 'ffgfs', 'tags': ['1', 'user_created', '23/07/2023_13:27:18'], 'schema': 'financial_user'}, relation_name='\"financial_data\".\"financial_user\".\"tetst\"', raw_code=\"{{ config(\\n    materialized='table',\\n    name='tetst',\\n    description='ffgfs',\\n    tags = ['1','user_created','23/07/2023_13:27:18'],\\n    schema = 'financial_user'\\n) }}\\n    -- depends_on: {{ref('dim_price_history')}}SELECT * from marts.dim_price_history\", language=<ModelLanguage.sql: 'sql'>, refs=[RefArgs(name='dim_price_history', package=None, version=None)], sources=[], metrics=[], depends_on=DependsOn(macros=[], nodes=['model.dbt_financial.dim_price_history']), compiled_path='target/compiled/dbt_financial/models/user/tetst.sql', compiled=True, compiled_code='\\n    -- depends_on: \"financial_data\".\"marts\".\"dim_price_history\"SELECT * from marts.dim_price_history', extra_ctes_injected=True, extra_ctes=[], _pre_injected_sql=None, contract=Contract(enforced=False, checksum=None), access=<AccessType.Protected: 'protected'>, constraints=[], version=None, latest_version=None), agate_table=None), RunResult(status=<RunStatus.Error: 'error'>, timing=[], thread_id='Thread-6 (worker)', execution_time=2.9538426399230957, adapter_response={}, message='Database Error in model unique_1 (models/user/unique_1.sql)\\n  syntax error at or near \")\"\\n  LINE 9:   );\\n            ^\\n  compiled Code at target/run/dbt_financial/models/user/unique_1.sql', failures=None, node=ModelNode(database='financial_data', schema='financial_user', name='unique_1', resource_type=<NodeType.Model: 'model'>, package_name='dbt_financial', path='user/unique_1.sql', original_file_path='models/user/unique_1.sql', unique_id='model.dbt_financial.unique_1', fqn=['dbt_financial', 'user', 'unique_1'], alias='unique_1', checksum=FileHash(name='sha256', checksum='893f1d138adef386e33eee76dae2064648fc25ed1c78e07eda3cbe07929346db'), config=NodeConfig(_extra={'name': 'unique_1', 'description': 'asd'}, enabled=True, alias=None, schema='financial_user', database=None, tags=['1', 'user_created', '23/07/2023_13:27:18'], meta={}, group=None, materialized='view', incremental_strategy=None, persist_docs={}, post_hook=[], pre_hook=[], quoting={}, column_types={}, full_refresh=None, unique_key=None, on_schema_change='ignore', grants={}, packages=[], docs=Docs(show=True, node_color=None), contract=ContractConfig(enforced=False)), _event_status={}, tags=['1', 'user_created', '23/07/2023_13:27:18'], description='', columns={}, meta={}, group=None, docs=Docs(show=True, node_color=None), patch_path=None, build_path='target/run/dbt_financial/models/user/unique_1.sql', deferred=False, unrendered_config={'materialized': 'view', 'name': 'unique_1', 'description': 'asd', 'tags': ['1', 'user_created', '23/07/2023_13:27:18'], 'schema': 'financial_user'}, created_at=1690093652.2877269, config_call_dict={'materialized': 'view', 'name': 'unique_1', 'description': 'asd', 'tags': ['1', 'user_created', '23/07/2023_13:27:18'], 'schema': 'financial_user'}, relation_name='\"financial_data\".\"financial_user\".\"unique_1\"', raw_code=\"{{ config(\\n    materialized='view',\\n    name='unique_1',\\n    description='asd',\\n    tags = ['1','user_created','23/07/2023_13:27:18'],\\n    schema = 'financial_user'\\n) }}\\n    -- depends_on: {{ref('dim_price_history')}}SELECT * from marts.dim_price_history\", language=<ModelLanguage.sql: 'sql'>, refs=[RefArgs(name='dim_price_history', package=None, version=None)], sources=[], metrics=[], depends_on=DependsOn(macros=[], nodes=['model.dbt_financial.dim_price_history']), compiled_path='target/compiled/dbt_financial/models/user/unique_1.sql', compiled=True, compiled_code='\\n    -- depends_on: \"financial_data\".\"marts\".\"dim_price_history\"SELECT * from marts.dim_price_history', extra_ctes_injected=True, extra_ctes=[], _pre_injected_sql=None, contract=Contract(enforced=False, checksum=None), access=<AccessType.Protected: 'protected'>, constraints=[], version=None, latest_version=None), agate_table=None)], elapsed_time=16.68212652206421, args={'defer': False, 'partial_parse': True, 'send_anonymous_usage_stats': True, 'use_colors': True, 'enable_legacy_logger': False, 'populate_cache': True, 'log_path': '/home/vu/Desktop/Projects/Thesis/financial/dbt/logs', 'macro_debugging': False, 'profiles_dir': '/home/vu/.dbt', 'which': 'run', 'log_level': 'info', 'static_parser': True, 'use_colors_file': True, 'log_format_file': 'debug', 'strict_mode': False, 'project_dir': '/home/vu/Desktop/Projects/Thesis/financial/dbt/', 'exclude': (), 'printer_width': 80, 'write_json': True, 'print': True, 'favor_state': False, 'introspect': True, 'cache_selected_only': False, 'quiet': False, 'warn_error_options': WarnErrorOptions(include=[], exclude=[]), 'version_check': True, 'log_level_file': 'debug', 'indirect_selection': 'eager', 'vars': {}, 'select': ('tag:23/07/2023_13:27:18',), 'log_format': 'default'}, generated_at=datetime.datetime(2023, 7, 23, 6, 27, 49, 98288))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{{ config(\n",
      "    materialized='table',\n",
      "    name='tetst',\n",
      "    description='ffgfs',\n",
      "    tags = ['1','user_created','23/07/2023_13:22:24'],\n",
      "    schema = 'financial_user'\n",
      ") }}\n"
     ]
    }
   ],
   "source": [
    "print(add_materialization(df.loc[i], \"\", EXEC_TIME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_names = set(get_tables_from_sql(\"SELECT * from marts.dim_price_history\", dialect=\"postgres\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_names.difference(dbt_tables_reporting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "UPDATE financial_query.query q \n",
    "                                SET success = v.success,\n",
    "                                    checked = v.checked\n",
    "\n",
    "                                FROM (values ('test_query_hgffhgf', 1, True, False), ('test_query_2_gftyf', 1, True, False), ('test_query_123', 1, True, False), ('tetst', 1, True, False)) AS v (name, user_id, checked, success)\n",
    "                                WHERE q.user_id = v.user_id \n",
    "                                AND q.name = v.name;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 'catvu113@gmail.com'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m email_dict[df\u001b[39m.\u001b[39;49mloc[i, \u001b[39m\"\u001b[39;49m\u001b[39muser_id\u001b[39;49m\u001b[39m\"\u001b[39;49m]]\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "email_dict[df.loc[i, \"user_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('test_query_hgffhgf', 1, False, False), ('test_query_2_gftyf', 1, False, False), ('test_query_123', 1, Null, False), ('tetst', 1, Null, False)\n"
     ]
    }
   ],
   "source": [
    "entries_to_update = str(tuple(zip(df.name, df.user_id, df.checked, df.success))).replace(\"None\", \"Null\")[1:-1]\n",
    "print(entries_to_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATE financial_query.query q \n",
      "                        SET success = v.success,\n",
      "                            checked = v.checked\n",
      "\n",
      "                        FROM (values ('test_query_hgffhgf', 1, True, False), ('test_query_2_gftyf', 1, True, False), ('test_query_123', 1, True, False), ('tetst', 1, True, False), ('unique_1', 1, True, False)) AS v (name, user_id, checked, success)\n",
      "                        WHERE q.user_id = v.user_id \n",
      "                        AND q.name = v.name;\n"
     ]
    }
   ],
   "source": [
    "update_sql_query = f\"\"\"UPDATE {QUERY_SCHEMA}.{QUERY_TABLE} q \n",
    "                        SET success = v.success,\n",
    "                            checked = v.checked\n",
    "\n",
    "                        FROM (values {entries_to_update}) AS v (name, user_id, checked, success)\n",
    "                        WHERE q.user_id = v.user_id \n",
    "                        AND q.name = v.name;\"\"\"\n",
    "print(update_sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_string</th>\n",
       "      <th>materialization</th>\n",
       "      <th>user_id</th>\n",
       "      <th>description</th>\n",
       "      <th>insert_time</th>\n",
       "      <th>name</th>\n",
       "      <th>checked</th>\n",
       "      <th>success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SELECT * from dim_bollinger</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2023-07-23 08:11:58.188941</td>\n",
       "      <td>unique_2</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SELECT * from marts.dim_bollinger</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>asdad</td>\n",
       "      <td>2023-07-23 08:12:43.670466</td>\n",
       "      <td>unique_3</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         query_string  materialization  user_id description  \\\n",
       "0        SELECT * from dim_bollinger                 2        1               \n",
       "1  SELECT * from marts.dim_bollinger                 2        1       asdad   \n",
       "\n",
       "                 insert_time      name  checked success  \n",
       "0 2023-07-23 08:11:58.188941  unique_2    False    None  \n",
       "1 2023-07-23 08:12:43.670466  unique_3    False    None  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select * from financial_query.query where checked = False\n",
      "PostgreSQL connection is closed\n",
      "unique_2\n",
      "True\n",
      "True\n",
      "Wrote model unique_2 contents\n",
      "unique_3\n",
      "True\n",
      "True\n",
      "Wrote model unique_3 contents\n",
      "entries\n",
      "('unique_2', 1, False, Null), ('unique_3', 1, False, Null)\n",
      "UPDATE financial_query.query q \n",
      "                                SET success = v.success,\n",
      "                                    checked = v.checked\n",
      "\n",
      "                                FROM (values ('unique_2', 1, False, Null), ('unique_3', 1, False, Null)) AS v (name, user_id, checked, success)\n",
      "                                WHERE q.user_id = v.user_id \n",
      "                                AND q.name = v.name;\n",
      "Error while updating data in PostgreSQL column \"success\" is of type boolean but expression is of type text\n",
      "LINE 2:                                 SET success = v.success,\n",
      "                                                      ^\n",
      "HINT:  You will need to rewrite or cast the expression.\n",
      "\n",
      "PostgreSQL connection is closed\n"
     ]
    },
    {
     "ename": "InFailedSqlTransaction",
     "evalue": "current transaction is aborted, commands ignored until end of transaction block\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDatatypeMismatch\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 196\u001b[0m, in \u001b[0;36mupdate_records\u001b[0;34m(update_values)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[39mprint\u001b[39m(update_sql_query)\n\u001b[0;32m--> 196\u001b[0m     cursor\u001b[39m.\u001b[39;49mexecute(update_sql_query)\n\u001b[1;32m    197\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mException\u001b[39;00m, psycopg2\u001b[39m.\u001b[39mError) \u001b[39mas\u001b[39;00m error:\n",
      "\u001b[0;31mDatatypeMismatch\u001b[0m: column \"success\" is of type boolean but expression is of type text\nLINE 2:                                 SET success = v.success,\n                                                      ^\nHINT:  You will need to rewrite or cast the expression.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInFailedSqlTransaction\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 106\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mentries\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    105\u001b[0m     \u001b[39mprint\u001b[39m(entries_to_update)\n\u001b[0;32m--> 106\u001b[0m     update_records(entries_to_update)\n\u001b[1;32m    109\u001b[0m \u001b[39m# initialize\u001b[39;00m\n\u001b[1;32m    110\u001b[0m dbt \u001b[39m=\u001b[39m dbtRunner()\n",
      "Cell \u001b[0;32mIn[4], line 200\u001b[0m, in \u001b[0;36mupdate_records\u001b[0;34m(update_values)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mException\u001b[39;00m, psycopg2\u001b[39m.\u001b[39mError) \u001b[39mas\u001b[39;00m error:\n\u001b[1;32m    198\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mError while updating data in PostgreSQL\u001b[39m\u001b[39m\"\u001b[39m, error)\n\u001b[0;32m--> 200\u001b[0m     cursor\u001b[39m.\u001b[39;49mexecute(update_sql_query)\n\u001b[1;32m    202\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[39m# closing database connection.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     \u001b[39mif\u001b[39;00m connection:\n",
      "\u001b[0;31mInFailedSqlTransaction\u001b[0m: current transaction is aborted, commands ignored until end of transaction block\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'23/07/2023_15:28:01'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXEC_TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m08:23:04  Running with dbt=1.5.1\n",
      "\u001b[0m08:23:04  Found 34 models, 73 tests, 6 snapshots, 0 analyses, 760 macros, 0 operations, 0 seed files, 12 sources, 1 exposure, 0 metrics, 0 groups\n",
      "\u001b[0m08:23:04  \n",
      "\u001b[0m08:23:15  Concurrency: 4 threads (target='dev_cloud')\n",
      "\u001b[0m08:23:15  \n",
      "\u001b[0m08:23:15  1 of 2 START sql view model financial_user.unique_2 ............................ [RUN]\n",
      "\u001b[0m08:23:15  2 of 2 START sql view model financial_user.unique_3 ............................ [RUN]\n",
      "\u001b[0m08:23:18  1 of 2 ERROR creating sql view model financial_user.unique_2 ................... [\u001b[31mERROR\u001b[0m in 3.17s]\n",
      "\u001b[0m08:23:19  2 of 2 OK created sql view model financial_user.unique_3 ....................... [\u001b[32mCREATE VIEW\u001b[0m in 4.27s]\n",
      "\u001b[0m08:23:21  \n",
      "\u001b[0m08:23:21  Finished running 2 view models in 0 hours 0 minutes and 16.71 seconds (16.71s).\n",
      "\u001b[0m08:23:21  \n",
      "\u001b[0m08:23:21  \u001b[31mCompleted with 1 error and 0 warnings:\u001b[0m\n",
      "\u001b[0m08:23:21  \n",
      "\u001b[0m08:23:21  \u001b[33mDatabase Error in model unique_2 (models/user/unique_2.sql)\u001b[0m\n",
      "\u001b[0m08:23:21    relation \"dim_bollinger\" does not exist\n",
      "\u001b[0m08:23:21    LINE 9:     SELECT * from dim_bollinger\n",
      "\u001b[0m08:23:21                              ^\n",
      "\u001b[0m08:23:21    compiled Code at target/run/dbt_financial/models/user/unique_2.sql\n",
      "\u001b[0m08:23:21  \n",
      "\u001b[0m08:23:21  Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2\n"
     ]
    }
   ],
   "source": [
    "    # initialize\n",
    "    dbt = dbtRunner()\n",
    "\n",
    "    # create CLI args as a list of strings\n",
    "    cli_args = [\n",
    "        \"run\",\n",
    "        \"--project-dir\",\n",
    "        DBT_PROJECT_DIR,\n",
    "        \"--select\",\n",
    "        \"tag:{exec_time}\".format(exec_time=EXEC_TIME),\n",
    "    ]\n",
    "\n",
    "    # run the command\n",
    "    res: dbtRunnerResult = dbt.invoke(cli_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT * from dim_bollinger\n",
    "\"\"\"\n",
    "parsed=sqlfluff.parse(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "partially_model, processed_status = get_ref(sql, dbt_tables, parsed, dbt_tables_reporting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    -- depends_on: {{ref('dim_bollinger')}}\\n    \\nSELECT * from dim_bollinger\\n\""
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partially_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tables referenced out of serving schemas', 'Success']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "422 Client Error: UNPROCESSABLE ENTITY for url: http://34.82.185.252:30007/api/v1/dataset/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39m# Serializing json\u001b[39;00m\n\u001b[1;32m     10\u001b[0m json_object \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mdumps(dictionary)\n\u001b[0;32m---> 11\u001b[0m response \u001b[39m=\u001b[39m superset\u001b[39m.\u001b[39;49mrequest(\u001b[39m\"\u001b[39;49m\u001b[39mPOST\u001b[39;49m\u001b[39m\"\u001b[39;49m, rison_request, json\u001b[39m=\u001b[39;49mdictionary)\n",
      "Cell \u001b[0;32mIn[3], line 96\u001b[0m, in \u001b[0;36mSupersetDBTConnectorSession.request\u001b[0;34m(self, method, endpoint, refresh_session_if_needed, headers, **request_kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession\u001b[39m.\u001b[39mrequest(method, url, headers\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheaders, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mrequest_kwargs)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mRequest finished with status: \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m, res\u001b[39m.\u001b[39mstatus_code)\n\u001b[0;32m---> 96\u001b[0m res\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m     97\u001b[0m \u001b[39mreturn\u001b[39;00m res\u001b[39m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/Desktop/Projects/Thesis/venv/lib/python3.10/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     http_error_msg \u001b[39m=\u001b[39m (\n\u001b[1;32m   1017\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m Server Error: \u001b[39m\u001b[39m{\u001b[39;00mreason\u001b[39m}\u001b[39;00m\u001b[39m for url: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1018\u001b[0m     )\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 422 Client Error: UNPROCESSABLE ENTITY for url: http://34.82.185.252:30007/api/v1/dataset/"
     ]
    }
   ],
   "source": [
    "rison_request=\"/dataset\"\n",
    "dictionary = {\n",
    "    # Parameter database\n",
    "    \"database\": DATABASE_ID,\n",
    "    \"schema\": USER_SCHEMA,\n",
    "    \"table_name\": df.loc[i, \"name\"],\n",
    "    \"owners\": [int(df.loc[i, \"user_id\"]), SUPERSET_ID],\n",
    "}\n",
    "# Serializing json\n",
    "json_object = json.dumps(dictionary)\n",
    "response = superset.request(\"POST\", rison_request, json=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 113,\n",
       " 'result': {'database': 1,\n",
       "  'owners': [1, 34],\n",
       "  'schema': 'financial_user',\n",
       "  'table_name': 'unique_4'}}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "rison_request=\"/dataset\"\n",
    "dictionary = {\n",
    "    # Parameter database\n",
    "    \"database\": DATABASE_ID,\n",
    "    \"schema\": USER_SCHEMA,\n",
    "    \"table_name\": \"unique_3\",\n",
    "    \"owners\": [int(df.loc[i, \"user_id\"]), SUPERSET_ID],\n",
    "}\n",
    "# Serializing json\n",
    "json_object = json.dumps(dictionary)\n",
    "response = superset.request(\"POST\", rison_request, json=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m09:48:04  Running with dbt=1.5.1\n",
      "\u001b[0m09:48:05  Performance info: target/perf_info.json\n"
     ]
    }
   ],
   "source": [
    "dbt = dbtRunner()\n",
    "cli_args = [\"parse\",\"--project-dir\",\n",
    "    DBT_PROJECT_DIR,]\n",
    "res = dbt.invoke(cli_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_string</th>\n",
       "      <th>materialization</th>\n",
       "      <th>user_id</th>\n",
       "      <th>description</th>\n",
       "      <th>insert_time</th>\n",
       "      <th>name</th>\n",
       "      <th>checked</th>\n",
       "      <th>success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SELECT * from dim_bollinger</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2023-07-23 08:11:58.188941</td>\n",
       "      <td>unique_2</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SELECT * from marts.dim_bollinger</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>asdad</td>\n",
       "      <td>2023-07-23 08:12:43.670466</td>\n",
       "      <td>unique_3</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         query_string  materialization  user_id description  \\\n",
       "0        SELECT * from dim_bollinger                 2        1               \n",
       "1  SELECT * from marts.dim_bollinger                 2        1       asdad   \n",
       "\n",
       "                 insert_time      name  checked success  \n",
       "0 2023-07-23 08:11:58.188941  unique_2    False    None  \n",
       "1 2023-07-23 08:12:43.670466  unique_3    False    None  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_2: error\n",
      "unique_3: success\n"
     ]
    }
   ],
   "source": [
    "for r in res.result:\n",
    "    print(f\"{r.node.name}: {r.status}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
