{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Union\n",
    "import pandas as pd\n",
    "import logging\n",
    "import re\n",
    "import sqlfluff\n",
    "import requests\n",
    "import ruamel.yaml\n",
    "import psycopg2\n",
    "from bs4 import BeautifulSoup\n",
    "from markdown import markdown\n",
    "import datetime\n",
    "from urllib.parse import unquote\n",
    "from typing import Any, Dict, Iterator, List, Union\n",
    "import json\n",
    "from pgsanity.pgsanity import check_string\n",
    "import os\n",
    "from dbt.cli.main import dbtRunner, dbtRunnerResult\n",
    "import smtplib\n",
    "import ssl\n",
    "from itertools import compress\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from jinja2 import Template\n",
    "with open(DBT_PROJECT_DIR + \"create_model.txt\", \"r\") as f:\n",
    "    MODEL_TEMPLATE = Template(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "MATERIALIZATION_MAPPING = {1: \"table\", 2: \"view\", 3: \"incremental\", 4: \"ephemereal\"}\n",
    "SUPERSET_USERNAME = \"fdp\"\n",
    "SUPERSET_PASSWORD = \"fdp\"\n",
    "SUPERSET_HOST = \"http://34.82.185.252:30007/\"\n",
    "DATABASE_USERNAME = \"fdp\"\n",
    "DATABASE_PASSWORD = \"fdp\"\n",
    "DATABASE_HOST = \"34.82.185.252\"\n",
    "DATABASE_PORT = 30007\n",
    "DATABASE_NAME = \"financial_data\"\n",
    "QUERY_SCHEMA=\"financial_query\"\n",
    "QUERY_TABLE=\"query\"\n",
    "MANIFEST_PATH=\"/home/vu/Desktop/Projects/Thesis/financial/dbt/target/manifest.json\"\n",
    "EMAIL_PORT = 465\n",
    "SMTP = \"smtp.gmail.com\"\n",
    "EMAIL_SENDER = \"catvu113@gmail.com\"\n",
    "EMAIL_PASSWORD = \"xhtzakhmnsbufufy\"\n",
    "USER_MODEL_PATH = \"/home/vu/Desktop/Projects/Thesis/financial/dbt/models/user\"\n",
    "DBT_PROJECT_DIR = \"/home/vu/Desktop/Projects/Thesis/financial/dbt/\"\n",
    "DATABASE_ID = 1\n",
    "SUPERSET_ID = 36\n",
    "USER_SCHEMA = \"user\"\n",
    "SERVING_SCHEMA=\"marts\"\n",
    "context = ssl.create_default_context()\n",
    "SMTP = smtplib.SMTP_SSL(\"smtp.gmail.com\", 465, context=context)\n",
    "SST_DATABASE_NAME = \"FDP Reader\"\n",
    "SERVING_AND_USER_WITH_DOT = (SERVING_SCHEMA+'.',USER_SCHEMA+'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_187818/1731185254.py:2: ExperimentalWarning: \"DagsterGraphQLClient\" is an experimental class. It may break in future versions, even between dot releases. To mute warnings for experimental functionality, invoke warnings.filterwarnings(\"ignore\", category=dagster.ExperimentalWarning) or use one of the other methods described at https://docs.python.org/3/library/warnings.html#describing-warning-filters.\n",
      "  client = DagsterGraphQLClient(\"34.82.185.252\", port_number=30003)\n"
     ]
    }
   ],
   "source": [
    "from dagster_graphql import DagsterGraphQLClient\n",
    "client = DagsterGraphQLClient(\"34.82.185.252\", port_number=30003)\n",
    "status = client.reload_repository_location(\"financial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupersetDBTSessionConnector:\n",
    "    \"\"\"A class for accessing the Superset API in an easy way.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Instantiates the class.\n",
    "\n",
    "        ''access_token'' will be instantiated via enviromental variable\n",
    "        If ``access_token`` is None, attempts to obtain it using ``refresh_token``.\n",
    "\n",
    "        Args:\n",
    "            api_url: Base API URL of a Superset instance, e.g. https://my-superset/api/v1.\n",
    "            access_token: Access token to use for accessing protected endpoints of the Superset\n",
    "                API. Can be automatically obtained if ``refresh_token`` is not None.\n",
    "            refresh_token: Refresh token to use for obtaining or refreshing the ``access_token``.\n",
    "                If None, no refresh will be done.\n",
    "        \"\"\"\n",
    "        self.url = SUPERSET_HOST\n",
    "        self.api_url = self.url + \"api/v1/\"\n",
    "\n",
    "        self.session = requests.session()\n",
    "\n",
    "        self.username = SUPERSET_USERNAME\n",
    "        self.password = SUPERSET_PASSWORD\n",
    "\n",
    "        self.refresh_session()\n",
    "\n",
    "    def refresh_session(self):\n",
    "        logging.info(\"Refreshing session\")\n",
    "\n",
    "        soup = BeautifulSoup(self.session.post(self.url + \"login\").text, \"html.parser\")\n",
    "        self.csrf_token = soup.find(\"input\", {\"id\": \"csrf_token\"})[\"value\"]  # type: ignore\n",
    "\n",
    "        data = {\n",
    "            \"username\": self.username,\n",
    "            \"password\": self.password,\n",
    "            \"provider\": \"db\",\n",
    "            \"refresh\": True,\n",
    "        }\n",
    "        headers = {\n",
    "            # 'Authorization': 'Bearer {}'.format(self.____access_token),\n",
    "            \"x-csrftoken\": self.csrf_token,\n",
    "        }\n",
    "        response = self.session.post(self.url + \"login\", json=data, headers=headers)  # type: ignore\n",
    "        return True\n",
    "\n",
    "    def request(self, method, endpoint, **request_kwargs):\n",
    "        \"\"\"Executes a request against the Superset API.\n",
    "\n",
    "        Args:\n",
    "            method: HTTP method to use.\n",
    "            endpoint: Endpoint to use.\n",
    "            **request_kwargs: Any ``requests.request`` arguments to use.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing response body parsed from JSON.\n",
    "\n",
    "        Raises:\n",
    "            HTTPError: There is an HTTP error (detected by ``requests.Response.raise_for_status``)\n",
    "                even after retrying with a fresh session.\n",
    "        \"\"\"\n",
    "\n",
    "        logging.info(\"About to %s execute request for endpoint %s\", method, endpoint)\n",
    "\n",
    "        url = self.api_url + endpoint\n",
    "        csrf_headers = {\n",
    "            # 'Authorization': 'Bearer {}'.format(self.__access_token),\n",
    "            \"x-csrftoken\": self.csrf_token,\n",
    "        }\n",
    "\n",
    "        res = self.session.request(method, url, headers=csrf_headers, **request_kwargs)  # type: ignore\n",
    "\n",
    "        logging.info(\"Request finished with status: %d\", res.status_code)\n",
    "\n",
    "        if res.status_code == 401 and self.refresh_session():\n",
    "            logging.info(f\"Retrying {method} request for {url} %s with refreshed session\")\n",
    "            res = self.session.request(method, url, headers=csrf_headers, **request_kwargs)  # type: ignore\n",
    "\n",
    "            logging.info(\"Request finished with status: %d\", res.status_code)\n",
    "\n",
    "        if res.status_code == 400 and self.refresh_session():\n",
    "            logging.info(f\"Retrying {method} request for {url} %s with refreshed session\")\n",
    "            res = self.session.request(method, url, headers=csrf_headers, **request_kwargs)  # type: ignore\n",
    "            logging.info(f\"Request finished with status: {res.status_code}\")\n",
    "        res.raise_for_status()\n",
    "        return res.json()\n",
    "\n",
    "\n",
    "def get_tables_from_dbt(dbt_manifest, dbt_db_name):\n",
    "    tables = {}\n",
    "    for table_type in [\"nodes\"]:\n",
    "        manifest_subset = dbt_manifest[table_type]\n",
    "\n",
    "        for table_key_long in manifest_subset:\n",
    "            table = manifest_subset[table_key_long]\n",
    "            name = table[\"name\"]\n",
    "            schema = table[\"schema\"]\n",
    "            database = table[\"database\"]\n",
    "            description = table[\"description\"]\n",
    "            alias = table[\"alias\"]\n",
    "            source = table[\"unique_id\"].split(\".\")[-2]\n",
    "            table_key = schema + \".\" + alias  # Key will be alias, not name\n",
    "            columns = table[\"columns\"]\n",
    "\n",
    "            if dbt_db_name is None or database == dbt_db_name:\n",
    "                # fail if it breaks uniqueness constraint\n",
    "                assert table_key not in tables, (\n",
    "                    f\"Table {table_key} is a duplicate name (schema + table) across databases. \"\n",
    "                    \"This would result in incorrect matching between Superset and dbt. \"\n",
    "                    \"To fix this, remove duplicates or add ``dbt_db_name``.\"\n",
    "                )\n",
    "                tables[table_key] = {\n",
    "                    \"name\": name,\n",
    "                    \"schema\": schema,\n",
    "                    \"database\": database,\n",
    "                    \"type\": table_type[:-1],\n",
    "                    \"ref\": f\"ref('{name}')\" if table_type == \"nodes\" else f\"source('{source}', '{name}')\",\n",
    "                    \"user\": None,\n",
    "                    \"columns\": columns,\n",
    "                    \"description\": description,\n",
    "                    \"alias\": alias,\n",
    "                }\n",
    "            if schema == \"user\":\n",
    "                tables[table_key][\"user\"] = table[\"tags\"][0]\n",
    "\n",
    "    assert tables, \"Manifest is empty!\"\n",
    "\n",
    "    return tables\n",
    "\n",
    "\n",
    "def get_physical_datasets_from_superset(superset: SupersetDBTSessionConnector, superset_db_id):\n",
    "    logging.info(\"Getting physical datasets from Superset.\")\n",
    "    page_number = 0\n",
    "    datasets = []\n",
    "    datasets_keys = set()\n",
    "    while True:\n",
    "        logging.info(\"Getting page %d.\", page_number + 1)\n",
    "        rison_request = f\"dataset/?q=(page_size:100,page:{page_number},order_column:changed_on_delta_humanized,order_direction:asc,filters:!((col:table_name,opr:nct,value:archived),(col:sql,opr:dataset_is_null_or_empty,value:true)))\"\n",
    "        res = superset.request(\"GET\", rison_request)\n",
    "        result = res[\"result\"]\n",
    "        if result:\n",
    "            for r in result:\n",
    "                name = r[\"table_name\"]\n",
    "                schema = r[\"schema\"]\n",
    "                database_name = r[\"database\"][\"database_name\"]\n",
    "                dataset_id = r[\"id\"]\n",
    "                database_id = r[\"database\"][\"id\"]\n",
    "                dataset_key = f\"{schema}.{name}\"  # same format as in dashboards\n",
    "\n",
    "                kind = r[\"kind\"]\n",
    "                if kind == \"physical\" and (superset_db_id is None or database_id == superset_db_id):\n",
    "                    dataset_id = r[\"id\"]\n",
    "\n",
    "                    name = r[\"table_name\"]\n",
    "                    schema = r[\"schema\"]\n",
    "                    dataset_key = f\"{schema}.{name}\"  # used as unique identifier\n",
    "\n",
    "                    dataset_dict = {\n",
    "                        \"id\": dataset_id,\n",
    "                        \"name\": name,\n",
    "                        \"schema\": schema,\n",
    "                        \"database\": database_name,\n",
    "                        \"dataset_id\": dataset_id,\n",
    "                        \"key\": dataset_key,\n",
    "                        \"table\": [dataset_key],\n",
    "                    }\n",
    "\n",
    "                    # fail if it breaks uniqueness constraint\n",
    "                    assert dataset_key not in datasets_keys, (\n",
    "                        f\"Dataset {dataset_key} is a duplicate name (schema + table) \"\n",
    "                        \"across databases. \"\n",
    "                        \"This would result in incorrect matching between Superset and dbt. \"\n",
    "                        \"To fix this, remove duplicates or add the ``superset_db_id`` argument.\"\n",
    "                    )\n",
    "\n",
    "                    datasets_keys.add(dataset_key)\n",
    "                    datasets.append(dataset_dict)\n",
    "\n",
    "            page_number += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def get_tables_from_sql_simple(sql):\n",
    "    \"\"\"\n",
    "    (Superset) Fallback SQL parsing using regular expressions to get tables names, ignoring tables without schema.\n",
    "    \"\"\"\n",
    "    sql = re.sub(r\"(--.*)|(#.*)\", \"\", sql)\n",
    "    sql = re.sub(r\"\\s+\", \" \", sql)\n",
    "    sql = re.sub(r\"(/\\*(.|\\n)*\\*/)\", \"\", sql)\n",
    "\n",
    "    regex = re.compile(r\"\\b(from|join)\\b\\s+(\\\"?(\\w+)\\\"?(\\.))?\\\"?(\\w+)\\\"?\\b\")\n",
    "    tables_match = regex.findall(sql)\n",
    "    tables = [\n",
    "        table[2] + \".\" + table[4] if table[2] != \"\" else table[4] for table in tables_match if table[4] != \"unnest\"\n",
    "    ]\n",
    "\n",
    "    tables = list(set(tables))\n",
    "    final_tables = [table for table in tables if table.startswith(SERVING_AND_USER_WITH_DOT)] # Assuming that no schema reference\n",
    "    return tables\n",
    "\n",
    "\n",
    "\n",
    "def get_tables_from_sql(sql, dialect, sql_parsed=None):\n",
    "    \"\"\"\n",
    "    (Superset) SQL parsing using sqlfluff to get clean tables names.\n",
    "    If sqlfluff parsing fails it runs the above regex parsing func.\n",
    "    Returns a tables list.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not sql_parsed:\n",
    "            sql_parsed = sqlfluff.parse(sql, dialect=dialect)\n",
    "        tables_raw = list(get_json_segment(sql_parsed, \"table_reference\"))  # type: ignore\n",
    "        tables_cleaned = []  # With schema\n",
    "        for table_ref in tables_raw:\n",
    "            if isinstance(table_ref, list):\n",
    "                table_ref_identifier = []\n",
    "                # Get last 2 \"naked_identifier\"\n",
    "                for dictionary in table_ref[::-1]:\n",
    "                    if \"naked_identifier\" in dictionary:\n",
    "                        table_ref_identifier.append(dictionary[\"naked_identifier\"])\n",
    "                        if len(table_ref_identifier) == 2:\n",
    "                            tables_cleaned.append(\".\".join(table_ref_identifier[::-1]))\n",
    "                            break\n",
    "            if isinstance(table_ref, dict):\n",
    "                tables_cleaned.append(table_ref[\"naked_identifier\"])\n",
    "    except (\n",
    "        sqlfluff.core.errors.SQLParseError,  # type: ignore\n",
    "        sqlfluff.core.errors.SQLLexError,  # type: ignore\n",
    "        sqlfluff.core.errors.SQLFluffUserError,  # type: ignore\n",
    "        sqlfluff.api.simple.APIParsingError,  # type: ignore\n",
    "    ) as e:  # type: ignore\n",
    "        logging.warning(\n",
    "            \"Parsing SQL through sqlfluff failed. \"\n",
    "            \"Let me attempt this via regular expressions at least and \"\n",
    "            \"check the problematic query and error below.\\n%s\",\n",
    "            sql,\n",
    "            exc_info=e,\n",
    "        )\n",
    "        tables_cleaned = get_tables_from_sql_simple(sql)\n",
    "\n",
    "    return tables_cleaned\n",
    "\n",
    "\n",
    "def get_json_segment(\n",
    "    parse_result: Dict[str, Any], segment_type: str\n",
    ") -> Iterator[Union[str, Dict[str, Any], List[Dict[str, Any]]]]:\n",
    "    \"\"\"Recursively search JSON parse result for specified segment type.\n",
    "\n",
    "    Args:\n",
    "        parse_result (Dict[str, Any]): JSON parse result from `sqlfluff.fix`.\n",
    "        segment_type (str): The segment type to search for.\n",
    "\n",
    "    Yields:\n",
    "        Iterator[Union[str, Dict[str, Any], List[Dict[str, Any]]]]:\n",
    "        Retrieves children of specified segment type as either a string for a raw\n",
    "        segment or as JSON or an array of JSON for non-raw segments.\n",
    "    \"\"\"\n",
    "    for k, v in parse_result.items():\n",
    "        if k == segment_type:\n",
    "            yield v\n",
    "        elif isinstance(v, dict):\n",
    "            yield from get_json_segment(v, segment_type)\n",
    "        elif isinstance(v, list):\n",
    "            for s in v:\n",
    "                yield from get_json_segment(s, segment_type)\n",
    "\n",
    "\n",
    "def get_dashboards_from_superset(superset: SupersetDBTSessionConnector, superset_db_id, user_id):\n",
    "    \"\"\"\n",
    "    This function gets\n",
    "    1. Get dashboards id list from Superset iterating on the pages of the url\n",
    "    2. Get a dashboard detail information :\n",
    "        title, owner, url, unique datasets names\n",
    "\n",
    "    Returns dashboards, dashboards_datasets\n",
    "    \"\"\"\n",
    "\n",
    "    logging.info(\"Getting published dashboards from Superset.\")\n",
    "    page_number = 0\n",
    "    dashboards_id = []\n",
    "    while True:\n",
    "        logging.info(\"Getting page %d.\", page_number + 1)\n",
    "        res = superset.request(\"GET\", f'/dashboard/?q={{\"page\":{page_number},\"page_size\":100}}')\n",
    "        result = res[\"result\"]\n",
    "        if result:\n",
    "            for r in result:\n",
    "                if r[\"published\"] and r[\"created_by\"][\"id\"] == user_id:\n",
    "                    dashboards_id.append(r[\"id\"])\n",
    "            page_number += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    assert len(dashboards_id) > 0, \"There are no dashboards in Superset!\"\n",
    "\n",
    "    logging.info(\"There are %d published dashboards in Superset.\", len(dashboards_id))\n",
    "\n",
    "    dashboards = []\n",
    "    dashboards_datasets_w_db = set()\n",
    "    for i, d in enumerate(dashboards_id):\n",
    "        logging.info(\"Getting info for dashboard %d/%d.\", i + 1, len(dashboards_id))\n",
    "        res = superset.request(\"GET\", f\"/dashboard/{d}\")\n",
    "        result = res[\"result\"]\n",
    "\n",
    "        dashboard_id = result[\"id\"]\n",
    "        title = result[\"dashboard_title\"]\n",
    "        url = SUPERSET_PUBLIC_HOST + \"superset/dashboard/\" + str(dashboard_id)\n",
    "        owner_name = result[\"owners\"][0][\"first_name\"] + \" \" + result[\"owners\"][0][\"last_name\"]\n",
    "\n",
    "        # take unique dataset names, formatted as \"[database].[schema].[table]\" by Superset\n",
    "        res_table_names = superset.request(\"GET\", f\"/dashboard/{d}/datasets\")\n",
    "        result_table_names = res_table_names[\"result\"]\n",
    "\n",
    "        testing = []\n",
    "        for i in range(0, len(result_table_names)):\n",
    "            testing.append(result_table_names[i][\"name\"])\n",
    "\n",
    "        # datasets_raw = list(set(result['table_names'].split(', ')))\n",
    "        datasets_raw = testing\n",
    "\n",
    "        # parse dataset names into parts\n",
    "        datasets_parsed = [dataset[1:-1].split(\"].[\", maxsplit=2) for dataset in datasets_raw]\n",
    "        datasets_parsed = [\n",
    "            [dataset[0], \"None\", dataset[1]]  # add None in the middle\n",
    "            if len(dataset) == 2\n",
    "            else dataset  # if missing the schema\n",
    "            for dataset in datasets_parsed\n",
    "        ]\n",
    "\n",
    "        # put them all back together to get \"database.schema.table\"\n",
    "        datasets_w_db = [\".\".join(dataset) for dataset in datasets_parsed]\n",
    "\n",
    "        dbt_project_name = \"your_dbt_project.\"\n",
    "        datasets_w_db = [dbt_project_name + sub for sub in testing]\n",
    "\n",
    "        dashboards_datasets_w_db.update(datasets_w_db)\n",
    "\n",
    "        # skip database, i.e. first item, to get only \"schema.table\"\n",
    "        datasets_wo_db = [\".\".join(dataset[1:]) for dataset in datasets_parsed]\n",
    "\n",
    "        datasets_wo_db = testing\n",
    "        dashboard = {\n",
    "            \"id\": dashboard_id,\n",
    "            \"title\": title,\n",
    "            \"url\": url,\n",
    "            \"owner_name\": owner_name,\n",
    "            \"datasets\": datasets_wo_db,  # add in \"schema.table\" format\n",
    "        }\n",
    "        dashboards.append(dashboard)\n",
    "    # test if unique when database disregarded\n",
    "    # loop to get the name of duplicated dataset and work with unique set of datasets w db\n",
    "    dashboards_datasets = set()\n",
    "    for dataset_w_db in dashboards_datasets_w_db:\n",
    "        dataset = \".\".join(dataset_w_db.split(\".\")[1:])  # similar logic as just a bit above\n",
    "\n",
    "        # fail if it breaks uniqueness constraint and not limited to one database\n",
    "        assert dataset not in dashboards_datasets or superset_db_id is not None, (\n",
    "            f\"Dataset {dataset} is a duplicate name (schema + table) across databases. \"\n",
    "            \"This would result in incorrect matching between Superset and dbt. \"\n",
    "            \"To fix this, remove duplicates or add ``superset_db_id``.\"\n",
    "        )\n",
    "\n",
    "        dashboards_datasets.add(dataset)\n",
    "\n",
    "    return dashboards, dashboards_datasets\n",
    "\n",
    "\n",
    "def get_datasets_from_superset_dbt_refs(\n",
    "    superset: SupersetDBTSessionConnector, dashboards_datasets, dbt_tables, sql_dialect, superset_db_id\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns datasets (dict) containing table info and dbt references\n",
    "    \"\"\"\n",
    "\n",
    "    logging.info(\"Getting datasets info from Superset.\")\n",
    "    page_number = 0\n",
    "    datasets = {}\n",
    "    while True:\n",
    "        logging.info(\"Getting page %d.\", page_number + 1)\n",
    "        res = superset.request(\"GET\", f'/dataset/?q={{\"page\":{page_number},\"page_size\":100}}')\n",
    "        result = res[\"result\"]\n",
    "        if result:\n",
    "            for r in result:\n",
    "                name = r[\"table_name\"]\n",
    "                schema = r[\"schema\"]\n",
    "                database_name = r[\"database\"][\"database_name\"]\n",
    "                database_id = r[\"database\"][\"id\"]\n",
    "\n",
    "                dataset_key = f\"{schema}.{name}\"  # same format as in dashboards\n",
    "\n",
    "                # only add datasets that are in dashboards, optionally limit to one database\n",
    "                if dataset_key in dashboards_datasets and (superset_db_id is None or database_id == superset_db_id):\n",
    "                    kind = r[\"kind\"]\n",
    "                    if kind == \"virtual\":  # built on custom sql\n",
    "                        sql = r[\"sql\"]\n",
    "                        tables = get_tables_from_sql(sql, sql_dialect)\n",
    "                        tables = [table if \".\" in table else f\"{schema}.{table}\" for table in tables]\n",
    "                    else:  # built on tables\n",
    "                        tables = [dataset_key]\n",
    "                    dbt_refs = [dbt_tables[table][\"ref\"] for table in tables if table in dbt_tables]\n",
    "\n",
    "                    datasets[dataset_key] = {\n",
    "                        \"name\": name,\n",
    "                        \"schema\": schema,\n",
    "                        \"database\": database_name,\n",
    "                        \"kind\": kind,\n",
    "                        \"tables\": tables,\n",
    "                        \"dbt_refs\": dbt_refs,\n",
    "                    }\n",
    "            page_number += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def refresh_columns_in_superset(superset: SupersetDBTSessionConnector, dataset_id):\n",
    "    logging.info(\"Refreshing columns in Superset.\")\n",
    "    superset.request(\"PUT\", f\"/dataset/{dataset_id}/refresh\")\n",
    "\n",
    "\n",
    "def add_sst_dataset_metadata(superset: SupersetDBTSessionConnector, dataset_id, sst_dataset_key, dbt_tables):\n",
    "    logging.info(\"Refreshing columns in Superset.\")\n",
    "    body = {\n",
    "        \"extra\": '{\"certification\": \\n  {\"certified_by\": \"Data Analytics Team\", \\n   \"details\": \"This table is the source of truth.\" \\n    \\n  }\\n}',\n",
    "        \"description\": dbt_tables[sst_dataset_key][\"description\"],\n",
    "        \"owners\": [SUPERSET_ID],\n",
    "    }\n",
    "    if dbt_tables[sst_dataset_key][\"user\"]:\n",
    "        body[\"owners\"].append(dbt_tables[sst_dataset_key][\"user\"])\n",
    "    superset.request(\"PUT\", f\"/dataset/{dataset_id}\", json=body)\n",
    "\n",
    "\n",
    "def add_superset_columns(superset: SupersetDBTSessionConnector, dataset):\n",
    "    logging.info(\"Pulling fresh columns info from Superset.\")\n",
    "    res = superset.request(\"GET\", f\"/dataset/{dataset['id']}\")\n",
    "    columns = res[\"result\"][\"columns\"]\n",
    "    dataset[\"columns\"] = columns\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def convert_markdown_to_plain_text(md_string):\n",
    "    \"\"\"Converts a markdown string to plaintext.\n",
    "\n",
    "    The following solution is used:\n",
    "    https://gist.github.com/lorey/eb15a7f3338f959a78cc3661fbc255fe\n",
    "    \"\"\"\n",
    "\n",
    "    # md -> html -> text since BeautifulSoup can extract text cleanly\n",
    "    html = markdown(md_string)\n",
    "\n",
    "    # remove code snippets\n",
    "    html = re.sub(r\"<pre>(.*?)</pre>\", \" \", html)\n",
    "    html = re.sub(r\"<code>(.*?)</code >\", \" \", html)\n",
    "\n",
    "    # extract text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    text = \"\".join(soup.findAll(text=True))\n",
    "\n",
    "    # make one line\n",
    "    single_line = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    # make fixes\n",
    "    single_line = re.sub(\"→\", \"->\", single_line)\n",
    "    single_line = re.sub(\"<null>\", '\"null\"', single_line)\n",
    "\n",
    "    return single_line\n",
    "\n",
    "\n",
    "def merge_columns_info(dataset, tables):\n",
    "    logging.info(\"Merging columns info from Superset and manifest.json file.\")\n",
    "\n",
    "    key = dataset[\"key\"]\n",
    "    sst_columns = dataset[\"columns\"]\n",
    "    dbt_columns = tables.get(key, {}).get(\"columns\", {})\n",
    "    columns_new = []\n",
    "    for sst_column in sst_columns:\n",
    "        # add the mandatory field\n",
    "        column_new = {\"column_name\": sst_column[\"column_name\"]}\n",
    "\n",
    "        # add optional fields only if not already None, otherwise it would error out\n",
    "        for field in [\n",
    "            \"expression\",\n",
    "            \"filterable\",\n",
    "            \"groupby\",\n",
    "            \"python_date_format\",\n",
    "            \"verbose_name\",\n",
    "            \"type\",\n",
    "            \"is_dttm\",\n",
    "            \"is_active\",\n",
    "        ]:\n",
    "            if sst_column[field] is not None:\n",
    "                column_new[field] = sst_column[field]\n",
    "\n",
    "        # add description\n",
    "        if (\n",
    "            sst_column[\"column_name\"] in dbt_columns\n",
    "            and \"description\" in dbt_columns[sst_column[\"column_name\"]]\n",
    "            and sst_column[\"expression\"] == \"\"\n",
    "        ):  # database columns\n",
    "            description = dbt_columns[sst_column[\"column_name\"]][\"description\"]\n",
    "            description = convert_markdown_to_plain_text(description)\n",
    "        else:  # if cant find in dbt\n",
    "            description = sst_column[\"description\"]\n",
    "        column_new[\"description\"] = description\n",
    "\n",
    "        columns_new.append(column_new)\n",
    "\n",
    "    dataset[\"columns_new\"] = columns_new\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def put_columns_to_superset(superset: SupersetDBTSessionConnector, dataset):\n",
    "    logging.info(\"Putting new columns info with descriptions back into Superset.\")\n",
    "    body = {\"columns\": dataset[\"columns_new\"]}\n",
    "    superset.request(\"PUT\", f\"/dataset/{dataset['id']}?override_columns=true\", json=body)\n",
    "\n",
    "\n",
    "def merge_dashboards_with_datasets(dashboards, datasets):\n",
    "    for dashboard in dashboards:\n",
    "        refs = set()\n",
    "        for dataset in dashboard[\"datasets\"]:\n",
    "            if dataset in datasets:\n",
    "                refs.update(datasets[dataset][\"dbt_refs\"])\n",
    "        refs = list(sorted(refs))\n",
    "\n",
    "        dashboard[\"refs\"] = refs\n",
    "\n",
    "    return dashboards\n",
    "\n",
    "\n",
    "def get_exposures_dict(dashboards):\n",
    "    dashboards.sort(key=lambda dashboard: dashboard[\"id\"])\n",
    "    titles = [dashboard[\"title\"] for dashboard in dashboards]\n",
    "    # fail if it breaks uniqueness constraint for exposure names\n",
    "    assert len(set(titles)) == len(titles), \"There are duplicate dashboard names!\"\n",
    "\n",
    "    exposures_dict = [\n",
    "        {\n",
    "            \"name\": f\"superset__{dashboard['title']}\",\n",
    "            \"type\": \"dashboard\",\n",
    "            \"url\": dashboard[\"url\"],\n",
    "            \"depends_on\": dashboard[\"refs\"],\n",
    "            \"owner\": {\"name\": dashboard[\"owner_name\"]},\n",
    "        }\n",
    "        for dashboard in dashboards\n",
    "    ]\n",
    "\n",
    "    return exposures_dict\n",
    "\n",
    "\n",
    "class YamlFormatted(ruamel.yaml.YAML):\n",
    "    def __init__(self):\n",
    "        super(YamlFormatted, self).__init__()\n",
    "        self.default_flow_style = False\n",
    "        self.allow_unicode = True\n",
    "        self.encoding = \"utf-8\"\n",
    "        self.block_seq_indent = 2\n",
    "        self.indent = 4\n",
    "        self.emitter.alt_null = \"''\"\n",
    "\n",
    "\n",
    "# Create Query\n",
    "\n",
    "\n",
    "def is_valid_table_name(table_name):\n",
    "    \"\"\"\n",
    "    Checks if the given string is a valid table name in PostgreSQL.\n",
    "\n",
    "    Args:\n",
    "        table_name: The string to check.\n",
    "\n",
    "    Returns:\n",
    "        True if the string is a valid table name, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # The regular expression to match a valid table name.\n",
    "    regex = re.compile(r\"^[a-zA-Z0-9_]{1,63}$\")\n",
    "\n",
    "    # Check if the string matches the regular expression.\n",
    "    if regex.match(table_name):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def is_unique_table_name(table_name, dbt_tables):\n",
    "    \"\"\"\n",
    "    Checks if the given string is a valid table name in PostgreSQL and dbt.\n",
    "\n",
    "    Args:\n",
    "        table_name: The string to check.\n",
    "        dbt_tables: Dict of get_dbt_tables\n",
    "    Returns:\n",
    "        True if the string is a valid table name, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # The regular expression to match a valid table name.\n",
    "    regex = re.compile(r\"^[a-zA-Z0-9_]{1,63}$\")\n",
    "\n",
    "    # Check if the string matches the regular expression.\n",
    "    if table_name not in dbt_tables:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_ref(original_query, dbt_tables, parsed_result, serving_tables_names):\n",
    "    \"\"\"\n",
    "    Returns content of a user-created dbt model file w/o config.\n",
    "\n",
    "    Args:\n",
    "        original_query: Query needed processing\n",
    "        dbt_tables: Dict of dicts obtained by get_tables_from_dbt.\n",
    "        schema_names: List of serving schema names.\n",
    "\n",
    "    Returns:\n",
    "        ref_tables: list of models that is referenced in the query\n",
    "    \"\"\"\n",
    "    # original_query = original_query[:-1] if original_query[-1] == \";\" else original_query # Maybe unneeded since not wrapping with\n",
    "    # Access table names\n",
    "    fixed_query = str(original_query)\n",
    "    table_names = set(get_tables_from_sql(fixed_query, dialect=\"postgres\", sql_parsed=parsed_result))\n",
    "    fixed_query = sqlfluff.fix(fixed_query, dialect=\"postgres\")\n",
    "    dbt_set = set(serving_tables_names)\n",
    "    if not table_names.issubset(dbt_set):  # serving_tables_names include schema\n",
    "        return None, \"Tables referenced out of serving schemas\"\n",
    "    # Put tables in subqueries\n",
    "    final_tables = tuple(table_names.intersection(serving_tables_names))  # Filter out\n",
    "\n",
    "    if len(final_tables) == 0:\n",
    "        return None, \"No tables referenced in dbt projects\"\n",
    "\n",
    "    return [dbt_tables[table][\"name\"] for table in final_tables], \"Success\"\n",
    "\n",
    "\n",
    "def get_records():\n",
    "    # Query records\n",
    "    connection = psycopg2.connect(\n",
    "        user=DATABASE_USERNAME,\n",
    "        password=DATABASE_PASSWORD,\n",
    "        host=DATABASE_HOST,\n",
    "        port=DATABASE_PORT,\n",
    "        database=DATABASE_NAME,\n",
    "    )\n",
    "    cursor = connection.cursor()\n",
    "    postgreSQL_select_Query = f\"\"\"select id, name, query_string, user_id, description, insert_time, checked, success \n",
    "                                from {QUERY_SCHEMA}.{QUERY_TABLE} where checked = False\"\"\"\n",
    "\n",
    "    logging.info(f\"Executing query to fetch records: {postgreSQL_select_Query}\")\n",
    "    cursor.execute(postgreSQL_select_Query)\n",
    "    query_columns = [\n",
    "        \"id\",\n",
    "        \"name\",\n",
    "        \"query_string\",\n",
    "        \"user_id\",\n",
    "        \"description\",\n",
    "        \"insert_time\",\n",
    "        \"checked\",\n",
    "        \"success\",\n",
    "    ]\n",
    "    df = pd.DataFrame(cursor.fetchall(), columns=query_columns)\n",
    "\n",
    "    postgreSQL_select_Query = f\"select name from {QUERY_SCHEMA}.{QUERY_TABLE} where success = True\"\n",
    "\n",
    "    logging.info(f\"Executing query to fetch records: {postgreSQL_select_Query}\")\n",
    "    cursor.execute(postgreSQL_select_Query)\n",
    "\n",
    "    succeeded = cursor.fetchall()\n",
    "\n",
    "    if connection:\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        logging.info(\"PostgreSQL connection is closed\")\n",
    "    return df, succeeded\n",
    "\n",
    "\n",
    "def update_records(df):\n",
    "    entries_to_update = str(tuple(zip(df.checked, df.success, df.id))).replace(\"None\", \"Null\")[1:-1]\n",
    "    if entries_to_update[-1]==\",\": entries_to_update=entries_to_update[:-1]\n",
    "    connection = psycopg2.connect(\n",
    "        user=DATABASE_USERNAME,\n",
    "        password=DATABASE_PASSWORD,\n",
    "        host=DATABASE_HOST,\n",
    "        port=DATABASE_PORT,\n",
    "        database=DATABASE_NAME,\n",
    "    )\n",
    "    cursor = connection.cursor()\n",
    "    update_sql_query = f\"\"\"UPDATE {QUERY_SCHEMA}.{QUERY_TABLE} q \n",
    "                            SET success = v.success,\n",
    "                                checked = v.checked\n",
    "\n",
    "                            FROM (VALUES {entries_to_update}) AS v (checked, success, id)\n",
    "                            WHERE q.id = v.id;\"\"\"\n",
    "    logging.info(f\"Executing query to update records: {update_sql_query}\")\n",
    "    cursor.execute(update_sql_query)\n",
    "    connection.commit()\n",
    "    # closing database connection.\n",
    "    if connection:\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        logging.info(\"PostgreSQL connection is closed\")\n",
    "\n",
    "\n",
    "def get_emails(superset, user_ids):\n",
    "    url = unquote(f\"/security/get_email/?q={list(user_ids)}\")\n",
    "    res = superset.request(\"GET\", url)\n",
    "    return res[\"emails\"]\n",
    "\n",
    "\n",
    "def get_mail_content(name, sql, status, dbt_reason=None):\n",
    "    if status == \"dbt success\":\n",
    "        message = \"\"\"\\\n",
    "Subject: Superset Model Creation\n",
    "\n",
    "Your Model {name} was successfully created. \n",
    "\n",
    "SQL:{sql}\n",
    "        \"\"\".format(\n",
    "            sql=sql, name=name\n",
    "        )\n",
    "\n",
    "    elif status == \"dbt fail\":\n",
    "        message = \"\"\"\\\n",
    "Subject: Superset Model Creation\n",
    "\n",
    "Your Model {name} was unsuccessfully created during dbt's run, please contact the administrator.\n",
    "\n",
    "Reason:\n",
    "{reason}\n",
    "\n",
    "SQL:\n",
    "{sql}\n",
    "        \"\"\".format(\n",
    "            reason=dbt_reason, sql=sql, name=name\n",
    "        )\n",
    "    else:\n",
    "        message = \"\"\"\\\n",
    "Subject: Superset Model Creation\n",
    "\n",
    "Your Model {name} was unsuccessfully created.\n",
    "\n",
    "Reason:\n",
    "{reason}\n",
    "\n",
    "SQL:\n",
    "{sql}\n",
    "        \"\"\".format(\n",
    "            reason=status, sql=sql, name=name\n",
    "        )\n",
    "    return message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReloadRepositoryLocationInfo(status=<ReloadRepositoryLocationStatus.SUCCESS: 'SUCCESS'>, failure_type=None, message=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull dashboard\n",
    "    superset = SupersetDBTSessionConnector()\n",
    "\n",
    "    logging.info(\"Starting the script!\")\n",
    "\n",
    "    dbt = dbtRunner()\n",
    "    cli_args = [\n",
    "        \"parse\",\n",
    "        \"--project-dir\",\n",
    "        DBT_PROJECT_DIR,\n",
    "    ]\n",
    "    res = dbt.invoke(cli_args)\n",
    "    \n",
    "    \n",
    "    with open('target/manifest.json') as f:\n",
    "        dbt_manifest = json.load(f)\n",
    "\n",
    "    try:\n",
    "        with open(EXPOSURES_PATH) as f:\n",
    "            yaml = ruamel.yaml.YAML(typ=\"safe\")\n",
    "            exposures = yaml.load(f)[\"exposures\"]\n",
    "\n",
    "    except (FileNotFoundError, TypeError):\n",
    "        Path(EXPOSURES_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "        Path(EXPOSURES_PATH).touch(exist_ok=True)\n",
    "        exposures = {}\n",
    "\n",
    "    dbt_tables = get_tables_from_dbt(dbt_manifest, None)\n",
    "\n",
    "    dashboards, dashboards_datasets = get_dashboards_from_superset(superset, DATABASE_ID, SUPERSET_ADMIN_ID)\n",
    "\n",
    "    datasets = get_datasets_from_superset_dbt_refs(superset, dashboards_datasets, dbt_tables, SQL_DIALECT, DATABASE_ID)\n",
    "\n",
    "    dashboards = merge_dashboards_with_datasets(dashboards, datasets)\n",
    "\n",
    "    exposures_dict = get_exposures_dict(dashboards)\n",
    "\n",
    "    # insert empty line before each exposure, except the first\n",
    "    exposures_yaml = ruamel.yaml.comments.CommentedSeq(exposures_dict)  # type: ignore\n",
    "    for e in range(len(exposures_yaml)):\n",
    "        if e != 0:\n",
    "            exposures_yaml.yaml_set_comment_before_after_key(e, before=\"\\n\")\n",
    "\n",
    "    exposures_yaml_schema = {\"version\": 2, \"exposures\": exposures_yaml}\n",
    "\n",
    "    exposures_yaml_file = YamlFormatted()\n",
    "\n",
    "    with open(EXPOSURES_PATH, \"w+\", encoding=\"utf-8\") as f:\n",
    "        exposures_yaml_file.dump(exposures_yaml_schema, f)\n",
    "\n",
    "    print(\"Transferred into a YAML file at \", EXPOSURES_PATH)\n",
    "    logging.info(\"All done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#push description\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "superset = SupersetDBTSessionConnector()\n",
    "\n",
    "logging.info(\"Starting the script!\")\n",
    "\n",
    "sst_datasets = get_physical_datasets_from_superset(superset, DATABASE_ID)\n",
    "logging.info(\"There are %d physical datasets in Superset.\", len(sst_datasets))\n",
    "\n",
    "if Path(MANIFEST_PATH).is_file():\n",
    "    with open(MANIFEST_PATH) as f:\n",
    "        dbt_manifest = json.load(f)\n",
    "else:\n",
    "    raise Exception(\"No manifest found at path\")\n",
    "\n",
    "dbt_tables = get_tables_from_dbt(dbt_manifest, None)\n",
    "\n",
    "for i, sst_dataset in enumerate(sst_datasets):\n",
    "    columns_refreshed = 0\n",
    "    logging.info(\"Processing dataset %d/%d.\", i + 1, len(sst_datasets))\n",
    "    sst_dataset_id = sst_dataset[\"id\"]\n",
    "    sst_dataset_key = sst_dataset[\"key\"]\n",
    "    if sst_dataset[\"schema\"] == USER_SCHEMA:\n",
    "        continue  # Don't push user description\n",
    "\n",
    "    refresh_columns_in_superset(superset, sst_dataset_id)\n",
    "    columns_refreshed = 1\n",
    "\n",
    "    if columns_refreshed == 1:\n",
    "        columns_refreshed = 1\n",
    "    else:\n",
    "        refresh_columns_in_superset(superset, sst_dataset_id)\n",
    "    # Otherwise, just adding the normal analytics certification\n",
    "    sst_dataset_w_cols = add_superset_columns(superset, sst_dataset)\n",
    "    sst_dataset_w_cols_new = merge_columns_info(sst_dataset_w_cols, dbt_tables)\n",
    "    put_columns_to_superset(superset, sst_dataset_w_cols_new)\n",
    "    add_sst_dataset_metadata(superset, sst_dataset_id, sst_dataset_key, dbt_tables)\n",
    "\n",
    "logging.info(\"All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull description\n",
    "\n",
    "res = superset.request(\"GET\", f\"/database/get_tables_descriptions/?db_id={DATABASE_ID}\")\n",
    "\n",
    "result = res[\"result\"]\n",
    "\n",
    "result_list = [\n",
    "    {\"name\": table_dict[\"table_name\"], \"description\": table_dict[\"table_desc\"], \"columns\": table_dict[\"columns\"]}\n",
    "    for table_dict in result\n",
    "    if table_dict[\"table_schema\"] == USER_SCHEMA\n",
    "]\n",
    "\n",
    "for table in result_list:\n",
    "    table[\"columns\"] = [column for column in table[\"columns\"] if column[\"description\"]]\n",
    "    if not table[\"columns\"]:\n",
    "        table.pop(\"columns\")\n",
    "\n",
    "desc_yaml_file = YamlFormatted()\n",
    "\n",
    "col_desc_yaml_schema = {\"version\": 2, \"models\": result_list}\n",
    "with open(DESC_YAML_PATH, \"w+\", encoding=\"utf-8\") as f:\n",
    "    desc_yaml_file.dump(col_desc_yaml_schema, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:file_log:\u001b[0m12:58:17.445296 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8fc529e680>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8fc529ea40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8fc529e770>]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m05:58:17  Running with dbt=1.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stdout_log:\u001b[0m05:58:17  Running with dbt=1.5.1\n",
      "INFO:file_log:\n",
      "\n",
      "============================== 12:58:17.447549 | cde03b94-b823-4cdd-a0f9-3e9e8255a356 ==============================\n",
      "\u001b[0m12:58:17.447549 [info ] [MainThread]: Running with dbt=1.5.1\n",
      "DEBUG:file_log:\u001b[0m12:58:17.452186 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/home/vu/.dbt', 'log_path': '/home/vu/Desktop/Projects/Thesis/financial/dbt/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}\n",
      "DEBUG:file_log:\u001b[0m12:58:17.490912 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'cde03b94-b823-4cdd-a0f9-3e9e8255a356', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8fc529e440>]}\n",
      "DEBUG:file_log:\u001b[0m12:58:17.498164 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'cde03b94-b823-4cdd-a0f9-3e9e8255a356', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8fc529e860>]}\n",
      "DEBUG:file_log:\u001b[0m12:58:17.607395 [debug] [MainThread]: checksum: 51f6b581eba8f8101bc020bf9faf8f96af641e2da86d581f66e2bdf0ff384b1c, vars: {}, profile: , target: , version: 1.5.1\n",
      "DEBUG:file_log:\u001b[0m12:58:17.787503 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.\n",
      "DEBUG:file_log:\u001b[0m12:58:17.789330 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing\n",
      "DEBUG:file_log:\u001b[0m12:58:17.816826 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'cde03b94-b823-4cdd-a0f9-3e9e8255a356', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8fbc94d3c0>]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m05:58:17  Performance info: target/perf_info.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stdout_log:\u001b[0m05:58:17  Performance info: target/perf_info.json\n",
      "INFO:file_log:\u001b[0m12:58:17.819912 [info ] [MainThread]: Performance info: target/perf_info.json\n",
      "DEBUG:file_log:\u001b[0m12:58:17.874199 [debug] [MainThread]: Command `cli parse` succeeded at 12:58:17.873816 after 0.43 seconds\n",
      "DEBUG:file_log:\u001b[0m12:58:17.876794 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8fbc8ebb80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8fc529e860>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8fbc70be80>]}\n",
      "DEBUG:file_log:\u001b[0m12:58:17.879711 [debug] [MainThread]: Flushing usage events\n"
     ]
    }
   ],
   "source": [
    "superset = SupersetDBTSessionConnector()\n",
    "superset_tables_dict_list = get_physical_datasets_from_superset(superset, DATABASE_ID)\n",
    "superset_tables_id_dict = dict([(table[\"key\"], table[\"id\"]) for table in superset_tables_dict_list])\n",
    "logging.info(\"Starting!\")\n",
    "\n",
    "dbt = dbtRunner()\n",
    "cli_args = [\n",
    "    \"parse\",\n",
    "    \"--project-dir\",\n",
    "    DBT_PROJECT_DIR,\n",
    "]\n",
    "res = dbt.invoke(cli_args)\n",
    "\n",
    "dbt_tables = {}\n",
    "with open('target/manifest.json') as f:\n",
    "    dbt_manifest = json.load(f)\n",
    "dbt_tables_temp = get_tables_from_dbt(dbt_manifest, None)\n",
    "dbt_tables = {**dbt_tables, **dbt_tables_temp}\n",
    "\n",
    "    \n",
    "\n",
    "# Getting the dbt tables keys\n",
    "dbt_tables_names = list(dbt_tables.keys())\n",
    "logging.info(\"Tables in manifest: {num}\".format(num=len(dbt_tables_names)))\n",
    "superset_dict_keys = [i[\"key\"] for i in superset_tables_dict_list]\n",
    "# Only tables that start with a given schema prefix name\n",
    "mapped = map(lambda x: x.startswith((SERVING_SCHEMA, USER_SCHEMA)), dbt_tables_names)\n",
    "mask = list(mapped)\n",
    "\n",
    "dbt_tables_reporting = list(compress(dbt_tables_names, mask))\n",
    "superset_tables = superset_dict_keys\n",
    "\n",
    "# Parsing as sets\n",
    "dbt_tables_reporting = set(dbt_tables_reporting)\n",
    "superset_tables = set(superset_tables)\n",
    "\n",
    "# To add to superset\n",
    "add_to_superset = list(dbt_tables_reporting.difference(superset_tables))\n",
    "\n",
    "# To remove from superset\n",
    "\n",
    "remove_from_superset = list(superset_tables.difference(dbt_tables_reporting))\n",
    "\n",
    "for i in add_to_superset:\n",
    "    logging.info(\"Starting datasets addition\")\n",
    "    rison_request = \"dataset/\"\n",
    "    array = i.split(\".\")\n",
    "    schema = array[0]\n",
    "    table_name = array[1]\n",
    "    # Data to be written\n",
    "    dictionary = {\n",
    "        # Parameter database\n",
    "        \"database\": DATABASE_ID,\n",
    "        \"schema\": schema,\n",
    "        \"table_name\": array[1],\n",
    "        \"owners\": [SUPERSET_ID],\n",
    "    }\n",
    "    # Add potential user\n",
    "    if dbt_tables[i][\"user\"]:\n",
    "        dictionary[\"owners\"].append(int(dbt_tables[i][\"user\"]))\n",
    "    # Serializing json\n",
    "    json_object = json.dumps(dictionary)\n",
    "    try:\n",
    "        response = superset.request(\"POST\", rison_request, json=dictionary)\n",
    "    except:\n",
    "        raise Exception(dictionary)\n",
    "logging.info(\"Done!\")\n",
    "logging.info(\"Starting superset datasets removal\")\n",
    "for i in remove_from_superset:\n",
    "    # Dataset id to be deleted\n",
    "    dataset_id = superset_tables_id_dict[i]\n",
    "\n",
    "    rison_request = \"/dataset/\" + str(dataset_id)\n",
    "    response = superset.request(\"DELETE\", rison_request)\n",
    "logging.info(\"Done with removing tables!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['marts.fact_business_model_rating',\n",
       " 'marts.fact_valuation_rating',\n",
       " 'marts.fact_bollinger',\n",
       " 'marts.fact_general_rating',\n",
       " 'marts.fact_income_statement',\n",
       " 'marts.fact_cash_flow',\n",
       " 'marts.fact_balance_sheet',\n",
       " 'user.user_created_example',\n",
       " 'marts.fact_bov',\n",
       " 'marts.fact_mfi',\n",
       " 'marts.fact_financial_health_rating',\n",
       " 'marts.fact_business_operation_rating',\n",
       " 'marts.fact_industry_health_rating']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_to_superset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[132], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dbt_tables[add_to_superset]\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "dbt_tables[add_to_superset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbt_tables[i][\"user\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'database': 1,\n",
       " 'schema': 'user',\n",
       " 'table_name': 'user_created_example',\n",
       " 'owners': [36, '1']}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary[\"owners\"].append(dbt_tables[i][\"user\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "422 Client Error: UNPROCESSABLE ENTITY for url: http://34.82.185.252:30007/api/v1/dataset/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[39m=\u001b[39m superset\u001b[39m.\u001b[39;49mrequest(\u001b[39m\"\u001b[39;49m\u001b[39mPOST\u001b[39;49m\u001b[39m\"\u001b[39;49m, rison_request, json\u001b[39m=\u001b[39;49mdictionary)\n",
      "Cell \u001b[0;32mIn[56], line 87\u001b[0m, in \u001b[0;36mSupersetDBTSessionConnector.request\u001b[0;34m(self, method, endpoint, **request_kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession\u001b[39m.\u001b[39mrequest(method, url, headers\u001b[39m=\u001b[39mcsrf_headers, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mrequest_kwargs)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     logging\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRequest finished with status: \u001b[39m\u001b[39m{\u001b[39;00mres\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 87\u001b[0m res\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m     88\u001b[0m \u001b[39mreturn\u001b[39;00m res\u001b[39m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/Desktop/Projects/Thesis/venv/lib/python3.10/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     http_error_msg \u001b[39m=\u001b[39m (\n\u001b[1;32m   1017\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m Server Error: \u001b[39m\u001b[39m{\u001b[39;00mreason\u001b[39m}\u001b[39;00m\u001b[39m for url: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1018\u001b[0m     )\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 422 Client Error: UNPROCESSABLE ENTITY for url: http://34.82.185.252:30007/api/v1/dataset/"
     ]
    }
   ],
   "source": [
    "response = superset.request(\"POST\", rison_request, json=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result': 'Ijc0YzhmMGQzNDg0MjRjNDhlY2ZmN2NiMTg0NjgxOWI2M2IzOTdjNzQi.ZNcW_g.jIT6WcA2XrpbD_VyUGrnl9YX0-Y'}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "superset = SupersetDBTSessionConnector()\n",
    "superset.request(\"GET\", \"security/csrf_token/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'superset'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUPERSET_USERNAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "session = requests.session()\n",
    "soup = BeautifulSoup(session.post(SUPERSET_HOST + \"login\").text, \"html.parser\")\n",
    "csrf_token = soup.find(\"input\", {\"id\": \"csrf_token\"})[\"value\"]  # type: ignore\n",
    "\n",
    "data = {\n",
    "    \"username\": \"fdp\",\n",
    "    \"password\": \"fdp\",\n",
    "    \"provider\": \"db\",\n",
    "    \"refresh\": True,\n",
    "}\n",
    "headers = {\n",
    "    # 'Authorization': 'Bearer {}'.format(self.____access_token),\n",
    "    \"x-csrftoken\": csrf_token,\n",
    "}\n",
    "response = session.post(SUPERSET_HOST + \"login\", json=data, headers=headers)  # type: ignore\n",
    "\n",
    "endpoint =\"security/csrf_token/\"\n",
    "method = \"GET\"\n",
    "api_url = SUPERSET_HOST +\"api/v1/\"\n",
    "\n",
    "logging.info(\"About to %s execute request for endpoint %s\", method, endpoint)\n",
    "\n",
    "url = api_url + endpoint\n",
    "csrf_headers = {\n",
    "    # 'Authorization': 'Bearer {}'.format(self.__access_token),\n",
    "    \"x-csrftoken\": csrf_token,\n",
    "}\n",
    "\n",
    "res = session.request(method, url, headers=csrf_headers)  # type: ignore\n",
    "\n",
    "logging.info(\"Request finished with status: %d\", res.status_code)\n",
    "\n",
    "# if res.status_code == 401 and res.json().get(\"msg\") == \"Token has expired\" and self.__refresh_session():\n",
    "#     logging.info(f\"Retrying {method} request for {url} %s with refreshed session\")\n",
    "#     res = self.__session.request(method, url, headers=csrf_headers, **request_kwargs)  # type: ignore\n",
    "\n",
    "#     logging.info(\"Request finished with status: %d\", res.status_code)\n",
    "\n",
    "# if (\n",
    "#     res.status_code == 400\n",
    "#     and res.json()[\"message\"] == \"400 Bad Request: The CSRF session token is missing.\"\n",
    "#     and self.__refresh_session()\n",
    "# ):\n",
    "#     logging.info(f\"Retrying {method} request for {url} %s with refreshed session\")\n",
    "#     res = self.__session.request(method, url, headers=csrf_headers, **request_kwargs)  # type: ignore\n",
    "#     logging.info(f\"Request finished with status: {res.status_code}\")\n",
    "res.raise_for_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IjM2ODljZDkwYWJkMjk5ODk3YzQ1NTY3YjEwZmQ3NGNkYmIzODhlM2Ei.ZNcWlA.wpDLbRG9SDj5cIhdpO739ooCbhM'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csrf_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"result\":\"IjM2ODljZDkwYWJkMjk5ODk3YzQ1NTY3YjEwZmQ3NGNkYmIzODhlM2Ei.ZNcWlg.Jj2oIbjzHgq1I5kf1LdprrFL7ow\"}\\n'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<requests.sessions.Session at 0x7f8fc54d0580>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fdp'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUPERSET_USERNAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "username = \"admin\"\n",
    "password = \"admin\"\n",
    "session = requests.session()\n",
    "\n",
    "login_form = session.post(SUPERSET_HOST+'/login')\n",
    "soup = BeautifulSoup(login_form.text, 'html.parser')\n",
    "csrf_token = soup.find('input',{'id':'csrf_token'})['value']\n",
    "data = {\n",
    "  'username': username,\n",
    "  'password': password,\n",
    "  # 'csrf_token': csrf_token\n",
    "}\n",
    "response = session.post(SUPERSET_HOST+'/login', data=data)\n",
    "# response = session.get('http://localhost:8088/api/v1/me')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ImI2MjU0MDAxOTczNDYxZmQ4ZTUyMmNjMjMwODFiNTkzZWJiMTk0Yjki.ZNcWKA.pn4FcmWsPWXGmSb93zjQJAQ_TSc'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csrf_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "superset = SupersetDBTSessionConnector()\n",
    "logging.info(\"Getting physical datasets from Superset.\")\n",
    "page_number = 0\n",
    "datasets = []\n",
    "datasets_keys = set()\n",
    "while True:\n",
    "    logging.info(\"Getting page %d.\", page_number + 1)\n",
    "    rison_request = f\"dataset/?q=(page_size:100,page:{page_number},order_column:changed_on_delta_humanized,order_direction:asc,filters:!((col:table_name,opr:nct,value:archived),(col:sql,opr:dataset_is_null_or_empty,value:true)))\"\n",
    "    res = superset.request(\"GET\", rison_request)\n",
    "    result = res[\"result\"]\n",
    "    if result:\n",
    "        for r in result:\n",
    "            name = r[\"table_name\"]\n",
    "            schema = r[\"schema\"]\n",
    "            database_name = r[\"database\"][\"database_name\"]\n",
    "            dataset_id = r[\"id\"]\n",
    "            database_id = r[\"database\"][\"id\"]\n",
    "            dataset_key = f\"{schema}.{name}\"  # same format as in dashboards\n",
    "\n",
    "            kind = r[\"kind\"]\n",
    "            if kind == \"physical\" and (1 is None or database_id == 1):\n",
    "                dataset_id = r[\"id\"]\n",
    "\n",
    "                name = r[\"table_name\"]\n",
    "                schema = r[\"schema\"]\n",
    "                dataset_key = f\"{schema}.{name}\"  # used as unique identifier\n",
    "\n",
    "                dataset_dict = {\n",
    "                    \"id\": dataset_id,\n",
    "                    \"name\": name,\n",
    "                    \"schema\": schema,\n",
    "                    \"database\": database_name,\n",
    "                    \"dataset_id\": dataset_id,\n",
    "                    \"key\": dataset_key,\n",
    "                    \"table\": [dataset_key],\n",
    "                }\n",
    "\n",
    "                # fail if it breaks uniqueness constraint\n",
    "                assert dataset_key not in datasets_keys, (\n",
    "                    f\"Dataset {dataset_key} is a duplicate name (schema + table) \"\n",
    "                    \"across databases. \"\n",
    "                    \"This would result in incorrect matching between Superset and dbt. \"\n",
    "                    \"To fix this, remove duplicates or add the ``superset_db_id`` argument.\"\n",
    "                )\n",
    "\n",
    "                datasets_keys.add(dataset_key)\n",
    "                datasets.append(dataset_dict)\n",
    "\n",
    "        page_number += 1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupersetDBTSessionConnector:\n",
    "    \"\"\"A class for accessing the Superset API in an easy way.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Instantiates the class.\n",
    "\n",
    "        ''access_token'' will be instantiated via enviromental variable\n",
    "        If ``access_token`` is None, attempts to obtain it using ``refresh_token``.\n",
    "\n",
    "        Args:\n",
    "            api_url: Base API URL of a Superset instance, e.g. https://my-superset/api/v1.\n",
    "            access_token: Access token to use for accessing protected endpoints of the Superset\n",
    "                API. Can be automatically obtained if ``refresh_token`` is not None.\n",
    "            refresh_token: Refresh token to use for obtaining or refreshing the ``access_token``.\n",
    "                If None, no refresh will be done.\n",
    "        \"\"\"\n",
    "        self.url = SUPERSET_HOST\n",
    "        self.api_url = self.url + \"api/v1/\"\n",
    "\n",
    "        self.session = requests.session()\n",
    "\n",
    "        self.username = SUPERSET_USERNAME\n",
    "        self.password = SUPERSET_PASSWORD\n",
    "\n",
    "        self.refresh_session()\n",
    "\n",
    "    def refresh_session(self):\n",
    "        logging.info(\"Refreshing session\")\n",
    "\n",
    "        soup = BeautifulSoup(self.session.post(self.url + \"login\").text, \"html.parser\")\n",
    "        self.csrf_token = soup.find(\"input\", {\"id\": \"csrf_token\"})[\"value\"]  # type: ignore\n",
    "\n",
    "        data = {\n",
    "            \"username\": self.username,\n",
    "            \"password\": self.password,\n",
    "            \"provider\": \"db\",\n",
    "            \"refresh\": True,\n",
    "        }\n",
    "        headers = {\n",
    "            # 'Authorization': 'Bearer {}'.format(self.____access_token),\n",
    "            \"x-csrftoken\": self.csrf_token,\n",
    "        }\n",
    "        response = self.session.post(self.url + \"login\", json=data, headers=headers)  # type: ignore\n",
    "        return True\n",
    "\n",
    "    def request(self, method, endpoint, **request_kwargs):\n",
    "        \"\"\"Executes a request against the Superset API.\n",
    "\n",
    "        Args:\n",
    "            method: HTTP method to use.\n",
    "            endpoint: Endpoint to use.\n",
    "            **request_kwargs: Any ``requests.request`` arguments to use.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing response body parsed from JSON.\n",
    "\n",
    "        Raises:\n",
    "            HTTPError: There is an HTTP error (detected by ``requests.Response.raise_for_status``)\n",
    "                even after retrying with a fresh session.\n",
    "        \"\"\"\n",
    "\n",
    "        logging.info(\"About to %s execute request for endpoint %s\", method, endpoint)\n",
    "\n",
    "        url = self.api_url + endpoint\n",
    "        csrf_headers = {\n",
    "            # 'Authorization': 'Bearer {}'.format(self.__access_token),\n",
    "            \"x-csrftoken\": self.csrf_token,\n",
    "        }\n",
    "\n",
    "        res = self.session.request(method, url, headers=csrf_headers, **request_kwargs)  # type: ignore\n",
    "\n",
    "        logging.info(\"Request finished with status: %d\", res.status_code)\n",
    "\n",
    "        # if res.status_code and self.refresh_session():\n",
    "        #     logging.info(f\"Retrying {method} request for {url} %s with refreshed session\")\n",
    "        #     res = self.session.request(method, url, headers=csrf_headers, **request_kwargs)  # type: ignore\n",
    "\n",
    "        #     logging.info(\"Request finished with status: %d\", res.status_code)\n",
    "\n",
    "        # if (\n",
    "        #     res.status_code == 400\n",
    "        #     and self.refresh_session()\n",
    "        # ):\n",
    "        #     logging.info(f\"Retrying {method} request for {url} %s with refreshed session\")\n",
    "        #     res = self.session.request(method, url, headers=csrf_headers, **request_kwargs)  # type: ignore\n",
    "        #     logging.info(f\"Request finished with status: {res.status_code}\")\n",
    "        res.raise_for_status()\n",
    "        return res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "superset=SupersetDBTSessionConnector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ImUwYjk5NWE1YmMxYjFjMGNlNzg5YWYyZGFmNWYwOGQ0ZjJkMmY3ZmUi.ZNcbCw.bHkXfs6xKdFJnqpySvRWzRqszNs'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "superset.csrf_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result': [{'columns': [{'description': None, 'name': 'ticker'},\n",
       "    {'description': None, 'name': 'website'},\n",
       "    {'description': None, 'name': 'outstanding_share'},\n",
       "    {'description': None, 'name': 'company_type'},\n",
       "    {'description': None, 'name': 'delta_in_year'},\n",
       "    {'description': None, 'name': 'industry'},\n",
       "    {'description': None, 'name': 'stock_rating'},\n",
       "    {'description': None, 'name': 'industry_en'},\n",
       "    {'description': None, 'name': 'no_employees'},\n",
       "    {'description': None, 'name': 'delta_in_week'},\n",
       "    {'description': None, 'name': 'industry_id'},\n",
       "    {'description': None, 'name': 'industry_id_v2'},\n",
       "    {'description': None, 'name': 'exchange'},\n",
       "    {'description': None, 'name': 'established_year'},\n",
       "    {'description': None, 'name': 'short_name_en'},\n",
       "    {'description': None, 'name': 'no_share_holders'},\n",
       "    {'description': None, 'name': 'issue_share'},\n",
       "    {'description': None, 'name': 'delta_in_month'},\n",
       "    {'description': None, 'name': 'foreign_percent'}],\n",
       "   'table_desc': None,\n",
       "   'table_name': 'dim_organization',\n",
       "   'table_schema': 'marts'},\n",
       "  {'columns': [{'description': None, 'name': 'ticker'},\n",
       "    {'description': None, 'name': 'open'},\n",
       "    {'description': None, 'name': 'high'},\n",
       "    {'description': None, 'name': 'low'},\n",
       "    {'description': None, 'name': 'close'},\n",
       "    {'description': None, 'name': 'volume'},\n",
       "    {'description': None, 'name': 'trading_date'}],\n",
       "   'table_desc': None,\n",
       "   'table_name': 'fact_price_history',\n",
       "   'table_schema': 'marts'},\n",
       "  {'columns': [{'description': None, 'name': 'price'},\n",
       "    {'description': None, 'name': 'id'},\n",
       "    {'description': None, 'name': 'volume'},\n",
       "    {'description': None, 'name': 'cp'},\n",
       "    {'description': None, 'name': 'rcp'},\n",
       "    {'description': None, 'name': 'a'},\n",
       "    {'description': None, 'name': 'ba'},\n",
       "    {'description': None, 'name': 'sa'},\n",
       "    {'description': None, 'name': 'hl'},\n",
       "    {'description': None, 'name': 'pcp'},\n",
       "    {'description': None, 'name': 'transaction_time'}],\n",
       "   'table_desc': None,\n",
       "   'table_name': 'fact_stock_intraday',\n",
       "   'table_schema': 'marts'}]}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "superset = SupersetDBTSessionConnector()\n",
    "superset.request(\"GET\", \"database/get_tables_descriptions/?db_id=1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result': [{'columns': [{'description': None, 'name': 'ticker'},\n",
       "    {'description': None, 'name': 'website'},\n",
       "    {'description': None, 'name': 'outstanding_share'},\n",
       "    {'description': None, 'name': 'company_type'},\n",
       "    {'description': None, 'name': 'delta_in_year'},\n",
       "    {'description': None, 'name': 'industry'},\n",
       "    {'description': None, 'name': 'stock_rating'},\n",
       "    {'description': None, 'name': 'industry_en'},\n",
       "    {'description': None, 'name': 'no_employees'},\n",
       "    {'description': None, 'name': 'delta_in_week'},\n",
       "    {'description': None, 'name': 'industry_id'},\n",
       "    {'description': None, 'name': 'industry_id_v2'},\n",
       "    {'description': None, 'name': 'exchange'},\n",
       "    {'description': None, 'name': 'established_year'},\n",
       "    {'description': None, 'name': 'short_name_en'},\n",
       "    {'description': None, 'name': 'no_share_holders'},\n",
       "    {'description': None, 'name': 'issue_share'},\n",
       "    {'description': None, 'name': 'delta_in_month'},\n",
       "    {'description': None, 'name': 'foreign_percent'}],\n",
       "   'table_desc': None,\n",
       "   'table_name': 'dim_organization',\n",
       "   'table_schema': 'marts'},\n",
       "  {'columns': [{'description': None, 'name': 'ticker'},\n",
       "    {'description': None, 'name': 'open'},\n",
       "    {'description': None, 'name': 'high'},\n",
       "    {'description': None, 'name': 'low'},\n",
       "    {'description': None, 'name': 'close'},\n",
       "    {'description': None, 'name': 'volume'},\n",
       "    {'description': None, 'name': 'trading_date'}],\n",
       "   'table_desc': None,\n",
       "   'table_name': 'fact_price_history',\n",
       "   'table_schema': 'marts'},\n",
       "  {'columns': [{'description': None, 'name': 'price'},\n",
       "    {'description': None, 'name': 'id'},\n",
       "    {'description': None, 'name': 'volume'},\n",
       "    {'description': None, 'name': 'cp'},\n",
       "    {'description': None, 'name': 'rcp'},\n",
       "    {'description': None, 'name': 'a'},\n",
       "    {'description': None, 'name': 'ba'},\n",
       "    {'description': None, 'name': 'sa'},\n",
       "    {'description': None, 'name': 'hl'},\n",
       "    {'description': None, 'name': 'pcp'},\n",
       "    {'description': None, 'name': 'transaction_time'}],\n",
       "   'table_desc': None,\n",
       "   'table_name': 'fact_stock_intraday',\n",
       "   'table_schema': 'marts'}]}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "superset.request(\"GET\", \"database/get_tables_descriptions/?db_id=1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
