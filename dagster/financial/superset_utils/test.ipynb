{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Union\n",
    "import pandas as pd\n",
    "import logging\n",
    "import re\n",
    "import sqlfluff\n",
    "import requests\n",
    "import ruamel.yaml\n",
    "import psycopg2\n",
    "from bs4 import BeautifulSoup\n",
    "from markdown import markdown\n",
    "import datetime\n",
    "from urllib.parse import unquote\n",
    "from typing import Any, Dict, Iterator, List, Union\n",
    "import json\n",
    "from pgsanity.pgsanity import check_string\n",
    "import os\n",
    "from dbt.cli.main import dbtRunner, dbtRunnerResult\n",
    "import smtplib\n",
    "import ssl\n",
    "from itertools import compress\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "MATERIALIZATION_MAPPING = {1: \"table\", 2: \"view\", 3: \"incremental\", 4: \"ephemereal\"}\n",
    "SUPERSET_USERNAME = \"superset\"\n",
    "SUPERSET_PASSWORD = \"superset\"\n",
    "SUPERSET_HOST = \"http://34.82.185.252:30007/\"\n",
    "DATABASE_USERNAME = \"fdp\"\n",
    "DATABASE_PASSWORD = \"fdp\"\n",
    "DATABASE_HOST = \"34.82.185.252\"\n",
    "DATABASE_PORT = 30005\n",
    "DATABASE_NAME = \"financial_data\"\n",
    "QUERY_SCHEMA=\"financial_query\"\n",
    "QUERY_TABLE=\"query\"\n",
    "MANIFEST_PATH=\"/home/vu/Desktop/Projects/Thesis/financial/dbt/target/manifest.json\"\n",
    "EMAIL_PORT = 465\n",
    "SMTP = \"smtp.gmail.com\"\n",
    "EMAIL_SENDER = \"catvu113@gmail.com\"\n",
    "EMAIL_PASSWORD = \"xhtzakhmnsbufufy\"\n",
    "USER_MODEL_PATH = \"/home/vu/Desktop/Projects/Thesis/financial/dbt/models/user\"\n",
    "DBT_PROJECT_DIR = \"/home/vu/Desktop/Projects/Thesis/financial/dbt/\"\n",
    "DATABASE_ID = 1\n",
    "SUPERSET_ID = 34\n",
    "USER_SCHEMA = \"user\"\n",
    "SERVING_SCHEMA=\"marts\"\n",
    "context = ssl.create_default_context()\n",
    "SMTP = smtplib.SMTP_SSL(\"smtp.gmail.com\", 465, context=context)\n",
    "SST_DATABASE_NAME = \"FDP Reader\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupersetDBTSessionConnector:\n",
    "    \"\"\"A class for accessing the Superset API in an easy way.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Instantiates the class.\n",
    "\n",
    "        ''access_token'' will be instantiated via enviromental variable\n",
    "        If ``access_token`` is None, attempts to obtain it using ``refresh_token``.\n",
    "\n",
    "        Args:\n",
    "            api_url: Base API URL of a Superset instance, e.g. https://my-superset/api/v1.\n",
    "            access_token: Access token to use for accessing protected endpoints of the Superset\n",
    "                API. Can be automatically obtained if ``refresh_token`` is not None.\n",
    "            refresh_token: Refresh token to use for obtaining or refreshing the ``access_token``.\n",
    "                If None, no refresh will be done.\n",
    "        \"\"\"\n",
    "        self.__url = SUPERSET_HOST\n",
    "        self.__api_url = self.__url + \"api/v1/\"\n",
    "\n",
    "        self.__session = requests.session()\n",
    "\n",
    "        self.__username = SUPERSET_USERNAME\n",
    "        self.__password = SUPERSET_PASSWORD\n",
    "\n",
    "        self.__refresh_session()\n",
    "\n",
    "    def __refresh_session(self):\n",
    "        logging.info(\"Refreshing session\")\n",
    "\n",
    "        soup = BeautifulSoup(self.__session.post(self.__url + \"login\").text, \"html.parser\")\n",
    "        self.__csrf_token = soup.find(\"input\", {\"id\": \"csrf_token\"})[\"value\"]  # type: ignore\n",
    "\n",
    "        data = {\n",
    "            \"username\": self.__username,\n",
    "            \"password\": self.__password,\n",
    "            \"provider\": \"db\",\n",
    "            \"refresh\": True,\n",
    "        }\n",
    "        headers = {\n",
    "            # 'Authorization': 'Bearer {}'.format(self.____access_token),\n",
    "            \"x-csrftoken\": self.__csrf_token,\n",
    "        }\n",
    "        response = self.__session.post(self.__url + \"login\", json=data, headers=headers)  # type: ignore\n",
    "        return True\n",
    "\n",
    "    def request(self, method, endpoint, **request_kwargs):\n",
    "        \"\"\"Executes a request against the Superset API.\n",
    "\n",
    "        Args:\n",
    "            method: HTTP method to use.\n",
    "            endpoint: Endpoint to use.\n",
    "            **request_kwargs: Any ``requests.request`` arguments to use.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing response body parsed from JSON.\n",
    "\n",
    "        Raises:\n",
    "            HTTPError: There is an HTTP error (detected by ``requests.Response.raise_for_status``)\n",
    "                even after retrying with a fresh session.\n",
    "        \"\"\"\n",
    "\n",
    "        logging.info(\"About to %s execute request for endpoint %s\", method, endpoint)\n",
    "\n",
    "        url = self.__api_url + endpoint\n",
    "        csrf_headers = {\n",
    "            # 'Authorization': 'Bearer {}'.format(self.__access_token),\n",
    "            \"x-csrftoken\": self.__csrf_token,\n",
    "        }\n",
    "\n",
    "        res = self.__session.request(method, url, headers=csrf_headers, **request_kwargs)  # type: ignore\n",
    "\n",
    "        logging.info(\"Request finished with status: %d\", res.status_code)\n",
    "\n",
    "        if res.status_code == 401 and res.json().get(\"msg\") == \"Token has expired\" and self.__refresh_session():\n",
    "            logging.info(f\"Retrying {method} request for {url} %s with refreshed session\")\n",
    "            res = self.__session.request(method, url, headers=csrf_headers, **request_kwargs)  # type: ignore\n",
    "\n",
    "            logging.info(\"Request finished with status: %d\", res.status_code)\n",
    "\n",
    "        if (\n",
    "            res.status_code == 400\n",
    "            and res.json()[\"message\"] == \"400 Bad Request: The CSRF session token is missing.\"\n",
    "            and self.__refresh_session()\n",
    "        ):\n",
    "            logging.info(f\"Retrying {method} request for {url} %s with refreshed session\")\n",
    "            res = self.__session.request(method, url, headers=csrf_headers, **request_kwargs)  # type: ignore\n",
    "            logging.info(f\"Request finished with status: {res.status_code}\")\n",
    "        res.raise_for_status()\n",
    "        return res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "superset = SupersetDBTSessionConnector()\n",
    "\n",
    "if Path(MANIFEST_PATH).is_file():\n",
    "    with open(MANIFEST_PATH) as f:\n",
    "        dbt_manifest = json.load(f)\n",
    "else:\n",
    "    raise Exception(\"No manifest found at path\")\n",
    "\n",
    "dbt_tables = get_tables_from_dbt(dbt_manifest, None)\n",
    "serving_dbt_models = [\n",
    "    dbt_tables[table] for table in dbt_tables if dbt_tables[table][\"schema\"] in (SERVING_SCHEMA, USER_SCHEMA)\n",
    "]\n",
    "datasources = []\n",
    "for model in serving_dbt_models:\n",
    "    datasources.append(\n",
    "        {\n",
    "            \"database_name\": SST_DATABASE_NAME,\n",
    "            \"datasource_name\": model[\"name\"],\n",
    "            \"datasource_type\": \"table\",\n",
    "            \"schema\": model[\"schema\"],\n",
    "        }\n",
    "    )\n",
    "datasources = {\"datasources\": datasources}\n",
    "superset.request(\"POST\", \"cachekey/invalidate\", json=datasources)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "force\": true,\n",
    "  \"thumb_size\": [\n",
    "    0\n",
    "  ],\n",
    "  \"window_size\": [\n",
    "    0\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "superset.request(\"GET\",f'chart/1/cache_screenshot/?q={{\"page\":{page_number},\"page_size\":100}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "superset = SupersetDBTSessionConnector()\n",
    "datasources = {\n",
    "    \"datasources\": [\n",
    "        {\n",
    "            \"database_name\": \"FDP Reader\",\n",
    "            \"datasource_name\": \"marts.fact_price_history\",\n",
    "            \"datasource_type\": \"table\",\n",
    "            \"schema\": \"marts\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "superset.request(\"POST\", \"cachekey/invalidate\", json=datasources)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_table_name(table_name):\n",
    "    \"\"\"\n",
    "    Checks if the given string is a valid table name in PostgreSQL.\n",
    "\n",
    "    Args:\n",
    "        table_name: The string to check.\n",
    "\n",
    "    Returns:\n",
    "        True if the string is a valid table name, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # The regular expression to match a valid table name.\n",
    "    regex = re.compile(r\"^[a-zA-Z0-9_]{1,63}$\")\n",
    "\n",
    "    # Check if the string matches the regular expression.\n",
    "    if regex.match(table_name):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def is_unique_table_name(table_name, dbt_tables):\n",
    "    \"\"\"\n",
    "    Checks if the given string is a valid table name in PostgreSQL and dbt.\n",
    "\n",
    "    Args:\n",
    "        table_name: The string to check.\n",
    "        dbt_tables: Dict of get_dbt_tables\n",
    "    Returns:\n",
    "        True if the string is a valid table name, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # The regular expression to match a valid table name.\n",
    "    regex = re.compile(r\"^[a-zA-Z0-9_]{1,63}$\")\n",
    "\n",
    "    # Check if the string matches the regular expression.\n",
    "    if table_name not in dbt_tables:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "def get_ref(original_query, dbt_tables, parsed_result, dbt_tables_names):\n",
    "    \"\"\"\n",
    "    Returns content of a user-created dbt model file w/o config.\n",
    "\n",
    "    Args:\n",
    "        original_query: Query needed processing\n",
    "        dbt_tables: Dict of dicts obtained by get_tables_from_dbt.\n",
    "        schema_names: List of serving schema names.\n",
    "\n",
    "    Returns:\n",
    "        String: the content of the dbt model.\n",
    "    \"\"\"\n",
    "    # original_query = original_query[:-1] if original_query[-1] == \";\" else original_query # Maybe unneeded since not wrapping with\n",
    "    # Access table names\n",
    "    fixed_query = str(original_query)\n",
    "    table_names = set(get_tables_from_sql(fixed_query, dialect=\"postgres\", sql_parsed=parsed_result))\n",
    "    fixed_query = sqlfluff.fix(fixed_query, dialect=\"postgres\")\n",
    "    if len(table_names.difference(dbt_tables_names)) > 0:  # dbt_tables_names include schema\n",
    "        return None, \"Tables referenced out of serving schemas\"\n",
    "    # Put tables in subqueries\n",
    "    final_tables = tuple(table_names.intersection(dbt_tables_names))  # Filter out\n",
    "\n",
    "    if len(final_tables) == 0:\n",
    "        return None, \"No tables referenced in dbt projects\"\n",
    "\n",
    "    table_to_ref = {}\n",
    "\n",
    "    # Add ref for original query\n",
    "    new_query = original_query\n",
    "    for table in final_tables:\n",
    "        new_query = (\n",
    "            \"\"\"\n",
    "-- depends_on: {{{{ref(\\'{table}\\')}}}}\n",
    "    \"\"\".format(\n",
    "                table=dbt_tables[table][\"name\"]  # Ensure that there is only table names, no schema names\n",
    "            )\n",
    "            + new_query\n",
    "        )\n",
    "    return new_query, \"Success\"\n",
    "\n",
    "def add_materialization(df_row, query, exec_time):\n",
    "    \"\"\"\n",
    "    Returns content of a user-created dbt model file with config.\n",
    "\n",
    "    Args:\n",
    "        df_row: Row of DataFrame taken from \"query\" table.\n",
    "        dbt_tables: List of tables name.\n",
    "        schema_names: List of serving schema names.\n",
    "\n",
    "    Returns:\n",
    "        String: the content of the dbt model.\n",
    "    \"\"\"\n",
    "    query = (\n",
    "        \"\"\"\n",
    "{{{{ config(\n",
    "    materialized=\\'{materialization}\\',\n",
    "    name='{name}',\n",
    "    description='{desc}',\n",
    "    tags = ['{user_id}','user_created','{created_time}'],\n",
    "    schema = 'financial_user'\n",
    ") }}}}\"\"\".format(\n",
    "            materialization=MATERIALIZATION_MAPPING[df_row[\"materialization\"]],\n",
    "            user_id=df_row[\"user_id\"],\n",
    "            name=df_row[\"name\"],\n",
    "            desc=df_row[\"description\"],\n",
    "            created_time=exec_time,\n",
    "        )\n",
    "        + query\n",
    "    )\n",
    "    return query\n",
    "\n",
    "\n",
    "def get_records():\n",
    "    # Query records\n",
    "    try:\n",
    "        connection = psycopg2.connect(\n",
    "            user=DATABASE_USERNAME,\n",
    "            password=DATABASE_PASSWORD,\n",
    "            host=DATABASE_HOST,\n",
    "            port=DATABASE_PORT,\n",
    "            database=DATABASE_NAME,\n",
    "        )\n",
    "        cursor = connection.cursor()\n",
    "        postgreSQL_select_Query = f\"select * from {QUERY_SCHEMA}.{QUERY_TABLE} where checked = False\"\n",
    "        # postgreSQL_select_Query = \"\"\"\n",
    "        # SELECT *\n",
    "        # FROM query\n",
    "        # WHERE insert_time  > now() - interval '30 second';\n",
    "        # \"\"\"\n",
    "        print(postgreSQL_select_Query)\n",
    "        cursor.execute(postgreSQL_select_Query)\n",
    "        query_columns = [\n",
    "            \"query_string\",\n",
    "            \"materialization\",\n",
    "            \"user_id\",\n",
    "            \"description\",\n",
    "            \"insert_time\",\n",
    "            \"name\",\n",
    "            \"checked\",\n",
    "            \"success\",\n",
    "        ]\n",
    "\n",
    "        df = pd.DataFrame(cursor.fetchall(), columns=query_columns)\n",
    "\n",
    "    except (Exception, psycopg2.Error) as error:\n",
    "        print(\"Error while fetching data from PostgreSQL\", error)\n",
    "    finally:\n",
    "        # closing database connection.\n",
    "        if connection:\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            print(\"PostgreSQL connection is closed\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def update_records(update_values):\n",
    "    try:\n",
    "        connection = psycopg2.connect(\n",
    "            user=DATABASE_USERNAME,\n",
    "            password=DATABASE_PASSWORD,\n",
    "            host=DATABASE_HOST,\n",
    "            port=DATABASE_PORT,\n",
    "            database=DATABASE_NAME,\n",
    "        )\n",
    "        cursor = connection.cursor()\n",
    "        update_sql_query = f\"\"\"UPDATE {QUERY_SCHEMA}.{QUERY_TABLE} q \n",
    "                                SET success = v.success,\n",
    "                                    checked = v.checked\n",
    "\n",
    "                                FROM (values {update_values}) AS v (name, user_id, checked, success)\n",
    "                                WHERE q.user_id = v.user_id \n",
    "                                AND q.name = v.name;\"\"\"\n",
    "        print(update_sql_query)\n",
    "        cursor.execute(update_sql_query)\n",
    "    except (Exception, psycopg2.Error) as error:\n",
    "        print(\"Error while updating data in PostgreSQL\", error)\n",
    "\n",
    "        cursor.execute(update_sql_query)\n",
    "\n",
    "    finally:\n",
    "        # closing database connection.\n",
    "        if connection:\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            print(\"PostgreSQL connection is closed\")\n",
    "\n",
    "\n",
    "def get_emails(superset, user_ids):\n",
    "    url = unquote(f\"/security/get_email/?q={list(user_ids)}\")\n",
    "    res = superset.request(\"GET\", url)\n",
    "    return res[\"emails\"]\n",
    "\n",
    "def get_tables_from_dbt(dbt_manifest, dbt_db_name):\n",
    "    \n",
    "    tables = {}\n",
    "    for table_type in [\"nodes\"]:\n",
    "        manifest_subset = dbt_manifest[table_type]\n",
    "\n",
    "        for table_key_long in manifest_subset:\n",
    "            table = manifest_subset[table_key_long]\n",
    "            name = table[\"name\"]\n",
    "            schema = table[\"schema\"]\n",
    "            database = table[\"database\"]\n",
    "            description = table[\"description\"]\n",
    "            alias = table[\"alias\"]\n",
    "            source = table[\"unique_id\"].split(\".\")[-2]\n",
    "            table_key = schema + \".\" + alias  # Key will be alias, not name\n",
    "            columns = table[\"columns\"]\n",
    "\n",
    "            if dbt_db_name is None or database == dbt_db_name:\n",
    "                # fail if it breaks uniqueness constraint\n",
    "                assert table_key not in tables, (\n",
    "                    f\"Table {table_key} is a duplicate name (schema + table) across databases. \"\n",
    "                    \"This would result in incorrect matching between Superset and dbt. \"\n",
    "                    \"To fix this, remove duplicates or add ``dbt_db_name``.\"\n",
    "                )\n",
    "                tables[table_key] = {\n",
    "                    \"name\": name,\n",
    "                    \"schema\": schema,\n",
    "                    \"database\": database,\n",
    "                    \"type\": table_type[:-1],\n",
    "                    \"ref\": f\"ref('{name}')\" if table_type == \"nodes\" else f\"source('{source}', '{name}')\",\n",
    "                    \"user\": None,\n",
    "                    \"columns\": columns,\n",
    "                    \"description\": description,\n",
    "                    \"alias\": alias,\n",
    "                }\n",
    "            if schema == \"user\":\n",
    "                tables[table_key][\"user\"] = table[\"tags\"][0]\n",
    "\n",
    "    assert tables, \"Manifest is empty!\"\n",
    "\n",
    "    return tables\n",
    "def get_tables_from_sql_simple(sql):\n",
    "    \"\"\"\n",
    "    (Superset) Fallback SQL parsing using regular expressions to get tables names.\n",
    "    \"\"\"\n",
    "    sql = re.sub(r\"(--.*)|(#.*)\", \"\", sql)\n",
    "    sql = re.sub(r\"\\s+\", \" \", sql).lower()\n",
    "    sql = re.sub(r\"(/\\*(.|\\n)*\\*/)\", \"\", sql)\n",
    "\n",
    "    regex = re.compile(r\"\\b(from|join)\\b\\s+(\\\"?(\\w+)\\\"?(\\.))?\\\"?(\\w+)\\\"?\\b\")\n",
    "    tables_match = regex.findall(sql)\n",
    "    tables = [\n",
    "        table[2] + \".\" + table[4] if table[2] != \"\" else table[4] for table in tables_match if table[4] != \"unnest\"\n",
    "    ]\n",
    "\n",
    "    tables = list(set(tables))\n",
    "\n",
    "    return tables\n",
    "\n",
    "\n",
    "def get_tables_from_sql(sql, dialect, sql_parsed=None):\n",
    "    \"\"\"\n",
    "    (Superset) SQL parsing using sqlfluff to get clean tables names.\n",
    "    If sqlfluff parsing fails it runs the above regex parsing func.\n",
    "    Returns a tables list.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not sql_parsed:\n",
    "            sql_parsed = sqlfluff.parse(sql, dialect=dialect)\n",
    "        tables_raw = list(get_json_segment(sql_parsed, \"table_reference\"))  # type: ignore\n",
    "        tables_cleaned = []  # With schema\n",
    "        for table_ref in tables_raw:\n",
    "            if isinstance(table_ref, list):\n",
    "                table_ref_identifier = []\n",
    "                # Get last 2 \"naked_identifier\"\n",
    "                for dictionary in table_ref[::-1]:\n",
    "                    if \"naked_identifier\" in dictionary:\n",
    "                        table_ref_identifier.append(dictionary[\"naked_identifier\"])\n",
    "                        if len(table_ref_identifier) == 2:\n",
    "                            tables_cleaned.append(\".\".join(table_ref_identifier[::-1]))\n",
    "                            break\n",
    "            if isinstance(table_ref, dict):\n",
    "                tables_cleaned.append(table_ref[\"naked_identifier\"])\n",
    "    except (\n",
    "        sqlfluff.core.errors.SQLParseError,  # type: ignore\n",
    "        sqlfluff.core.errors.SQLLexError,  # type: ignore\n",
    "        sqlfluff.core.errors.SQLFluffUserError,  # type: ignore\n",
    "        sqlfluff.api.simple.APIParsingError,  # type: ignore\n",
    "    ) as e:  # type: ignore\n",
    "        logging.warning(\n",
    "            \"Parsing SQL through sqlfluff failed. \"\n",
    "            \"Let me attempt this via regular expressions at least and \"\n",
    "            \"check the problematic query and error below.\\n%s\",\n",
    "            sql,\n",
    "            exc_info=e,\n",
    "        )\n",
    "        tables_cleaned = get_tables_from_sql_simple(sql)\n",
    "\n",
    "    return tables_cleaned\n",
    "\n",
    "def get_json_segment(\n",
    "    parse_result: Dict[str, Any], segment_type: str\n",
    ") -> Iterator[Union[str, Dict[str, Any], List[Dict[str, Any]]]]:\n",
    "    \"\"\"Recursively search JSON parse result for specified segment type.\n",
    "\n",
    "    Args:\n",
    "        parse_result (Dict[str, Any]): JSON parse result from `sqlfluff.fix`.\n",
    "        segment_type (str): The segment type to search for.\n",
    "\n",
    "    Yields:\n",
    "        Iterator[Union[str, Dict[str, Any], List[Dict[str, Any]]]]:\n",
    "        Retrieves children of specified segment type as either a string for a raw\n",
    "        segment or as JSON or an array of JSON for non-raw segments.\n",
    "    \"\"\"\n",
    "    for k, v in parse_result.items():\n",
    "        if k == segment_type:\n",
    "            yield v\n",
    "        elif isinstance(v, dict):\n",
    "            yield from get_json_segment(v, segment_type)\n",
    "        elif isinstance(v, list):\n",
    "            for s in v:\n",
    "                yield from get_json_segment(s, segment_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select * from financial_query.query where checked = False\n",
      "PostgreSQL connection is closed\n"
     ]
    }
   ],
   "source": [
    "df = get_records()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_string</th>\n",
       "      <th>materialization</th>\n",
       "      <th>user_id</th>\n",
       "      <th>description</th>\n",
       "      <th>insert_time</th>\n",
       "      <th>name</th>\n",
       "      <th>checked</th>\n",
       "      <th>success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SELECT * from marts.dim_balance_sheet</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2023-07-24 09:37:06.123662</td>\n",
       "      <td>asd</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SELECT * from dim_bollinger</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2023-07-23 08:11:58.188941</td>\n",
       "      <td>unique_2</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SELECT * from marts.dim_bollinger</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>asdad</td>\n",
       "      <td>2023-07-23 08:12:43.670466</td>\n",
       "      <td>unique_3</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SELECT * from marts.dim_balance_sheet</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>asdasdwa</td>\n",
       "      <td>2023-07-23 09:37:33.026336</td>\n",
       "      <td>unique_4</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               query_string  materialization  user_id  \\\n",
       "0     SELECT * from marts.dim_balance_sheet                2        1   \n",
       "1              SELECT * from dim_bollinger                 2        1   \n",
       "2        SELECT * from marts.dim_bollinger                 2        1   \n",
       "3  SELECT * from marts.dim_balance_sheet                   2        1   \n",
       "\n",
       "  description                insert_time      name  checked success  \n",
       "0             2023-07-24 09:37:06.123662       asd    False    None  \n",
       "1             2023-07-23 08:11:58.188941  unique_2    False    None  \n",
       "2       asdad 2023-07-23 08:12:43.670466  unique_3    False    None  \n",
       "3    asdasdwa 2023-07-23 09:37:33.026336  unique_4    False    None  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select * from financial_query.query where checked = False\n",
      "PostgreSQL connection is closed\n",
      "asd\n",
      "True\n",
      "True\n",
      "Wrote model asd contents\n",
      "unique_2\n",
      "True\n",
      "True\n",
      "unique_3\n",
      "True\n",
      "True\n",
      "Wrote model unique_3 contents\n",
      "unique_4\n",
      "True\n",
      "True\n",
      "Wrote model unique_4 contents\n",
      "\u001b[0m10:44:42  Running with dbt=1.5.1\n",
      "\u001b[0m10:44:43  Found 33 models, 73 tests, 6 snapshots, 0 analyses, 760 macros, 0 operations, 0 seed files, 12 sources, 1 exposure, 0 metrics, 0 groups\n",
      "\u001b[0m10:44:43  \n",
      "\u001b[0m10:44:52  Concurrency: 4 threads (target='dev_cloud')\n",
      "\u001b[0m10:44:52  \n",
      "\u001b[0m10:44:52  1 of 3 START sql view model financial_user.asd ................................. [RUN]\n",
      "\u001b[0m10:44:52  2 of 3 START sql view model financial_user.unique_3 ............................ [RUN]\n",
      "\u001b[0m10:44:52  3 of 3 START sql view model financial_user.unique_4 ............................ [RUN]\n",
      "\u001b[0m10:44:55  2 of 3 OK created sql view model financial_user.unique_3 ....................... [\u001b[32mCREATE VIEW\u001b[0m in 3.00s]\n",
      "\u001b[0m10:44:55  3 of 3 OK created sql view model financial_user.unique_4 ....................... [\u001b[32mCREATE VIEW\u001b[0m in 3.00s]\n",
      "\u001b[0m10:44:55  1 of 3 OK created sql view model financial_user.asd ............................ [\u001b[32mCREATE VIEW\u001b[0m in 3.02s]\n",
      "\u001b[0m10:44:57  \n",
      "\u001b[0m10:44:57  Finished running 3 view models in 0 hours 0 minutes and 14.55 seconds (14.55s).\n",
      "\u001b[0m10:44:57  \n",
      "\u001b[0m10:44:57  \u001b[32mCompleted successfully\u001b[0m\n",
      "\u001b[0m10:44:57  \n",
      "\u001b[0m10:44:57  Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3\n",
      "unique_3: success\n",
      "unique_4: success\n",
      "asd: success\n",
      "entries\n",
      "('asd', 1, True, True), ('unique_2', 1, True, False), ('unique_3', 1, True, True), ('unique_4', 1, True, True)\n",
      "UPDATE financial_query.query q \n",
      "                                SET success = v.success,\n",
      "                                    checked = v.checked\n",
      "\n",
      "                                FROM (values ('asd', 1, True, True), ('unique_2', 1, True, False), ('unique_3', 1, True, True), ('unique_4', 1, True, True)) AS v (name, user_id, checked, success)\n",
      "                                WHERE q.user_id = v.user_id \n",
      "                                AND q.name = v.name;\n",
      "PostgreSQL connection is closed\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger(\"create_query\")\n",
    "df = get_records()\n",
    "\n",
    "dbt = dbtRunner()\n",
    "cli_args = [\n",
    "    \"parse\",\n",
    "    \"--project-dir\",\n",
    "    DBT_PROJECT_DIR,\n",
    "]\n",
    "\n",
    "# if df.empty:\n",
    "#     print(df.empty)\n",
    "#     return \"Early stopping because no records\"\n",
    "\n",
    "# Get dagster execution time, see: https://stackoverflow.com/questions/75099470/getting-current-execution-date-in-a-task-or-asset-in-dagster\n",
    "EXEC_TIME = datetime.datetime.today().strftime(\"%d/%m/%Y_%H:%M:%S\")\n",
    "# raise Exception(DBT_PROJECT_DIR, MANIFEST_PATH, USER_MODEL_PATH)\n",
    "# Get all schema names in project\n",
    "# Either this or defined schema name available to the user before\n",
    "with open(MANIFEST_PATH) as f:\n",
    "    dbt_manifest = json.load(f)\n",
    "    dbt_tables = get_tables_from_dbt(dbt_manifest, None)\n",
    "\n",
    "# Getting the dbt tables keys\n",
    "dbt_tables_names = set(list(dbt_tables.keys()))\n",
    "status = []  # Status of preliminary checking\n",
    "dbt_names_aliases = [dbt_tables[table][\"name\"] for table in dbt_tables] + [\n",
    "    dbt_tables[table][\"alias\"] for table in dbt_tables\n",
    "]  # Name and aliases wo schema\n",
    "\n",
    "for i in df.index:\n",
    "    # Check name validity\n",
    "    print(df.loc[i][\"name\"])\n",
    "    name_validation = is_valid_table_name(df.loc[i][\"name\"])\n",
    "    print(name_validation)\n",
    "    if not name_validation:\n",
    "        status.append(\"Invalid name\")\n",
    "        df.loc[i, \"success\"] = False\n",
    "        continue\n",
    "    name_unique = is_unique_table_name(df.loc[i][\"name\"], dbt_names_aliases)  # check aliases and name\n",
    "    print(name_unique)\n",
    "    if not name_unique:\n",
    "        status.append(\"Model name is duplicated with another existing model\")\n",
    "        df.loc[i, \"success\"] = False\n",
    "        continue\n",
    "    # Check syntax\n",
    "    query_string = df.loc[i][\"query_string\"]\n",
    "    query_string = query_string + \";\" if query_string[-1] != \";\" else query_string\n",
    "    validation = check_string(query_string)\n",
    "    if not validation[0]:\n",
    "        df.loc[i, \"success\"] = False\n",
    "        status.append(\"Invalid query: {error}\".format(error=validation[1]))\n",
    "        continue\n",
    "    # Check multi-query\n",
    "    parsed = sqlfluff.parse(query_string, \"postgres\")[\"file\"]\n",
    "    if type(parsed) == list:\n",
    "        df.loc[i, \"success\"] = False\n",
    "        status.append(\"Multiple statement\")\n",
    "        continue\n",
    "    # Check select statements\n",
    "    # if list(statement_list[0][\"statement\"].keys())[0] != \"select_statement\":\n",
    "    #     df.loc[i, \"success\"] = False\n",
    "    #     status.append(\"Query is not 'SELECT'\")\n",
    "    #     continue\n",
    "    # Check tables and add model ref\n",
    "    partially_model, processed_status = get_ref(df.loc[i, \"query_string\"], dbt_tables, parsed, dbt_tables_names)\n",
    "    if processed_status != \"Success\":\n",
    "        df.loc[i, \"success\"] = False\n",
    "        status.append(processed_status)\n",
    "        continue\n",
    "    # Add config\n",
    "    processed_model = add_materialization(df.loc[i], partially_model, EXEC_TIME)\n",
    "\n",
    "    model_path = USER_MODEL_PATH + \"/{name}.sql\".format(name=df.loc[i, \"name\"])\n",
    "\n",
    "    with open(model_path, \"w+\") as f:\n",
    "        f.write(processed_model)\n",
    "        print(\"Wrote model {name} contents\".format(name=df.loc[i, \"name\"]))\n",
    "        f.close()\n",
    "    status.append(\"Success\")\n",
    "\n",
    "# Get Emails from API\n",
    "superset = SupersetDBTConnectorSession(logger=logger)\n",
    "users = set(df[\"user_id\"].to_list())\n",
    "email_dict = get_emails(superset, users)\n",
    "\n",
    "SMTP.login(EMAIL_SENDER, EMAIL_PASSWORD)\n",
    "\n",
    "for i in df.index:\n",
    "    # Check Success\n",
    "    if df.loc[i, \"success\"] == False:\n",
    "        message = \"\"\"\\\n",
    "Subject: Superset Model Creation\n",
    "\n",
    "Your Model was unsuccessfully created.\n",
    "\n",
    "Reason:\n",
    "{reason}\n",
    "\n",
    "SQL:\n",
    "{sql}\n",
    "\"\"\".format(\n",
    "            reason=status[i], sql=df.loc[i, \"query_string\"]\n",
    "        )\n",
    "\n",
    "        df.loc[i, \"checked\"] = True\n",
    "        SMTP.sendmail(EMAIL_SENDER, email_dict[str(df.loc[i, \"user_id\"])], message)\n",
    "\n",
    "# If every record is unsuccesful, terminate script early\n",
    "# if not df[\"success\"].any():\n",
    "#     entries_to_update = str(tuple(zip(df.name, df.user_id, df.checked, df.success))).replace(\"None\", \"Null\")[1:-1]\n",
    "#     print(\"entries\")\n",
    "#     print(entries_to_update)\n",
    "#     update_records(entries_to_update)\n",
    "#     return \"Early stopping because no successful records\"\n",
    "\n",
    "# initialize\n",
    "dbt = dbtRunner()\n",
    "\n",
    "# create CLI args as a list of strings\n",
    "cli_args = [\n",
    "    \"run\",\n",
    "    \"--project-dir\",\n",
    "    DBT_PROJECT_DIR,\n",
    "    \"--select\",\n",
    "    \"tag:{exec_time}\".format(exec_time=EXEC_TIME),\n",
    "]\n",
    "\n",
    "# run the command\n",
    "res: dbtRunnerResult = dbt.invoke(cli_args)\n",
    "\n",
    "# inspect the results\n",
    "for r in res.result:\n",
    "    print(f\"{r.node.name}: {r.status}\")\n",
    "# Map df index to result\n",
    "dbt_res_df_map = {}\n",
    "\n",
    "for i in df.index:\n",
    "    for r in res.result:\n",
    "        if r.node.name == df.loc[i, \"name\"]:\n",
    "            dbt_res_df_map[i] = r\n",
    "            break\n",
    "\n",
    "for i in df.index:\n",
    "    # Check Success\n",
    "    if df.loc[i, \"success\"] is not False:\n",
    "        if dbt_res_df_map[i].status == \"success\":\n",
    "            df.loc[i, \"success\"] = True\n",
    "            rison_request = \"/dataset/\"\n",
    "            # Data to be written\n",
    "            dictionary = {\n",
    "                # Parameter database\n",
    "                \"database\": DATABASE_ID,\n",
    "                \"schema\": USER_SCHEMA,\n",
    "                \"table_name\": df.loc[i, \"name\"],\n",
    "                \"owners\": [int(df.loc[i, \"user_id\"]), SUPERSET_ID],\n",
    "            }\n",
    "            # Serializing json\n",
    "            json_object = json.dumps(dictionary)\n",
    "            response = superset.request(\"POST\", rison_request, json=dictionary)\n",
    "\n",
    "            message = \"\"\"\\\n",
    "Subject: Superset Model Creation\n",
    "\n",
    "Your Model was successfully created. \n",
    "\n",
    "SQL:{sql}\n",
    "\"\"\".format(\n",
    "                sql=df.loc[i, \"query_string\"]\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            df.loc[i, \"success\"] = False\n",
    "            message = \"\"\"\\\n",
    "Subject: Superset Model Creation\n",
    "\n",
    "Your Model was unsuccessfully created during dbt's run, please contact the administrator.\n",
    "\n",
    "Reason:\n",
    "{reason}\n",
    "\n",
    "SQL:\n",
    "{sql}\n",
    "\"\"\".format(\n",
    "                reason=dbt_res_df_map[i].message, sql=df.loc[i, \"query_string\"]\n",
    "            )\n",
    "\n",
    "        df.loc[i, \"checked\"] = True\n",
    "\n",
    "        SMTP.sendmail(EMAIL_SENDER, email_dict[str(df.loc[i, \"user_id\"])], message)\n",
    "\n",
    "# Delete unsucessful model\n",
    "for i in df.index:\n",
    "    # Check Success\n",
    "    if not df.loc[i, \"success\"]:\n",
    "        model_path = \"models/user/{name}.sql\".format(name=df.loc[i, \"name\"])\n",
    "        if os.path.exists(model_path):\n",
    "            os.remove(model_path)\n",
    "\n",
    "entries_to_update = str(tuple(zip(df.name, df.user_id, df.checked, df.success))).replace(\"None\", \"Null\")[1:-1]\n",
    "print(\"entries\")\n",
    "print(entries_to_update)\n",
    "update_records(entries_to_update)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entries\n",
      "('asd', 1, True, True), ('unique_2', 1, True, False), ('unique_3', 1, True, True), ('unique_4', 1, True, True)\n"
     ]
    }
   ],
   "source": [
    "entries_to_update = str(tuple(zip(df.name, df.user_id, df.checked, df.success))).replace(\"None\", \"Null\")[1:-1]\n",
    "print(\"entries\")\n",
    "print(entries_to_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATE financial_query.query q \n",
      "                                SET success = v.success,\n",
      "                                    checked = v.checked\n",
      "\n",
      "                                FROM (values ('asd', 1, True, True), ('unique_2', 1, True, False), ('unique_3', 1, True, True), ('unique_4', 1, True, True)) AS v (name, user_id, checked, success)\n",
      "                                WHERE q.user_id = v.user_id \n",
      "                                AND q.name = v.name;\n",
      "PostgreSQL connection is closed\n"
     ]
    }
   ],
   "source": [
    "update_records(entries_to_update)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in dbt_res_df_map.keys():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_1: success\n",
      "False\n",
      "tetst: success\n",
      "False\n",
      "unique_1: success\n",
      "False\n",
      "tetst: success\n",
      "False\n",
      "unique_1: success\n",
      "False\n",
      "tetst: success\n",
      "False\n",
      "unique_1: success\n",
      "False\n",
      "tetst: success\n",
      "True\n",
      "unique_1: success\n",
      "True\n",
      "tetst: success\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for i in df.index:\n",
    "    for r in res.result:\n",
    "        print(f\"{r.node.name}: {r.status}\")\n",
    "        print(r.node.name == df.loc[i, \"name\"])\n",
    "# Map df index to result\n",
    "dbt_res_df_map = {}\n",
    "\n",
    "for i in df.index:\n",
    "    for r in res.result:\n",
    "        if r.node.name == df.loc[i, \"name\"]:\n",
    "            dbt_res_df_map[i] = r\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: RunResult(status=<RunStatus.Success: 'success'>, timing=[TimingInfo(name='compile', started_at=datetime.datetime(2023, 7, 23, 6, 49, 39, 256404), completed_at=datetime.datetime(2023, 7, 23, 6, 49, 39, 279021)), TimingInfo(name='execute', started_at=datetime.datetime(2023, 7, 23, 6, 49, 39, 283493), completed_at=datetime.datetime(2023, 7, 23, 6, 49, 42, 346239))], thread_id='Thread-40 (worker)', execution_time=3.102137327194214, adapter_response={'_message': 'SELECT 1979', 'code': 'SELECT', 'rows_affected': 1979}, message='SELECT 1979', failures=None, node=ModelNode(database='financial_data', schema='financial_user', name='tetst', resource_type=<NodeType.Model: 'model'>, package_name='dbt_financial', path='user/tetst.sql', original_file_path='models/user/tetst.sql', unique_id='model.dbt_financial.tetst', fqn=['dbt_financial', 'user', 'tetst'], alias='tetst', checksum=FileHash(name='sha256', checksum='8e23edf47bc6221ab2224b44712988691722cb47b9151c36229d1e70454a867a'), config=NodeConfig(_extra={'name': 'tetst', 'description': 'ffgfs'}, enabled=True, alias=None, schema='financial_user', database=None, tags=['1', 'user_created', '23/07/2023_13:49:17'], meta={}, group=None, materialized='table', incremental_strategy=None, persist_docs={}, post_hook=[], pre_hook=[], quoting={}, column_types={}, full_refresh=None, unique_key=None, on_schema_change='ignore', grants={}, packages=[], docs=Docs(show=True, node_color=None), contract=ContractConfig(enforced=False)), _event_status={}, tags=['1', 'user_created', '23/07/2023_13:49:17'], description='', columns={}, meta={}, group=None, docs=Docs(show=True, node_color=None), patch_path=None, build_path='target/run/dbt_financial/models/user/tetst.sql', deferred=False, unrendered_config={'materialized': 'table', 'name': 'tetst', 'description': 'ffgfs', 'tags': ['1', 'user_created', '23/07/2023_13:49:17'], 'schema': 'financial_user'}, created_at=1690094968.6971893, config_call_dict={'materialized': 'table', 'name': 'tetst', 'description': 'ffgfs', 'tags': ['1', 'user_created', '23/07/2023_13:49:17'], 'schema': 'financial_user'}, relation_name='\"financial_data\".\"financial_user\".\"tetst\"', raw_code=\"{{ config(\\n    materialized='table',\\n    name='tetst',\\n    description='ffgfs',\\n    tags = ['1','user_created','23/07/2023_13:49:17'],\\n    schema = 'financial_user'\\n) }}\\n    -- depends_on: {{ref('dim_price_history')}}\\n    SELECT * from marts.dim_price_history\", language=<ModelLanguage.sql: 'sql'>, refs=[RefArgs(name='dim_price_history', package=None, version=None)], sources=[], metrics=[], depends_on=DependsOn(macros=[], nodes=['model.dbt_financial.dim_price_history']), compiled_path='target/compiled/dbt_financial/models/user/tetst.sql', compiled=True, compiled_code='\\n    -- depends_on: \"financial_data\".\"marts\".\"dim_price_history\"\\n    SELECT * from marts.dim_price_history', extra_ctes_injected=True, extra_ctes=[], _pre_injected_sql=None, contract=Contract(enforced=False, checksum=None), access=<AccessType.Protected: 'protected'>, constraints=[], version=None, latest_version=None), agate_table=None),\n",
       " 4: RunResult(status=<RunStatus.Success: 'success'>, timing=[TimingInfo(name='compile', started_at=datetime.datetime(2023, 7, 23, 6, 49, 39, 265906), completed_at=datetime.datetime(2023, 7, 23, 6, 49, 39, 282062)), TimingInfo(name='execute', started_at=datetime.datetime(2023, 7, 23, 6, 49, 39, 296474), completed_at=datetime.datetime(2023, 7, 23, 6, 49, 42, 342207))], thread_id='Thread-41 (worker)', execution_time=3.0983898639678955, adapter_response={'_message': 'CREATE VIEW', 'code': 'CREATE VIEW', 'rows_affected': -1}, message='CREATE VIEW', failures=None, node=ModelNode(database='financial_data', schema='financial_user', name='unique_1', resource_type=<NodeType.Model: 'model'>, package_name='dbt_financial', path='user/unique_1.sql', original_file_path='models/user/unique_1.sql', unique_id='model.dbt_financial.unique_1', fqn=['dbt_financial', 'user', 'unique_1'], alias='unique_1', checksum=FileHash(name='sha256', checksum='d973ce87c0ba39cacb13b8ba5239fb8f7d0fbb7034bf9a17584f22c8459f5356'), config=NodeConfig(_extra={'name': 'unique_1', 'description': 'asd'}, enabled=True, alias=None, schema='financial_user', database=None, tags=['1', 'user_created', '23/07/2023_13:49:17'], meta={}, group=None, materialized='view', incremental_strategy=None, persist_docs={}, post_hook=[], pre_hook=[], quoting={}, column_types={}, full_refresh=None, unique_key=None, on_schema_change='ignore', grants={}, packages=[], docs=Docs(show=True, node_color=None), contract=ContractConfig(enforced=False)), _event_status={}, tags=['1', 'user_created', '23/07/2023_13:49:17'], description='', columns={}, meta={}, group=None, docs=Docs(show=True, node_color=None), patch_path=None, build_path='target/run/dbt_financial/models/user/unique_1.sql', deferred=False, unrendered_config={'materialized': 'view', 'name': 'unique_1', 'description': 'asd', 'tags': ['1', 'user_created', '23/07/2023_13:49:17'], 'schema': 'financial_user'}, created_at=1690094968.7050407, config_call_dict={'materialized': 'view', 'name': 'unique_1', 'description': 'asd', 'tags': ['1', 'user_created', '23/07/2023_13:49:17'], 'schema': 'financial_user'}, relation_name='\"financial_data\".\"financial_user\".\"unique_1\"', raw_code=\"{{ config(\\n    materialized='view',\\n    name='unique_1',\\n    description='asd',\\n    tags = ['1','user_created','23/07/2023_13:49:17'],\\n    schema = 'financial_user'\\n) }}\\n    -- depends_on: {{ref('dim_price_history')}}\\n    SELECT * from marts.dim_price_history\", language=<ModelLanguage.sql: 'sql'>, refs=[RefArgs(name='dim_price_history', package=None, version=None)], sources=[], metrics=[], depends_on=DependsOn(macros=[], nodes=['model.dbt_financial.dim_price_history']), compiled_path='target/compiled/dbt_financial/models/user/unique_1.sql', compiled=True, compiled_code='\\n    -- depends_on: \"financial_data\".\"marts\".\"dim_price_history\"\\n    SELECT * from marts.dim_price_history', extra_ctes_injected=True, extra_ctes=[], _pre_injected_sql=None, contract=Contract(enforced=False, checksum=None), access=<AccessType.Protected: 'protected'>, constraints=[], version=None, latest_version=None), agate_table=None)}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbt_res_df_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: RunResult(status=<RunStatus.Success: 'success'>, timing=[TimingInfo(name='compile', started_at=datetime.datetime(2023, 7, 23, 6, 36, 18, 918788), completed_at=datetime.datetime(2023, 7, 23, 6, 36, 18, 941524)), TimingInfo(name='execute', started_at=datetime.datetime(2023, 7, 23, 6, 36, 18, 946818), completed_at=datetime.datetime(2023, 7, 23, 6, 36, 22, 629960))], thread_id='Thread-26 (worker)', execution_time=3.7253305912017822, adapter_response={'_message': 'SELECT 1979', 'code': 'SELECT', 'rows_affected': 1979}, message='SELECT 1979', failures=None, node=ModelNode(database='financial_data', schema='financial_user', name='tetst', resource_type=<NodeType.Model: 'model'>, package_name='dbt_financial', path='user/tetst.sql', original_file_path='models/user/tetst.sql', unique_id='model.dbt_financial.tetst', fqn=['dbt_financial', 'user', 'tetst'], alias='tetst', checksum=FileHash(name='sha256', checksum='446ff4b5dcaab790e15e72b1b78d8cec877be517e2f2d157d7fcb5c8c82ed6c1'), config=NodeConfig(_extra={'name': 'tetst', 'description': 'ffgfs'}, enabled=True, alias=None, schema='financial_user', database=None, tags=['1', 'user_created', '23/07/2023_13:35:52'], meta={}, group=None, materialized='table', incremental_strategy=None, persist_docs={}, post_hook=[], pre_hook=[], quoting={}, column_types={}, full_refresh=None, unique_key=None, on_schema_change='ignore', grants={}, packages=[], docs=Docs(show=True, node_color=None), contract=ContractConfig(enforced=False)), _event_status={}, tags=['1', 'user_created', '23/07/2023_13:35:52'], description='', columns={}, meta={}, group=None, docs=Docs(show=True, node_color=None), patch_path=None, build_path='target/run/dbt_financial/models/user/tetst.sql', deferred=False, unrendered_config={'materialized': 'table', 'name': 'tetst', 'description': 'ffgfs', 'tags': ['1', 'user_created', '23/07/2023_13:35:52'], 'schema': 'financial_user'}, created_at=1690094166.8347187, config_call_dict={'materialized': 'table', 'name': 'tetst', 'description': 'ffgfs', 'tags': ['1', 'user_created', '23/07/2023_13:35:52'], 'schema': 'financial_user'}, relation_name='\"financial_data\".\"financial_user\".\"tetst\"', raw_code=\"{{ config(\\n    materialized='table',\\n    name='tetst',\\n    description='ffgfs',\\n    tags = ['1','user_created','23/07/2023_13:35:52'],\\n    schema = 'financial_user'\\n) }}\\n    -- depends_on: {{ref('dim_price_history')}}\\n    SELECT * from marts.dim_price_history\", language=<ModelLanguage.sql: 'sql'>, refs=[RefArgs(name='dim_price_history', package=None, version=None)], sources=[], metrics=[], depends_on=DependsOn(macros=[], nodes=['model.dbt_financial.dim_price_history']), compiled_path='target/compiled/dbt_financial/models/user/tetst.sql', compiled=True, compiled_code='\\n    -- depends_on: \"financial_data\".\"marts\".\"dim_price_history\"\\n    SELECT * from marts.dim_price_history', extra_ctes_injected=True, extra_ctes=[], _pre_injected_sql=None, contract=Contract(enforced=False, checksum=None), access=<AccessType.Protected: 'protected'>, constraints=[], version=None, latest_version=None), agate_table=None)}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbt_res_df_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbt_res_df_map[3].status==\"success\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.index:\n",
    "    # Check Success\n",
    "    if not df.loc[i, \"success\"]:\n",
    "        model_path = USER_MODEL_PATH + \"/{name}.sql\".format(name=df.loc[i, \"name\"])\n",
    "        if os.path.exists(model_path):\n",
    "            os.remove(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_string</th>\n",
       "      <th>materialization</th>\n",
       "      <th>user_id</th>\n",
       "      <th>description</th>\n",
       "      <th>insert_time</th>\n",
       "      <th>name</th>\n",
       "      <th>success</th>\n",
       "      <th>checked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SELECT * from marts.dim_price_history</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>test description</td>\n",
       "      <td>2023-07-15 05:40:36.283000</td>\n",
       "      <td>test_query_hgffhgf</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SELECT * from dim_price_history</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>test_desc</td>\n",
       "      <td>2023-07-15 06:40:36.283000</td>\n",
       "      <td>test_query_2_gftyf</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SELECT * from dim_price_history</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>test_desc</td>\n",
       "      <td>2023-07-20 14:13:51.968099</td>\n",
       "      <td>test_query_123</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SELECT * from marts.dim_price_history</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>ffgfs</td>\n",
       "      <td>2023-07-22 09:47:39.173512</td>\n",
       "      <td>tetst</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SELECT * from marts.dim_price_history</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>asd</td>\n",
       "      <td>2023-07-22 09:47:39.173000</td>\n",
       "      <td>unique_1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            query_string  materialization  user_id  \\\n",
       "0  SELECT * from marts.dim_price_history                1        1   \n",
       "1        SELECT * from dim_price_history                2        1   \n",
       "2        SELECT * from dim_price_history                1        1   \n",
       "3  SELECT * from marts.dim_price_history                1        1   \n",
       "4  SELECT * from marts.dim_price_history                2        1   \n",
       "\n",
       "        description                insert_time                name  success  \\\n",
       "0  test description 2023-07-15 05:40:36.283000  test_query_hgffhgf    False   \n",
       "1         test_desc 2023-07-15 06:40:36.283000  test_query_2_gftyf    False   \n",
       "2         test_desc 2023-07-20 14:13:51.968099      test_query_123    False   \n",
       "3             ffgfs 2023-07-22 09:47:39.173512               tetst    False   \n",
       "4               asd 2023-07-22 09:47:39.173000            unique_1    False   \n",
       "\n",
       "  checked  \n",
       "0    True  \n",
       "1    True  \n",
       "2    True  \n",
       "3    True  \n",
       "4    True  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunExecutionResult(results=[RunResult(status=<RunStatus.Error: 'error'>, timing=[], thread_id='Thread-5 (worker)', execution_time=2.9518167972564697, adapter_response={}, message='Database Error in model tetst (models/user/tetst.sql)\\n  syntax error at or near \")\"\\n  LINE 14:   );\\n             ^\\n  compiled Code at target/run/dbt_financial/models/user/tetst.sql', failures=None, node=ModelNode(database='financial_data', schema='financial_user', name='tetst', resource_type=<NodeType.Model: 'model'>, package_name='dbt_financial', path='user/tetst.sql', original_file_path='models/user/tetst.sql', unique_id='model.dbt_financial.tetst', fqn=['dbt_financial', 'user', 'tetst'], alias='tetst', checksum=FileHash(name='sha256', checksum='0914864f78b4327814b8b537d6a6a724dfb24533730814a7129d43b775b9f5ed'), config=NodeConfig(_extra={'name': 'tetst', 'description': 'ffgfs'}, enabled=True, alias=None, schema='financial_user', database=None, tags=['1', 'user_created', '23/07/2023_13:27:18'], meta={}, group=None, materialized='table', incremental_strategy=None, persist_docs={}, post_hook=[], pre_hook=[], quoting={}, column_types={}, full_refresh=None, unique_key=None, on_schema_change='ignore', grants={}, packages=[], docs=Docs(show=True, node_color=None), contract=ContractConfig(enforced=False)), _event_status={}, tags=['1', 'user_created', '23/07/2023_13:27:18'], description='', columns={}, meta={}, group=None, docs=Docs(show=True, node_color=None), patch_path=None, build_path='target/run/dbt_financial/models/user/tetst.sql', deferred=False, unrendered_config={'materialized': 'table', 'name': 'tetst', 'description': 'ffgfs', 'tags': ['1', 'user_created', '23/07/2023_13:27:18'], 'schema': 'financial_user'}, created_at=1690093652.259449, config_call_dict={'materialized': 'table', 'name': 'tetst', 'description': 'ffgfs', 'tags': ['1', 'user_created', '23/07/2023_13:27:18'], 'schema': 'financial_user'}, relation_name='\"financial_data\".\"financial_user\".\"tetst\"', raw_code=\"{{ config(\\n    materialized='table',\\n    name='tetst',\\n    description='ffgfs',\\n    tags = ['1','user_created','23/07/2023_13:27:18'],\\n    schema = 'financial_user'\\n) }}\\n    -- depends_on: {{ref('dim_price_history')}}SELECT * from marts.dim_price_history\", language=<ModelLanguage.sql: 'sql'>, refs=[RefArgs(name='dim_price_history', package=None, version=None)], sources=[], metrics=[], depends_on=DependsOn(macros=[], nodes=['model.dbt_financial.dim_price_history']), compiled_path='target/compiled/dbt_financial/models/user/tetst.sql', compiled=True, compiled_code='\\n    -- depends_on: \"financial_data\".\"marts\".\"dim_price_history\"SELECT * from marts.dim_price_history', extra_ctes_injected=True, extra_ctes=[], _pre_injected_sql=None, contract=Contract(enforced=False, checksum=None), access=<AccessType.Protected: 'protected'>, constraints=[], version=None, latest_version=None), agate_table=None), RunResult(status=<RunStatus.Error: 'error'>, timing=[], thread_id='Thread-6 (worker)', execution_time=2.9538426399230957, adapter_response={}, message='Database Error in model unique_1 (models/user/unique_1.sql)\\n  syntax error at or near \")\"\\n  LINE 9:   );\\n            ^\\n  compiled Code at target/run/dbt_financial/models/user/unique_1.sql', failures=None, node=ModelNode(database='financial_data', schema='financial_user', name='unique_1', resource_type=<NodeType.Model: 'model'>, package_name='dbt_financial', path='user/unique_1.sql', original_file_path='models/user/unique_1.sql', unique_id='model.dbt_financial.unique_1', fqn=['dbt_financial', 'user', 'unique_1'], alias='unique_1', checksum=FileHash(name='sha256', checksum='893f1d138adef386e33eee76dae2064648fc25ed1c78e07eda3cbe07929346db'), config=NodeConfig(_extra={'name': 'unique_1', 'description': 'asd'}, enabled=True, alias=None, schema='financial_user', database=None, tags=['1', 'user_created', '23/07/2023_13:27:18'], meta={}, group=None, materialized='view', incremental_strategy=None, persist_docs={}, post_hook=[], pre_hook=[], quoting={}, column_types={}, full_refresh=None, unique_key=None, on_schema_change='ignore', grants={}, packages=[], docs=Docs(show=True, node_color=None), contract=ContractConfig(enforced=False)), _event_status={}, tags=['1', 'user_created', '23/07/2023_13:27:18'], description='', columns={}, meta={}, group=None, docs=Docs(show=True, node_color=None), patch_path=None, build_path='target/run/dbt_financial/models/user/unique_1.sql', deferred=False, unrendered_config={'materialized': 'view', 'name': 'unique_1', 'description': 'asd', 'tags': ['1', 'user_created', '23/07/2023_13:27:18'], 'schema': 'financial_user'}, created_at=1690093652.2877269, config_call_dict={'materialized': 'view', 'name': 'unique_1', 'description': 'asd', 'tags': ['1', 'user_created', '23/07/2023_13:27:18'], 'schema': 'financial_user'}, relation_name='\"financial_data\".\"financial_user\".\"unique_1\"', raw_code=\"{{ config(\\n    materialized='view',\\n    name='unique_1',\\n    description='asd',\\n    tags = ['1','user_created','23/07/2023_13:27:18'],\\n    schema = 'financial_user'\\n) }}\\n    -- depends_on: {{ref('dim_price_history')}}SELECT * from marts.dim_price_history\", language=<ModelLanguage.sql: 'sql'>, refs=[RefArgs(name='dim_price_history', package=None, version=None)], sources=[], metrics=[], depends_on=DependsOn(macros=[], nodes=['model.dbt_financial.dim_price_history']), compiled_path='target/compiled/dbt_financial/models/user/unique_1.sql', compiled=True, compiled_code='\\n    -- depends_on: \"financial_data\".\"marts\".\"dim_price_history\"SELECT * from marts.dim_price_history', extra_ctes_injected=True, extra_ctes=[], _pre_injected_sql=None, contract=Contract(enforced=False, checksum=None), access=<AccessType.Protected: 'protected'>, constraints=[], version=None, latest_version=None), agate_table=None)], elapsed_time=16.68212652206421, args={'defer': False, 'partial_parse': True, 'send_anonymous_usage_stats': True, 'use_colors': True, 'enable_legacy_logger': False, 'populate_cache': True, 'log_path': '/home/vu/Desktop/Projects/Thesis/financial/dbt/logs', 'macro_debugging': False, 'profiles_dir': '/home/vu/.dbt', 'which': 'run', 'log_level': 'info', 'static_parser': True, 'use_colors_file': True, 'log_format_file': 'debug', 'strict_mode': False, 'project_dir': '/home/vu/Desktop/Projects/Thesis/financial/dbt/', 'exclude': (), 'printer_width': 80, 'write_json': True, 'print': True, 'favor_state': False, 'introspect': True, 'cache_selected_only': False, 'quiet': False, 'warn_error_options': WarnErrorOptions(include=[], exclude=[]), 'version_check': True, 'log_level_file': 'debug', 'indirect_selection': 'eager', 'vars': {}, 'select': ('tag:23/07/2023_13:27:18',), 'log_format': 'default'}, generated_at=datetime.datetime(2023, 7, 23, 6, 27, 49, 98288))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{{ config(\n",
      "    materialized='table',\n",
      "    name='tetst',\n",
      "    description='ffgfs',\n",
      "    tags = ['1','user_created','23/07/2023_13:22:24'],\n",
      "    schema = 'financial_user'\n",
      ") }}\n"
     ]
    }
   ],
   "source": [
    "print(add_materialization(df.loc[i], \"\", EXEC_TIME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_names = set(get_tables_from_sql(\"SELECT * from marts.dim_price_history\", dialect=\"postgres\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_names.difference(dbt_tables_reporting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "UPDATE financial_query.query q \n",
    "                                SET success = v.success,\n",
    "                                    checked = v.checked\n",
    "\n",
    "                                FROM (values ('test_query_hgffhgf', 1, True, False), ('test_query_2_gftyf', 1, True, False), ('test_query_123', 1, True, False), ('tetst', 1, True, False)) AS v (name, user_id, checked, success)\n",
    "                                WHERE q.user_id = v.user_id \n",
    "                                AND q.name = v.name;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 'catvu113@gmail.com'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m email_dict[df\u001b[39m.\u001b[39;49mloc[i, \u001b[39m\"\u001b[39;49m\u001b[39muser_id\u001b[39;49m\u001b[39m\"\u001b[39;49m]]\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "email_dict[df.loc[i, \"user_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('test_query_hgffhgf', 1, False, False), ('test_query_2_gftyf', 1, False, False), ('test_query_123', 1, Null, False), ('tetst', 1, Null, False)\n"
     ]
    }
   ],
   "source": [
    "entries_to_update = str(tuple(zip(df.name, df.user_id, df.checked, df.success))).replace(\"None\", \"Null\")[1:-1]\n",
    "print(entries_to_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATE financial_query.query q \n",
      "                        SET success = v.success,\n",
      "                            checked = v.checked\n",
      "\n",
      "                        FROM (values ('test_query_hgffhgf', 1, True, False), ('test_query_2_gftyf', 1, True, False), ('test_query_123', 1, True, False), ('tetst', 1, True, False), ('unique_1', 1, True, False)) AS v (name, user_id, checked, success)\n",
      "                        WHERE q.user_id = v.user_id \n",
      "                        AND q.name = v.name;\n"
     ]
    }
   ],
   "source": [
    "update_sql_query = f\"\"\"UPDATE {QUERY_SCHEMA}.{QUERY_TABLE} q \n",
    "                        SET success = v.success,\n",
    "                            checked = v.checked\n",
    "\n",
    "                        FROM (values {entries_to_update}) AS v (name, user_id, checked, success)\n",
    "                        WHERE q.user_id = v.user_id \n",
    "                        AND q.name = v.name;\"\"\"\n",
    "print(update_sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_string</th>\n",
       "      <th>materialization</th>\n",
       "      <th>user_id</th>\n",
       "      <th>description</th>\n",
       "      <th>insert_time</th>\n",
       "      <th>name</th>\n",
       "      <th>checked</th>\n",
       "      <th>success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SELECT * from dim_bollinger</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2023-07-23 08:11:58.188941</td>\n",
       "      <td>unique_2</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SELECT * from marts.dim_bollinger</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>asdad</td>\n",
       "      <td>2023-07-23 08:12:43.670466</td>\n",
       "      <td>unique_3</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         query_string  materialization  user_id description  \\\n",
       "0        SELECT * from dim_bollinger                 2        1               \n",
       "1  SELECT * from marts.dim_bollinger                 2        1       asdad   \n",
       "\n",
       "                 insert_time      name  checked success  \n",
       "0 2023-07-23 08:11:58.188941  unique_2    False    None  \n",
       "1 2023-07-23 08:12:43.670466  unique_3    False    None  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select * from financial_query.query where checked = False\n",
      "PostgreSQL connection is closed\n",
      "unique_2\n",
      "True\n",
      "True\n",
      "Wrote model unique_2 contents\n",
      "unique_3\n",
      "True\n",
      "True\n",
      "Wrote model unique_3 contents\n",
      "entries\n",
      "('unique_2', 1, False, Null), ('unique_3', 1, False, Null)\n",
      "UPDATE financial_query.query q \n",
      "                                SET success = v.success,\n",
      "                                    checked = v.checked\n",
      "\n",
      "                                FROM (values ('unique_2', 1, False, Null), ('unique_3', 1, False, Null)) AS v (name, user_id, checked, success)\n",
      "                                WHERE q.user_id = v.user_id \n",
      "                                AND q.name = v.name;\n",
      "Error while updating data in PostgreSQL column \"success\" is of type boolean but expression is of type text\n",
      "LINE 2:                                 SET success = v.success,\n",
      "                                                      ^\n",
      "HINT:  You will need to rewrite or cast the expression.\n",
      "\n",
      "PostgreSQL connection is closed\n"
     ]
    },
    {
     "ename": "InFailedSqlTransaction",
     "evalue": "current transaction is aborted, commands ignored until end of transaction block\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDatatypeMismatch\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 196\u001b[0m, in \u001b[0;36mupdate_records\u001b[0;34m(update_values)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[39mprint\u001b[39m(update_sql_query)\n\u001b[0;32m--> 196\u001b[0m     cursor\u001b[39m.\u001b[39;49mexecute(update_sql_query)\n\u001b[1;32m    197\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mException\u001b[39;00m, psycopg2\u001b[39m.\u001b[39mError) \u001b[39mas\u001b[39;00m error:\n",
      "\u001b[0;31mDatatypeMismatch\u001b[0m: column \"success\" is of type boolean but expression is of type text\nLINE 2:                                 SET success = v.success,\n                                                      ^\nHINT:  You will need to rewrite or cast the expression.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInFailedSqlTransaction\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 106\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mentries\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    105\u001b[0m     \u001b[39mprint\u001b[39m(entries_to_update)\n\u001b[0;32m--> 106\u001b[0m     update_records(entries_to_update)\n\u001b[1;32m    109\u001b[0m \u001b[39m# initialize\u001b[39;00m\n\u001b[1;32m    110\u001b[0m dbt \u001b[39m=\u001b[39m dbtRunner()\n",
      "Cell \u001b[0;32mIn[4], line 200\u001b[0m, in \u001b[0;36mupdate_records\u001b[0;34m(update_values)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mException\u001b[39;00m, psycopg2\u001b[39m.\u001b[39mError) \u001b[39mas\u001b[39;00m error:\n\u001b[1;32m    198\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mError while updating data in PostgreSQL\u001b[39m\u001b[39m\"\u001b[39m, error)\n\u001b[0;32m--> 200\u001b[0m     cursor\u001b[39m.\u001b[39;49mexecute(update_sql_query)\n\u001b[1;32m    202\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[39m# closing database connection.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     \u001b[39mif\u001b[39;00m connection:\n",
      "\u001b[0;31mInFailedSqlTransaction\u001b[0m: current transaction is aborted, commands ignored until end of transaction block\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'23/07/2023_15:28:01'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXEC_TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m08:23:04  Running with dbt=1.5.1\n",
      "\u001b[0m08:23:04  Found 34 models, 73 tests, 6 snapshots, 0 analyses, 760 macros, 0 operations, 0 seed files, 12 sources, 1 exposure, 0 metrics, 0 groups\n",
      "\u001b[0m08:23:04  \n",
      "\u001b[0m08:23:15  Concurrency: 4 threads (target='dev_cloud')\n",
      "\u001b[0m08:23:15  \n",
      "\u001b[0m08:23:15  1 of 2 START sql view model financial_user.unique_2 ............................ [RUN]\n",
      "\u001b[0m08:23:15  2 of 2 START sql view model financial_user.unique_3 ............................ [RUN]\n",
      "\u001b[0m08:23:18  1 of 2 ERROR creating sql view model financial_user.unique_2 ................... [\u001b[31mERROR\u001b[0m in 3.17s]\n",
      "\u001b[0m08:23:19  2 of 2 OK created sql view model financial_user.unique_3 ....................... [\u001b[32mCREATE VIEW\u001b[0m in 4.27s]\n",
      "\u001b[0m08:23:21  \n",
      "\u001b[0m08:23:21  Finished running 2 view models in 0 hours 0 minutes and 16.71 seconds (16.71s).\n",
      "\u001b[0m08:23:21  \n",
      "\u001b[0m08:23:21  \u001b[31mCompleted with 1 error and 0 warnings:\u001b[0m\n",
      "\u001b[0m08:23:21  \n",
      "\u001b[0m08:23:21  \u001b[33mDatabase Error in model unique_2 (models/user/unique_2.sql)\u001b[0m\n",
      "\u001b[0m08:23:21    relation \"dim_bollinger\" does not exist\n",
      "\u001b[0m08:23:21    LINE 9:     SELECT * from dim_bollinger\n",
      "\u001b[0m08:23:21                              ^\n",
      "\u001b[0m08:23:21    compiled Code at target/run/dbt_financial/models/user/unique_2.sql\n",
      "\u001b[0m08:23:21  \n",
      "\u001b[0m08:23:21  Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2\n"
     ]
    }
   ],
   "source": [
    "    # initialize\n",
    "    dbt = dbtRunner()\n",
    "\n",
    "    # create CLI args as a list of strings\n",
    "    cli_args = [\n",
    "        \"run\",\n",
    "        \"--project-dir\",\n",
    "        DBT_PROJECT_DIR,\n",
    "        \"--select\",\n",
    "        \"tag:{exec_time}\".format(exec_time=EXEC_TIME),\n",
    "    ]\n",
    "\n",
    "    # run the command\n",
    "    res: dbtRunnerResult = dbt.invoke(cli_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT * from dim_bollinger\n",
    "\"\"\"\n",
    "parsed=sqlfluff.parse(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "partially_model, processed_status = get_ref(sql, dbt_tables, parsed, dbt_tables_reporting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    -- depends_on: {{ref('dim_bollinger')}}\\n    \\nSELECT * from dim_bollinger\\n\""
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partially_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tables referenced out of serving schemas', 'Success']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "422 Client Error: UNPROCESSABLE ENTITY for url: http://34.82.185.252:30007/api/v1/dataset/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39m# Serializing json\u001b[39;00m\n\u001b[1;32m     10\u001b[0m json_object \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mdumps(dictionary)\n\u001b[0;32m---> 11\u001b[0m response \u001b[39m=\u001b[39m superset\u001b[39m.\u001b[39;49mrequest(\u001b[39m\"\u001b[39;49m\u001b[39mPOST\u001b[39;49m\u001b[39m\"\u001b[39;49m, rison_request, json\u001b[39m=\u001b[39;49mdictionary)\n",
      "Cell \u001b[0;32mIn[3], line 96\u001b[0m, in \u001b[0;36mSupersetDBTConnectorSession.request\u001b[0;34m(self, method, endpoint, refresh_session_if_needed, headers, **request_kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession\u001b[39m.\u001b[39mrequest(method, url, headers\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheaders, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mrequest_kwargs)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mRequest finished with status: \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m, res\u001b[39m.\u001b[39mstatus_code)\n\u001b[0;32m---> 96\u001b[0m res\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m     97\u001b[0m \u001b[39mreturn\u001b[39;00m res\u001b[39m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/Desktop/Projects/Thesis/venv/lib/python3.10/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     http_error_msg \u001b[39m=\u001b[39m (\n\u001b[1;32m   1017\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m Server Error: \u001b[39m\u001b[39m{\u001b[39;00mreason\u001b[39m}\u001b[39;00m\u001b[39m for url: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1018\u001b[0m     )\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 422 Client Error: UNPROCESSABLE ENTITY for url: http://34.82.185.252:30007/api/v1/dataset/"
     ]
    }
   ],
   "source": [
    "rison_request=\"/dataset\"\n",
    "dictionary = {\n",
    "    # Parameter database\n",
    "    \"database\": DATABASE_ID,\n",
    "    \"schema\": USER_SCHEMA,\n",
    "    \"table_name\": df.loc[i, \"name\"],\n",
    "    \"owners\": [int(df.loc[i, \"user_id\"]), SUPERSET_ID],\n",
    "}\n",
    "# Serializing json\n",
    "json_object = json.dumps(dictionary)\n",
    "response = superset.request(\"POST\", rison_request, json=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 113,\n",
       " 'result': {'database': 1,\n",
       "  'owners': [1, 34],\n",
       "  'schema': 'financial_user',\n",
       "  'table_name': 'unique_4'}}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "rison_request=\"/dataset\"\n",
    "dictionary = {\n",
    "    # Parameter database\n",
    "    \"database\": DATABASE_ID,\n",
    "    \"schema\": USER_SCHEMA,\n",
    "    \"table_name\": \"unique_3\",\n",
    "    \"owners\": [int(df.loc[i, \"user_id\"]), SUPERSET_ID],\n",
    "}\n",
    "# Serializing json\n",
    "json_object = json.dumps(dictionary)\n",
    "response = superset.request(\"POST\", rison_request, json=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m09:48:04  Running with dbt=1.5.1\n",
      "\u001b[0m09:48:05  Performance info: target/perf_info.json\n"
     ]
    }
   ],
   "source": [
    "dbt = dbtRunner()\n",
    "cli_args = [\"parse\",\"--project-dir\",\n",
    "    DBT_PROJECT_DIR,]\n",
    "res = dbt.invoke(cli_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_string</th>\n",
       "      <th>materialization</th>\n",
       "      <th>user_id</th>\n",
       "      <th>description</th>\n",
       "      <th>insert_time</th>\n",
       "      <th>name</th>\n",
       "      <th>checked</th>\n",
       "      <th>success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SELECT * from dim_bollinger</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2023-07-23 08:11:58.188941</td>\n",
       "      <td>unique_2</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SELECT * from marts.dim_bollinger</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>asdad</td>\n",
       "      <td>2023-07-23 08:12:43.670466</td>\n",
       "      <td>unique_3</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         query_string  materialization  user_id description  \\\n",
       "0        SELECT * from dim_bollinger                 2        1               \n",
       "1  SELECT * from marts.dim_bollinger                 2        1       asdad   \n",
       "\n",
       "                 insert_time      name  checked success  \n",
       "0 2023-07-23 08:11:58.188941  unique_2    False    None  \n",
       "1 2023-07-23 08:12:43.670466  unique_3    False    None  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_2: error\n",
      "unique_3: success\n"
     ]
    }
   ],
   "source": [
    "for r in res.result:\n",
    "    print(f\"{r.node.name}: {r.status}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
