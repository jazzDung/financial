{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Union\n",
    "import pandas as pd\n",
    "import logging\n",
    "import re\n",
    "import sqlfluff\n",
    "import requests\n",
    "import ruamel.yaml\n",
    "import psycopg2\n",
    "from bs4 import BeautifulSoup\n",
    "from markdown import markdown\n",
    "import datetime\n",
    "from urllib.parse import unquote\n",
    "from typing import Any, Dict, Iterator, List, Union\n",
    "import json\n",
    "from pgsanity.pgsanity import check_string\n",
    "import os\n",
    "from dbt.cli.main import dbtRunner, dbtRunnerResult\n",
    "import smtplib\n",
    "import ssl\n",
    "from itertools import compress\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MATERIALIZATION_MAPPING = {1: \"table\", 2: \"view\", 3: \"incremental\", 4: \"ephemereal\"}\n",
    "SUPERSET_USERNAME = \"superset\"\n",
    "SUPERSET_PASSWORD = \"superset\"\n",
    "SUPERSET_HOST = \"http://34.82.185.252:30007/\"\n",
    "DATABASE_USERNAME = \"fdp\"\n",
    "DATABASE_PASSWORD = \"fdp\"\n",
    "DATABASE_HOST = \"34.82.185.252\"\n",
    "DATABASE_PORT = 30005\n",
    "DATABASE_NAME = \"financial_data\"\n",
    "QUERY_SCHEMA=\"financial_query\"\n",
    "QUERY_TABLE=\"query\"\n",
    "MANIFEST_PATH=\"/home/vu/Desktop/Projects/Thesis/financial/dbt/target/manifest.json\"\n",
    "EMAIL_PORT = 465\n",
    "SMTP = \"smtp.gmail.com\"\n",
    "EMAIL_SENDER = \"catvu113@gmail.com\"\n",
    "EMAIL_PASSWORD = \"xhtzakhmnsbufufy\"\n",
    "USER_MODEL_PATH = \"/home/vu/Desktop/Projects/Thesis/financial/dbt/models/user\"\n",
    "DBT_PROJECT_DIR = \"/home/vu/Desktop/Projects/Thesis/financial/dbt/\"\n",
    "DATABASE_ID = 1\n",
    "SUPERSET_ID = 34\n",
    "USER_SCHEMA = \"user\"\n",
    "SERVING_SCHEMA=\"marts\"\n",
    "context = ssl.create_default_context()\n",
    "SMTP = smtplib.SMTP_SSL(\"smtp.gmail.com\", 465, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#### ADD ALL OF THE END POINT USED TO CSRF EXEMPT LIST TO RUN PARALLELY\n",
    "#### ONLY USE SESSION FOR SEQUENTIAL RUNNING SCRIPTS\n",
    "\n",
    "\n",
    "class SupersetDBTConnectorSession:\n",
    "    \"\"\"A class for accessing the Superset API in an easy way.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Instantiates the class.\n",
    "\n",
    "        ''access_token'' will be instantiated via enviromental variable\n",
    "        If ``access_token`` is None, attempts to obtain it using ``refresh_token``.\n",
    "\n",
    "        Args:\n",
    "            api_url: Base API URL of a Superset instance, e.g. https://my-superset/api/v1.\n",
    "            access_token: Access token to use for accessing protected endpoints of the Superset\n",
    "                API. Can be automatically obtained if ``refresh_token`` is not None.\n",
    "            refresh_token: Refresh token to use for obtaining or refreshing the ``access_token``.\n",
    "                If None, no refresh will be done.\n",
    "        \"\"\"\n",
    "        self.url = SUPERSET_HOST\n",
    "        self.api_url = self.url + \"api/v1/\"\n",
    "\n",
    "        self.session = requests.session()\n",
    "\n",
    "        self.username = SUPERSET_USERNAME\n",
    "        self.password = SUPERSET_PASSWORD\n",
    "        self.headers = {}\n",
    "        self._refresh_session()\n",
    "\n",
    "    def _refresh_session(self):\n",
    "        logging.info(\"Refreshing session\")\n",
    "\n",
    "        self.soup = BeautifulSoup(self.session.post(self.url + \"login\").text, \"html.parser\")\n",
    "        self.csrf_token = self.soup.find(\"input\", {\"id\": \"csrf_token\"})[\"value\"]  # type: ignore\n",
    "\n",
    "        data = {\n",
    "            \"username\": self.username,\n",
    "            \"password\": self.password,\n",
    "            \"provider\": \"db\",\n",
    "            \"refresh\": True,\n",
    "        }\n",
    "        self.headers = {\n",
    "            # 'Authorization': 'Bearer {}'.format(self.access_token),\n",
    "            \"x-csrftoken\": self.csrf_token,\n",
    "        }\n",
    "        response = self.session.post(self.url + \"login\", json=data, headers=self.headers)  # type: ignore\n",
    "        return True\n",
    "\n",
    "    def request(self, method, endpoint, refresh_session_if_needed=True, headers=None, **request_kwargs):\n",
    "        \"\"\"Executes a request against the Superset API.\n",
    "\n",
    "        Args:\n",
    "            method: HTTP method to use.\n",
    "            endpoint: Endpoint to use.\n",
    "            refresh_token_if_needed: Whether the ``access_token`` should be automatically refreshed\n",
    "                if needed.\n",
    "            headers: Additional headers to use.\n",
    "            **request_kwargs: Any ``requests.request`` arguments to use.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing response body parsed from JSON.\n",
    "\n",
    "        Raises:\n",
    "            HTTPError: There is an HTTP error (detected by ``requests.Response.raise_for_status``)\n",
    "                even after retrying with a fresh session.\n",
    "        \"\"\"\n",
    "\n",
    "        logging.info(\"About to %s execute request for endpoint %s\", method, endpoint)\n",
    "\n",
    "        if headers is None:\n",
    "            headers = {}\n",
    "\n",
    "        url = self.api_url + endpoint\n",
    "        res = self.session.request(method, url, headers=self.headers, **request_kwargs)  # type: ignore\n",
    "\n",
    "        logging.info(\"Request finished with status: %d\", res.status_code)\n",
    "\n",
    "        if (\n",
    "            refresh_session_if_needed\n",
    "            and res.status_code == 401\n",
    "            and res.json().get(\"msg\") == \"Token has expired\"\n",
    "            and self._refresh_session()\n",
    "        ):\n",
    "            logging.info(f\"Retrying {method} request for {url} %s with refreshed session\")\n",
    "            res = self.session.request(method, url, headers=self.headers, **request_kwargs)  # type: ignore\n",
    "\n",
    "            logging.info(\"Request finished with status: %d\", res.status_code)\n",
    "\n",
    "        if (\n",
    "            refresh_session_if_needed\n",
    "            and res.status_code == 400\n",
    "            and res.json()[\"message\"] == \"400 Bad Request: The CSRF session token is missing.\"\n",
    "            and self._refresh_session()\n",
    "        ):\n",
    "            logging.info(f\"Retrying {method} request for {url} %s with refreshed session\")\n",
    "            res = self.session.request(method, url, headers=self.headers, **request_kwargs)  # type: ignore\n",
    "            logging.info(f\"Request finished with status: {res.status_code}\")\n",
    "        res.raise_for_status()\n",
    "        return res.json()\n",
    "\n",
    "\n",
    "def get_tables_from_dbt(dbt_manifest, dbt_db_name):\n",
    "    tables = {}\n",
    "    for table_type in [\"nodes\"]:\n",
    "        manifest_subset = dbt_manifest[table_type]\n",
    "\n",
    "        for table_key_long in manifest_subset:\n",
    "            table = manifest_subset[table_key_long]\n",
    "            name = table[\"name\"]\n",
    "            schema = table[\"schema\"]\n",
    "            database = table[\"database\"]\n",
    "            description = table[\"description\"]\n",
    "            alias = table[\"alias\"]\n",
    "            source = table[\"unique_id\"].split(\".\")[-2]\n",
    "            table_key = schema + \".\" + alias  # Key will be alias, not name\n",
    "            columns = table[\"columns\"]\n",
    "\n",
    "            if dbt_db_name is None or database == dbt_db_name:\n",
    "                # fail if it breaks uniqueness constraint\n",
    "                assert table_key not in tables, (\n",
    "                    f\"Table {table_key} is a duplicate name (schema + table) across databases. \"\n",
    "                    \"This would result in incorrect matching between Superset and dbt. \"\n",
    "                    \"To fix this, remove duplicates or add ``dbt_db_name``.\"\n",
    "                )\n",
    "                tables[table_key] = {\n",
    "                    \"name\": name,\n",
    "                    \"schema\": schema,\n",
    "                    \"database\": database,\n",
    "                    \"type\": table_type[:-1],\n",
    "                    \"ref\": f\"ref('{name}')\" if table_type == \"nodes\" else f\"source('{source}', '{name}')\",\n",
    "                    \"user\": None,\n",
    "                    \"columns\": columns,\n",
    "                    \"description\": description,\n",
    "                    \"alias\": alias,\n",
    "                }\n",
    "            if schema == \"user\":\n",
    "                tables[table_key][\"user\"] = table[\"tags\"][0]\n",
    "\n",
    "    assert tables, \"Manifest is empty!\"\n",
    "\n",
    "    return tables\n",
    "\n",
    "\n",
    "def get_physical_datasets_from_superset(superset: SupersetDBTConnectorSession, superset_db_id):\n",
    "    logging.info(\"Getting physical datasets from Superset.\")\n",
    "    page_number = 0\n",
    "    datasets = []\n",
    "    datasets_keys = set()\n",
    "    while True:\n",
    "        logging.info(\"Getting page %d.\", page_number + 1)\n",
    "        rison_request = f\"dataset/?q=(page_size:100,page:{page_number},order_column:changed_on_delta_humanized,order_direction:asc,filters:!((col:table_name,opr:nct,value:archived),(col:sql,opr:dataset_is_null_or_empty,value:true)))\"\n",
    "        res = superset.request(\"GET\", rison_request)\n",
    "        result = res[\"result\"]\n",
    "        if result:\n",
    "            for r in result:\n",
    "                name = r[\"table_name\"]\n",
    "                schema = r[\"schema\"]\n",
    "                database_name = r[\"database\"][\"database_name\"]\n",
    "                dataset_id = r[\"id\"]\n",
    "                database_id = r[\"database\"][\"id\"]\n",
    "                dataset_key = f\"{schema}.{name}\"  # same format as in dashboards\n",
    "\n",
    "                kind = r[\"kind\"]\n",
    "                if kind == \"physical\" and (superset_db_id is None or database_id == superset_db_id):\n",
    "                    dataset_id = r[\"id\"]\n",
    "\n",
    "                    name = r[\"table_name\"]\n",
    "                    schema = r[\"schema\"]\n",
    "                    dataset_key = f\"{schema}.{name}\"  # used as unique identifier\n",
    "\n",
    "                    dataset_dict = {\n",
    "                        \"id\": dataset_id,\n",
    "                        \"name\": name,\n",
    "                        \"schema\": schema,\n",
    "                        \"database\": database_name,\n",
    "                        \"dataset_id\": dataset_id,\n",
    "                        \"key\": dataset_key,\n",
    "                        \"table\": [dataset_key],\n",
    "                    }\n",
    "\n",
    "                    # fail if it breaks uniqueness constraint\n",
    "                    assert dataset_key not in datasets_keys, (\n",
    "                        f\"Dataset {dataset_key} is a duplicate name (schema + table) \"\n",
    "                        \"across databases. \"\n",
    "                        \"This would result in incorrect matching between Superset and dbt. \"\n",
    "                        \"To fix this, remove duplicates or add the ``superset_db_id`` argument.\"\n",
    "                    )\n",
    "\n",
    "                    datasets_keys.add(dataset_key)\n",
    "                    datasets.append(dataset_dict)\n",
    "\n",
    "            page_number += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def get_tables_from_sql_simple(sql):\n",
    "    \"\"\"\n",
    "    (Superset) Fallback SQL parsing using regular expressions to get tables names.\n",
    "    \"\"\"\n",
    "    sql = re.sub(r\"(--.*)|(#.*)\", \"\", sql)\n",
    "    sql = re.sub(r\"\\s+\", \" \", sql).lower()\n",
    "    sql = re.sub(r\"(/\\*(.|\\n)*\\*/)\", \"\", sql)\n",
    "\n",
    "    regex = re.compile(r\"\\b(from|join)\\b\\s+(\\\"?(\\w+)\\\"?(\\.))?\\\"?(\\w+)\\\"?\\b\")\n",
    "    tables_match = regex.findall(sql)\n",
    "    tables = [\n",
    "        table[2] + \".\" + table[4] if table[2] != \"\" else table[4] for table in tables_match if table[4] != \"unnest\"\n",
    "    ]\n",
    "\n",
    "    tables = list(set(tables))\n",
    "\n",
    "    return tables\n",
    "\n",
    "\n",
    "def get_tables_from_sql(sql, dialect, sql_parsed=None):\n",
    "    \"\"\"\n",
    "    (Superset) SQL parsing using sqlfluff to get clean tables names.\n",
    "    If sqlfluff parsing fails it runs the above regex parsing func.\n",
    "    Returns a tables list.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not sql_parsed:\n",
    "            sql_parsed = sqlfluff.parse(sql, dialect=dialect)\n",
    "        tables_raw = list(get_json_segment(sql_parsed, \"table_reference\"))  # type: ignore\n",
    "        tables_cleaned = []  # With schema\n",
    "        for table_ref in tables_raw:\n",
    "            if isinstance(table_ref, list):\n",
    "                table_ref_identifier = []\n",
    "                # Get last 2 \"naked_identifier\"\n",
    "                for dictionary in table_ref[::-1]:\n",
    "                    if \"naked_identifier\" in dictionary:\n",
    "                        table_ref_identifier.append(dictionary[\"naked_identifier\"])\n",
    "                        if len(table_ref_identifier) == 2:\n",
    "                            tables_cleaned.append(\".\".join(table_ref_identifier[::-1]))\n",
    "                            break\n",
    "            if isinstance(table_ref, dict):\n",
    "                tables_cleaned.append(table_ref[\"naked_identifier\"])\n",
    "    except (\n",
    "        sqlfluff.core.errors.SQLParseError,  # type: ignore\n",
    "        sqlfluff.core.errors.SQLLexError,  # type: ignore\n",
    "        sqlfluff.core.errors.SQLFluffUserError,  # type: ignore\n",
    "        sqlfluff.api.simple.APIParsingError,  # type: ignore\n",
    "    ) as e:  # type: ignore\n",
    "        logging.warning(\n",
    "            \"Parsing SQL through sqlfluff failed. \"\n",
    "            \"Let me attempt this via regular expressions at least and \"\n",
    "            \"check the problematic query and error below.\\n%s\",\n",
    "            sql,\n",
    "            exc_info=e,\n",
    "        )\n",
    "        tables_cleaned = get_tables_from_sql_simple(sql)\n",
    "\n",
    "    return tables_cleaned\n",
    "\n",
    "\n",
    "def get_json_segment(\n",
    "    parse_result: Dict[str, Any], segment_type: str\n",
    ") -> Iterator[Union[str, Dict[str, Any], List[Dict[str, Any]]]]:\n",
    "    \"\"\"Recursively search JSON parse result for specified segment type.\n",
    "\n",
    "    Args:\n",
    "        parse_result (Dict[str, Any]): JSON parse result from `sqlfluff.fix`.\n",
    "        segment_type (str): The segment type to search for.\n",
    "\n",
    "    Yields:\n",
    "        Iterator[Union[str, Dict[str, Any], List[Dict[str, Any]]]]:\n",
    "        Retrieves children of specified segment type as either a string for a raw\n",
    "        segment or as JSON or an array of JSON for non-raw segments.\n",
    "    \"\"\"\n",
    "    for k, v in parse_result.items():\n",
    "        if k == segment_type:\n",
    "            yield v\n",
    "        elif isinstance(v, dict):\n",
    "            yield from get_json_segment(v, segment_type)\n",
    "        elif isinstance(v, list):\n",
    "            for s in v:\n",
    "                yield from get_json_segment(s, segment_type)\n",
    "\n",
    "\n",
    "def get_dashboards_from_superset(superset: SupersetDBTConnectorSession, superset_db_id, user_id):\n",
    "    \"\"\"\n",
    "    This function gets\n",
    "    1. Get dashboards id list from Superset iterating on the pages of the url\n",
    "    2. Get a dashboard detail information :\n",
    "        title, owner, url, unique datasets names\n",
    "\n",
    "    Returns dashboards, dashboards_datasets\n",
    "    \"\"\"\n",
    "\n",
    "    logging.info(\"Getting published dashboards from Superset.\")\n",
    "    page_number = 0\n",
    "    dashboards_id = []\n",
    "    while True:\n",
    "        logging.info(\"Getting page %d.\", page_number + 1)\n",
    "        res = superset.request(\"GET\", f'/dashboard/?q={{\"page\":{page_number},\"page_size\":100}}')\n",
    "        result = res[\"result\"]\n",
    "        if result:\n",
    "            for r in result:\n",
    "                if r[\"published\"] and r[\"created_by\"][\"id\"] == user_id:\n",
    "                    dashboards_id.append(r[\"id\"])\n",
    "            page_number += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    assert len(dashboards_id) > 0, \"There are no dashboards in Superset!\"\n",
    "\n",
    "    logging.info(\"There are %d published dashboards in Superset.\", len(dashboards_id))\n",
    "\n",
    "    dashboards = []\n",
    "    dashboards_datasets_w_db = set()\n",
    "    for i, d in enumerate(dashboards_id):\n",
    "        logging.info(\"Getting info for dashboard %d/%d.\", i + 1, len(dashboards_id))\n",
    "        res = superset.request(\"GET\", f\"/dashboard/{d}\")\n",
    "        result = res[\"result\"]\n",
    "\n",
    "        dashboard_id = result[\"id\"]\n",
    "        title = result[\"dashboard_title\"]\n",
    "        url = superset.url + \"/superset/dashboard/\" + str(dashboard_id)\n",
    "        owner_name = result[\"owners\"][0][\"first_name\"] + \" \" + result[\"owners\"][0][\"last_name\"]\n",
    "\n",
    "        # take unique dataset names, formatted as \"[database].[schema].[table]\" by Superset\n",
    "        res_table_names = superset.request(\"GET\", f\"/dashboard/{d}/datasets\")\n",
    "        result_table_names = res_table_names[\"result\"]\n",
    "\n",
    "        testing = []\n",
    "        for i in range(0, len(result_table_names)):\n",
    "            testing.append(result_table_names[i][\"name\"])\n",
    "\n",
    "        # datasets_raw = list(set(result['table_names'].split(', ')))\n",
    "        datasets_raw = testing\n",
    "\n",
    "        # parse dataset names into parts\n",
    "        datasets_parsed = [dataset[1:-1].split(\"].[\", maxsplit=2) for dataset in datasets_raw]\n",
    "        datasets_parsed = [\n",
    "            [dataset[0], \"None\", dataset[1]]  # add None in the middle\n",
    "            if len(dataset) == 2\n",
    "            else dataset  # if missing the schema\n",
    "            for dataset in datasets_parsed\n",
    "        ]\n",
    "\n",
    "        # put them all back together to get \"database.schema.table\"\n",
    "        datasets_w_db = [\".\".join(dataset) for dataset in datasets_parsed]\n",
    "\n",
    "        dbt_project_name = \"your_dbt_project.\"\n",
    "        datasets_w_db = [dbt_project_name + sub for sub in testing]\n",
    "\n",
    "        dashboards_datasets_w_db.update(datasets_w_db)\n",
    "\n",
    "        # skip database, i.e. first item, to get only \"schema.table\"\n",
    "        datasets_wo_db = [\".\".join(dataset[1:]) for dataset in datasets_parsed]\n",
    "\n",
    "        datasets_wo_db = testing\n",
    "        dashboard = {\n",
    "            \"id\": dashboard_id,\n",
    "            \"title\": title,\n",
    "            \"url\": url,\n",
    "            \"owner_name\": owner_name,\n",
    "            \"owner_email\": \"\",  # required for dbt to accept owner_name but not in response\n",
    "            \"datasets\": datasets_wo_db,  # add in \"schema.table\" format\n",
    "        }\n",
    "        dashboards.append(dashboard)\n",
    "    # test if unique when database disregarded\n",
    "    # loop to get the name of duplicated dataset and work with unique set of datasets w db\n",
    "    dashboards_datasets = set()\n",
    "    for dataset_w_db in dashboards_datasets_w_db:\n",
    "        dataset = \".\".join(dataset_w_db.split(\".\")[1:])  # similar logic as just a bit above\n",
    "\n",
    "        # fail if it breaks uniqueness constraint and not limited to one database\n",
    "        assert dataset not in dashboards_datasets or superset_db_id is not None, (\n",
    "            f\"Dataset {dataset} is a duplicate name (schema + table) across databases. \"\n",
    "            \"This would result in incorrect matching between Superset and dbt. \"\n",
    "            \"To fix this, remove duplicates or add ``superset_db_id``.\"\n",
    "        )\n",
    "\n",
    "        dashboards_datasets.add(dataset)\n",
    "\n",
    "    return dashboards, dashboards_datasets\n",
    "\n",
    "\n",
    "def get_datasets_from_superset_dbt_refs(\n",
    "    superset: SupersetDBTConnectorSession, dashboards_datasets, dbt_tables, sql_dialect, superset_db_id\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns datasets (dict) containing table info and dbt references\n",
    "    \"\"\"\n",
    "\n",
    "    logging.info(\"Getting datasets info from Superset.\")\n",
    "    page_number = 0\n",
    "    datasets = {}\n",
    "    while True:\n",
    "        logging.info(\"Getting page %d.\", page_number + 1)\n",
    "        res = superset.request(\"GET\", f'/dataset/?q={{\"page\":{page_number},\"page_size\":100}}')\n",
    "        result = res[\"result\"]\n",
    "        if result:\n",
    "            for r in result:\n",
    "                name = r[\"table_name\"]\n",
    "                schema = r[\"schema\"]\n",
    "                database_name = r[\"database\"][\"database_name\"]\n",
    "                database_id = r[\"database\"][\"id\"]\n",
    "\n",
    "                dataset_key = f\"{schema}.{name}\"  # same format as in dashboards\n",
    "\n",
    "                # only add datasets that are in dashboards, optionally limit to one database\n",
    "                if dataset_key in dashboards_datasets and (superset_db_id is None or database_id == superset_db_id):\n",
    "                    kind = r[\"kind\"]\n",
    "                    if kind == \"virtual\":  # built on custom sql\n",
    "                        sql = r[\"sql\"]\n",
    "                        tables = get_tables_from_sql(sql, sql_dialect)\n",
    "                        tables = [table if \".\" in table else f\"{schema}.{table}\" for table in tables]\n",
    "                    else:  # built on tables\n",
    "                        tables = [dataset_key]\n",
    "                    dbt_refs = [dbt_tables[table][\"ref\"] for table in tables if table in dbt_tables]\n",
    "\n",
    "                    datasets[dataset_key] = {\n",
    "                        \"name\": name,\n",
    "                        \"schema\": schema,\n",
    "                        \"database\": database_name,\n",
    "                        \"kind\": kind,\n",
    "                        \"tables\": tables,\n",
    "                        \"dbt_refs\": dbt_refs,\n",
    "                    }\n",
    "            page_number += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def refresh_columns_in_superset(superset: SupersetDBTConnectorSession, dataset_id):\n",
    "    logging.info(\"Refreshing columns in Superset.\")\n",
    "    superset.request(\"PUT\", f\"/dataset/{dataset_id}/refresh\")\n",
    "\n",
    "\n",
    "def add_certifications_in_superset(superset: SupersetDBTConnectorSession, dataset_id, sst_dataset_key, dbt_tables):\n",
    "    logging.info(\"Refreshing columns in Superset.\")\n",
    "    body = {\n",
    "        \"extra\": '{\"certification\": \\n  {\"certified_by\": \"Data Analytics Team\", \\n   \"details\": \"This table is the source of truth.\" \\n    \\n  }\\n}',\n",
    "        \"description\": dbt_tables[sst_dataset_key][\"description\"],\n",
    "        \"owners\": [SUPERSET_ID],\n",
    "    }\n",
    "    if \"user\" in dbt_tables[sst_dataset_key].keys():\n",
    "        body[\"owners\"].append(dbt_tables[sst_dataset_key][\"user\"])\n",
    "    superset.request(\"PUT\", f\"/dataset/{dataset_id}\", json=body)\n",
    "\n",
    "\n",
    "def add_superset_columns(superset: SupersetDBTConnectorSession, dataset):\n",
    "    logging.info(\"Pulling fresh columns info from Superset.\")\n",
    "    res = superset.request(\"GET\", f\"/dataset/{dataset['id']}\")\n",
    "    columns = res[\"result\"][\"columns\"]\n",
    "    dataset[\"columns\"] = columns\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def convert_markdown_to_plain_text(md_string):\n",
    "    \"\"\"Converts a markdown string to plaintext.\n",
    "\n",
    "    The following solution is used:\n",
    "    https://gist.github.com/lorey/eb15a7f3338f959a78cc3661fbc255fe\n",
    "    \"\"\"\n",
    "\n",
    "    # md -> html -> text since BeautifulSoup can extract text cleanly\n",
    "    html = markdown(md_string)\n",
    "\n",
    "    # remove code snippets\n",
    "    html = re.sub(r\"<pre>(.*?)</pre>\", \" \", html)\n",
    "    html = re.sub(r\"<code>(.*?)</code >\", \" \", html)\n",
    "\n",
    "    # extract text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    text = \"\".join(soup.findAll(text=True))\n",
    "\n",
    "    # make one line\n",
    "    single_line = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    # make fixes\n",
    "    single_line = re.sub(\"→\", \"->\", single_line)\n",
    "    single_line = re.sub(\"<null>\", '\"null\"', single_line)\n",
    "\n",
    "    return single_line\n",
    "\n",
    "\n",
    "def merge_columns_info(dataset, tables):\n",
    "    logging.info(\"Merging columns info from Superset and manifest.json file.\")\n",
    "\n",
    "    key = dataset[\"key\"]\n",
    "    sst_columns = dataset[\"columns\"]\n",
    "    dbt_columns = tables.get(key, {}).get(\"columns\", {})\n",
    "    columns_new = []\n",
    "    for sst_column in sst_columns:\n",
    "        # add the mandatory field\n",
    "        column_new = {\"column_name\": sst_column[\"column_name\"]}\n",
    "\n",
    "        # add optional fields only if not already None, otherwise it would error out\n",
    "        for field in [\n",
    "            \"expression\",\n",
    "            \"filterable\",\n",
    "            \"groupby\",\n",
    "            \"python_date_format\",\n",
    "            \"verbose_name\",\n",
    "            \"type\",\n",
    "            \"is_dttm\",\n",
    "            \"is_active\",\n",
    "        ]:\n",
    "            if sst_column[field] is not None:\n",
    "                column_new[field] = sst_column[field]\n",
    "\n",
    "        # add description\n",
    "        if (\n",
    "            sst_column[\"column_name\"] in dbt_columns\n",
    "            and \"description\" in dbt_columns[sst_column[\"column_name\"]]\n",
    "            and sst_column[\"expression\"] == \"\"\n",
    "        ):  # database columns\n",
    "            description = dbt_columns[sst_column[\"column_name\"]][\"description\"]\n",
    "            description = convert_markdown_to_plain_text(description)\n",
    "        else:  # if cant find in dbt\n",
    "            description = sst_column[\"description\"]\n",
    "        column_new[\"description\"] = description\n",
    "\n",
    "        columns_new.append(column_new)\n",
    "\n",
    "    dataset[\"columns_new\"] = columns_new\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def put_columns_to_superset(superset: SupersetDBTConnectorSession, dataset):\n",
    "    logging.info(\"Putting new columns info with descriptions back into Superset.\")\n",
    "    body = {\"columns\": dataset[\"columns_new\"]}\n",
    "    superset.request(\"PUT\", f\"/dataset/{dataset['id']}?override_columns=true\", json=body)\n",
    "\n",
    "\n",
    "def merge_dashboards_with_datasets(dashboards, datasets):\n",
    "    for dashboard in dashboards:\n",
    "        refs = set()\n",
    "        for dataset in dashboard[\"datasets\"]:\n",
    "            if dataset in datasets:\n",
    "                refs.update(datasets[dataset][\"dbt_refs\"])\n",
    "        refs = list(sorted(refs))\n",
    "\n",
    "        dashboard[\"refs\"] = refs\n",
    "\n",
    "    return dashboards\n",
    "\n",
    "\n",
    "def get_exposures_dict(dashboards, exposures):\n",
    "    dashboards.sort(key=lambda dashboard: dashboard[\"id\"])\n",
    "    titles = [dashboard[\"title\"] for dashboard in dashboards]\n",
    "    # fail if it breaks uniqueness constraint for exposure names\n",
    "    assert len(set(titles)) == len(titles), \"There are duplicate dashboard names!\"\n",
    "\n",
    "    exposures_orig = {exposure[\"url\"]: exposure for exposure in exposures}\n",
    "    exposures_dict = [\n",
    "        {\n",
    "            \"name\": f\"superset__{dashboard['title']}\",\n",
    "            \"type\": \"dashboard\",\n",
    "            \"url\": dashboard[\"url\"],\n",
    "            \"description\": exposures_orig.get(dashboard[\"url\"], {}).get(\"description\", \"\"),\n",
    "            \"depends_on\": dashboard[\"refs\"],\n",
    "            \"owner\": {\"name\": dashboard[\"owner_name\"], \"email\": dashboard[\"owner_email\"]},\n",
    "        }\n",
    "        for dashboard in dashboards\n",
    "    ]\n",
    "\n",
    "    return exposures_dict\n",
    "\n",
    "\n",
    "class YamlFormatted(ruamel.yaml.YAML):\n",
    "    def __init__(self):\n",
    "        super(YamlFormatted, self).__init__()\n",
    "        self.default_flow_style = False\n",
    "        self.allow_unicode = True\n",
    "        self.encoding = \"utf-8\"\n",
    "        self.block_seq_indent = 2\n",
    "        self.indent = 4\n",
    "        self.emitter.alt_null = \"''\"\n",
    "\n",
    "\n",
    "# Create Query\n",
    "\n",
    "\n",
    "def is_valid_table_name(table_name):\n",
    "    \"\"\"\n",
    "    Checks if the given string is a valid table name in PostgreSQL.\n",
    "\n",
    "    Args:\n",
    "        table_name: The string to check.\n",
    "\n",
    "    Returns:\n",
    "        True if the string is a valid table name, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # The regular expression to match a valid table name.\n",
    "    regex = re.compile(r\"^[a-zA-Z0-9_]{1,63}$\")\n",
    "\n",
    "    # Check if the string matches the regular expression.\n",
    "    if regex.match(table_name):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def is_unique_table_name(table_name, dbt_tables):\n",
    "    \"\"\"\n",
    "    Checks if the given string is a valid table name in PostgreSQL and dbt.\n",
    "\n",
    "    Args:\n",
    "        table_name: The string to check.\n",
    "        dbt_tables: Dict of get_dbt_tables\n",
    "    Returns:\n",
    "        True if the string is a valid table name, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # The regular expression to match a valid table name.\n",
    "    regex = re.compile(r\"^[a-zA-Z0-9_]{1,63}$\")\n",
    "\n",
    "    # Check if the string matches the regular expression.\n",
    "    if table_name not in dbt_tables:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_ref(original_query, dbt_tables, parsed_result, dbt_tables_names):\n",
    "    \"\"\"\n",
    "    Returns content of a user-created dbt model file w/o config.\n",
    "\n",
    "    Args:\n",
    "        original_query: Query needed processing\n",
    "        dbt_tables: Dict of dicts obtained by get_tables_from_dbt.\n",
    "        schema_names: List of serving schema names.\n",
    "\n",
    "    Returns:\n",
    "        String: the content of the dbt model.\n",
    "    \"\"\"\n",
    "    # original_query = original_query[:-1] if original_query[-1] == \";\" else original_query # Maybe unneeded since not wrapping with\n",
    "    # Access table names\n",
    "    fixed_query = str(original_query)\n",
    "    table_names = set(get_tables_from_sql(fixed_query, dialect=\"postgres\", sql_parsed=parsed_result))\n",
    "    fixed_query = sqlfluff.fix(fixed_query, dialect=\"postgres\")\n",
    "    if len(table_names.difference(dbt_tables_names)) > 0:  # dbt_tables_names include schema\n",
    "        return None, \"Tables referenced out of serving schemas\"\n",
    "    # Put tables in subqueries\n",
    "    final_tables = tuple(table_names.intersection(dbt_tables_names))  # Filter out\n",
    "\n",
    "    if len(final_tables) == 0:\n",
    "        return None, \"No tables referenced in dbt projects\"\n",
    "\n",
    "    table_to_ref = {}\n",
    "\n",
    "    # Add ref for original query\n",
    "    new_query = original_query\n",
    "    for table in final_tables:\n",
    "        new_query = (\n",
    "            \"\"\"\n",
    "-- depends_on: {{{{ref(\\'{table}\\')}}}}\n",
    "    \"\"\".format(\n",
    "                table=dbt_tables[table][\"name\"]  # Ensure that there is only table names, no schema names\n",
    "            )\n",
    "            + new_query\n",
    "        )\n",
    "    return new_query, \"Success\"\n",
    "\n",
    "\n",
    "def add_materialization(df_row, query, exec_time):\n",
    "    \"\"\"\n",
    "    Returns content of a user-created dbt model file with config.\n",
    "\n",
    "    Args:\n",
    "        df_row: Row of DataFrame taken from \"query\" table.\n",
    "        dbt_tables: List of tables name.\n",
    "        schema_names: List of serving schema names.\n",
    "\n",
    "    Returns:\n",
    "        String: the content of the dbt model.\n",
    "    \"\"\"\n",
    "    query = (\n",
    "        \"\"\"\n",
    "{{{{ config(\n",
    "    materialized=\\'{materialization}\\',\n",
    "    name='{name}',\n",
    "    description='{desc}',\n",
    "    tags = ['{user_id}','user_created','{created_time}'],\n",
    "    schema = '{schema}'\n",
    ") }}}}\"\"\".format(\n",
    "            materialization=MATERIALIZATION_MAPPING[df_row[\"materialization\"]],\n",
    "            user_id=df_row[\"user_id\"],\n",
    "            name=df_row[\"name\"],\n",
    "            desc=df_row[\"description\"],\n",
    "            created_time=exec_time,\n",
    "            schema=USER_SCHEMA,\n",
    "        )\n",
    "        + query\n",
    "    )\n",
    "    return query\n",
    "\n",
    "\n",
    "def get_records():\n",
    "    # Query records\n",
    "    try:\n",
    "        connection = psycopg2.connect(\n",
    "            user=DATABASE_USERNAME,\n",
    "            password=DATABASE_PASSWORD,\n",
    "            host=DATABASE_HOST,\n",
    "            port=DATABASE_PORT,\n",
    "            database=DATABASE_NAME,\n",
    "        )\n",
    "        cursor = connection.cursor()\n",
    "        postgreSQL_select_Query = f\"select * from {QUERY_SCHEMA}.{QUERY_TABLE} where checked = False\"\n",
    "        # postgreSQL_select_Query = \"\"\"\n",
    "        # SELECT *\n",
    "        # FROM query\n",
    "        # WHERE insert_time  > now() - interval '30 second';\n",
    "        # \"\"\"\n",
    "        logging.info(f\"Executing query to fetch records: {postgreSQL_select_Query}\")\n",
    "        cursor.execute(postgreSQL_select_Query)\n",
    "        query_columns = [\n",
    "            \"query_string\",\n",
    "            \"materialization\",\n",
    "            \"user_id\",\n",
    "            \"description\",\n",
    "            \"insert_time\",\n",
    "            \"name\",\n",
    "            \"checked\",\n",
    "            \"success\",\n",
    "        ]\n",
    "\n",
    "        df = pd.DataFrame(cursor.fetchall(), columns=query_columns)\n",
    "\n",
    "    finally:\n",
    "        # closing database connection.\n",
    "        if connection:\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            logging.info(\"PostgreSQL connection is closed\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def update_records(update_values):\n",
    "    try:\n",
    "        connection = psycopg2.connect(\n",
    "            user=DATABASE_USERNAME,\n",
    "            password=DATABASE_PASSWORD,\n",
    "            host=DATABASE_HOST,\n",
    "            port=DATABASE_PORT,\n",
    "            database=DATABASE_NAME,\n",
    "        )\n",
    "        cursor = connection.cursor()\n",
    "        update_sql_query = f\"\"\"UPDATE {QUERY_SCHEMA}.{QUERY_TABLE} q \n",
    "                                SET success = v.success,\n",
    "                                    checked = v.checked\n",
    "\n",
    "                                FROM (VALUES {update_values}) AS v (name, user_id, checked, success)\n",
    "                                WHERE q.user_id = v.user_id \n",
    "                                AND q.name = v.name;\"\"\"\n",
    "        logging.info(f\"Executing query to update records: {update_sql_query}\")\n",
    "        cursor.execute(update_sql_query)\n",
    "\n",
    "    finally:\n",
    "        # closing database connection.\n",
    "        if connection:\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            logging.info(\"PostgreSQL connection is closed\")\n",
    "\n",
    "\n",
    "def get_emails(superset, user_ids):\n",
    "    url = unquote(f\"/security/get_email/?q={list(user_ids)}\")\n",
    "    res = superset.request(\"GET\", url)\n",
    "    return res[\"emails\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Refreshing session\n",
      "INFO:root:Starting the script!\n",
      "INFO:root:Getting physical datasets from Superset.\n",
      "INFO:root:Getting page 1.\n",
      "INFO:root:About to GET execute request for endpoint dataset/?q=(page_size:100,page:0,order_column:changed_on_delta_humanized,order_direction:asc,filters:!((col:table_name,opr:nct,value:archived),(col:sql,opr:dataset_is_null_or_empty,value:true)))\n",
      "INFO:root:Request finished with status: 200\n",
      "INFO:root:Getting page 2.\n",
      "INFO:root:About to GET execute request for endpoint dataset/?q=(page_size:100,page:1,order_column:changed_on_delta_humanized,order_direction:asc,filters:!((col:table_name,opr:nct,value:archived),(col:sql,opr:dataset_is_null_or_empty,value:true)))\n",
      "INFO:root:Request finished with status: 200\n",
      "INFO:root:There are 14 physical datasets in Superset.\n",
      "INFO:root:Processing dataset 1/14.\n",
      "INFO:root:Refreshing columns in Superset.\n",
      "INFO:root:About to PUT execute request for endpoint /dataset/126/refresh\n",
      "INFO:root:Request finished with status: 200\n",
      "INFO:root:Pulling fresh columns info from Superset.\n",
      "INFO:root:About to GET execute request for endpoint /dataset/126\n",
      "INFO:root:Request finished with status: 200\n",
      "INFO:root:Merging columns info from Superset and manifest.json file.\n",
      "INFO:root:Putting new columns info with descriptions back into Superset.\n",
      "INFO:root:About to PUT execute request for endpoint /dataset/126?override_columns=true\n",
      "INFO:root:Request finished with status: 200\n",
      "INFO:root:Refreshing columns in Superset.\n",
      "INFO:root:About to PUT execute request for endpoint /dataset/126\n",
      "INFO:root:Request finished with status: 400\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "400 Client Error: BAD REQUEST for url: http://34.82.185.252:30007/api/v1/dataset/126",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m     sst_dataset_w_cols_new \u001b[39m=\u001b[39m merge_columns_info(sst_dataset_w_cols, dbt_tables)\n\u001b[1;32m     36\u001b[0m     put_columns_to_superset(superset, sst_dataset_w_cols_new)\n\u001b[0;32m---> 37\u001b[0m     add_certifications_in_superset(superset, sst_dataset_id, sst_dataset_key, dbt_tables)\n\u001b[1;32m     39\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mAll done!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 447\u001b[0m, in \u001b[0;36madd_certifications_in_superset\u001b[0;34m(superset, dataset_id, sst_dataset_key, dbt_tables)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m dbt_tables[sst_dataset_key]\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    446\u001b[0m     body[\u001b[39m\"\u001b[39m\u001b[39mowners\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mappend(dbt_tables[sst_dataset_key][\u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m--> 447\u001b[0m superset\u001b[39m.\u001b[39;49mrequest(\u001b[39m\"\u001b[39;49m\u001b[39mPUT\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m/dataset/\u001b[39;49m\u001b[39m{\u001b[39;49;00mdataset_id\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m, json\u001b[39m=\u001b[39;49mbody)\n",
      "Cell \u001b[0;32mIn[5], line 99\u001b[0m, in \u001b[0;36mSupersetDBTConnectorSession.request\u001b[0;34m(self, method, endpoint, refresh_session_if_needed, headers, **request_kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession\u001b[39m.\u001b[39mrequest(method, url, headers\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheaders, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mrequest_kwargs)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     logging\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRequest finished with status: \u001b[39m\u001b[39m{\u001b[39;00mres\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 99\u001b[0m res\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    100\u001b[0m \u001b[39mreturn\u001b[39;00m res\u001b[39m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/Desktop/Projects/Thesis/venv/lib/python3.10/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     http_error_msg \u001b[39m=\u001b[39m (\n\u001b[1;32m   1017\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m Server Error: \u001b[39m\u001b[39m{\u001b[39;00mreason\u001b[39m}\u001b[39;00m\u001b[39m for url: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1018\u001b[0m     )\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 400 Client Error: BAD REQUEST for url: http://34.82.185.252:30007/api/v1/dataset/126"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "superset = SupersetDBTConnectorSession()\n",
    "\n",
    "logging.info(\"Starting the script!\")\n",
    "\n",
    "sst_datasets = get_physical_datasets_from_superset(superset, DATABASE_ID)\n",
    "logging.info(\"There are %d physical datasets in Superset.\", len(sst_datasets))\n",
    "\n",
    "if Path(MANIFEST_PATH).is_file():\n",
    "    with open(MANIFEST_PATH) as f:\n",
    "        dbt_manifest = json.load(f)\n",
    "else:\n",
    "    raise Exception(\"No manifest found at path\")\n",
    "\n",
    "dbt_tables = get_tables_from_dbt(dbt_manifest, None)\n",
    "\n",
    "for i, sst_dataset in enumerate(sst_datasets):\n",
    "    columns_refreshed = 0\n",
    "    logging.info(\"Processing dataset %d/%d.\", i + 1, len(sst_datasets))\n",
    "    sst_dataset_id = sst_dataset[\"id\"]\n",
    "    sst_dataset_key = sst_dataset[\"key\"]\n",
    "    if sst_dataset[\"schema\"] == USER_SCHEMA:\n",
    "        continue  # Don't push user description\n",
    "\n",
    "    refresh_columns_in_superset(superset, sst_dataset_id)\n",
    "    columns_refreshed = 1\n",
    "\n",
    "    if columns_refreshed == 1:\n",
    "        columns_refreshed = 1\n",
    "    else:\n",
    "        refresh_columns_in_superset(superset, sst_dataset_id)\n",
    "    # Otherwise, just adding the normal analytics certification\n",
    "    sst_dataset_w_cols = add_superset_columns(superset, sst_dataset)\n",
    "    sst_dataset_w_cols_new = merge_columns_info(sst_dataset_w_cols, dbt_tables)\n",
    "    put_columns_to_superset(superset, sst_dataset_w_cols_new)\n",
    "    add_certifications_in_superset(superset, sst_dataset_id, sst_dataset_key, dbt_tables)\n",
    "\n",
    "logging.info(\"All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'dim_organization',\n",
       " 'schema': 'marts',\n",
       " 'database': 'financial_data',\n",
       " 'type': 'node',\n",
       " 'ref': \"ref('dim_organization')\",\n",
       " 'user': None,\n",
       " 'columns': {},\n",
       " 'description': '',\n",
       " 'alias': 'dim_organization'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbt_tables[sst_dataset_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'a'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dbt_tables[\u001b[39m\"\u001b[39;49m\u001b[39ma\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'a'"
     ]
    }
   ],
   "source": [
    "dbt_tables[\"a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_certifications_in_superset(superset: SupersetDBTConnectorSession, dataset_id, sst_dataset_key, dbt_tables):\n",
    "    logging.info(\"Refreshing columns in Superset.\")\n",
    "    body = {\n",
    "        \"extra\": '{\"certification\": \\n  {\"certified_by\": \"Data Analytics Team\", \\n   \"details\": \"This table is the source of truth.\" \\n    \\n  }\\n}',\n",
    "        \"description\": dbt_tables[sst_dataset_key][\"description\"],\n",
    "        \"owners\": [SUPERSET_ID],\n",
    "    }\n",
    "    if \"user\" in dbt_tables[sst_dataset_key].keys() :\n",
    "        body[\"owners\"].append(dbt_tables[sst_dataset_key][\"user\"])\n",
    "    superset.request(\"PUT\", f\"/dataset/{dataset_id}\", json=body)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
